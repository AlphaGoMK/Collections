# Object Detection

## mAP

> AP is averaged over all categories. Traditionally, this is called â€œmean average precisionâ€ (mAP). We make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context.  

[mAP (mean Average Precision) for Object Detection - Jonathan Hui - Medium](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)

---

## CornerNet

> anchor-free

#### Motivation

1. anchor boxå¤ªå¤šï¼Œåªæœ‰å°‘éƒ¨åˆ†å’ŒGTé‡åˆ
2. anchor é€‰æ‹©éœ€è¦äººä¸ºè®¾è®¡ï¼ˆæ•°é‡ï¼Œå°ºå¯¸ï¼Œæ¯”ä¾‹ï¼‰ï¼Œä¸åŒå°ºåº¦anchorè®¾ç½®ä¸åŒ

ä¸¤ä¸ªéƒ¨åˆ†ï¼š 1) æ£€æµ‹è§’ç½‘ç»œï¼Œå·¦ä¸Š+å³ä¸‹ 2) åµŒå…¥ç½‘ç»œï¼Œç”¨äºåŒ¹é…è§’ç‚¹

#### Method

![](Figures/1e682da0e78dfe82e953f541d1580f86f14af8f6.png)

**æ£€æµ‹cornerç½‘ç»œ**ï¼šæç‰¹å¾ä¹‹åï¼Œç»è¿‡corner poolingäº§ç”Ÿæ¯ä¸ªç±»åˆ«çš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„heat mapã€‚ä¸ºæ­£è´Ÿæ ·æœ¬åŒ¹é…ï¼Œåªæƒ©ç½šGTä¸€å®šèŒƒå›´å¤–çš„é¢„æµ‹ç‚¹ï¼ˆé€šè¿‡IoU thresholdé™åˆ¶radiusï¼‰
**è®¡ç®—åµŒå…¥embedç½‘ç»œ**ï¼šç”¨äºåŒ¹é…åŒæ¡†çš„å·¦ä¸Šå³ä¸‹ã€‚æŸå¤±å‡½æ•°ï¼šåŒæ¡†å·¦ä¸Šå’Œå³ä¸‹æ¥è¿‘ï¼ˆvarianceå°ï¼‰ï¼Œä¸åŒæ¡†çš„å¹³å‡embedè·ç¦»å¤§ã€‚
<u>**Corner pooling** </u>ï¼šè§£å†³è§’ç‚¹ç‰¹å¾å°‘ï¼Œå–ä¸€æ¡çº¿ä¸Šçš„æœ€å¤§å€¼pooling
![](Figures/63eebde8b0adc4b30eacb0eb14e2b291b78b0e87.png)
*Hourglass network*ï¼šæç‰¹å¾

**two stageå¾—åˆ°é¢„æµ‹æ¡†ä¹‹åé‡‡ç”¨RoI pooling/alignæå–æ£€æµ‹æ¡†å†…éƒ¨çš„ä¿¡æ¯ï¼Œåªæœ‰å†…éƒ¨ä¿¡æ¯(approx.)çš„ç‰¹å¾å†è¿›è¡Œä¸€æ¬¡æ¡†å®šå’Œåˆ†ç±»ï¼ˆrefineæ­¥éª¤ï¼‰ï¼›è€Œä¸€é˜¶æ®µçš„æ–¹æ³•åœ¨æå–åˆ°ç‰¹å¾ä¹‹ååˆ†æˆä¸¤æ”¯è¿›è¡Œæ¡†å®šå’Œåˆ†ç±»ï¼Œæ²¡æœ‰å¯¹è¿‘ä¼¼çš„åªåŒ…å«æ¡†å†…ç‰©ä½“çš„ç‰¹å¾(å±€éƒ¨ç‰¹å¾)è¿›è¡Œå†ä¸€æ¬¡æå–ï¼Œæ‰€ä»¥ç²¾åº¦è¾ƒå·®**

-----

## CenterNet

> anchor-free

#### Motivation

CornerNetåªç”¨åˆ°è¾¹ç¼˜çš„ç‰¹å¾ä¿¡æ¯ï¼Œæ²¡æœ‰ç”¨åˆ°å†…éƒ¨çš„ç‰¹å¾ä¿¡æ¯ï¼ˆé€ æˆä¸æ­¢å¯¹ç‰©ä½“çš„è¾¹ç¼˜æ•æ„Ÿï¼Œä¹Ÿå¯¹èƒŒæ™¯çš„è¾¹ç¼˜æ•æ„Ÿï¼‰ã€‚å†…éƒ¨ä¿¡æ¯å¯¹äºå†³å®šä¸¤ä¸ªkeypointæ˜¯å¦æ˜¯åŒä¸€ä¸ªæ¡†æœ‰å¸®åŠ©ã€‚
CenterNeté¢„æµ‹ä¸‰å…ƒç»„ï¼šå·¦ä¸Šï¼Œå³ä¸‹ï¼Œä¸­å¿ƒ

#### Method

![](Figures/d9ab741df8f3e313182e9e5f3f47cc6d89eba5c4.png)
äºŒåˆ†æ”¯ï¼šäº§ç”Ÿcornerç‚¹å¹¶matchå½¢æˆæ¡†ï¼›äº§ç”Ÿcenterç‚¹ã€‚å¦‚æœcenterç‚¹åœ¨æ¡†çš„central regionï¼Œè®¡ç®—æ¡†ï¼Œå¦åˆ™åˆ é™¤
<u>**central region**</u> ç¡®å®šåˆ¤å®šçš„ä¸­å¿ƒåŒºåŸŸçš„å¤§å°ï¼šå¤§æ¡†åå°ï¼Œå°æ¡†åå¤§ï¼Œæ•´ä½“çº¿æ€§

$\left\{\begin{array}{l}
{\operatorname{ctl}_{\mathrm{x}}=\frac{(n+1) \mathrm{tl}_{\mathrm{x}}+(n-1) \mathrm{br}_{\mathrm{x}}}{2 n}} \\
{\operatorname{ctl}_{\mathrm{y}}=\frac{(n+1) \mathrm{tl}_{\mathrm{y}}+(n-1) \mathrm{br}_{\mathrm{y}}}{2 n}} \\
{\operatorname{cbr}_{\mathrm{x}}=\frac{(n-1) \mathrm{tl}_{\mathrm{x}}+(n+1) \mathrm{br}_{\mathrm{x}}}{2 n}} \\
{\operatorname{cbr}_{\mathrm{y}}=\frac{(n-1) \mathrm{tl}_{\mathrm{y}}+(n+1) \mathrm{br}_{\mathrm{y}}}{2 n}}
\end{array}\right.$
Nç¦»æ•£å˜åŒ–ï¼Œè¿‡thresholdåå˜ç³»æ•°nã€‚å°$\to$threshold$\to$å¤§ï¼Œ3$\to$5

#### Enrich center&corner information

<u>**center pooling**</u> ç”¨åœ¨é¢„æµ‹ä¸­å¿ƒç‚¹æ—¶ï¼Œå¢åŠ ä¸­å¿ƒç‚¹çš„recognizableç‰¹å¾
ä¸­å¿ƒåŒºåŸŸçš„è·å–ğŸ‘‡
![](Figures/7a71e75ad5c63ff76c7ab0afe79787198c444c38.png)

```
ä¾‹å¦‚æ‰¾leftmostçš„ï¼ˆå³æŠŠhorizontalæœ€å¤§å€¼ä¼ åˆ°æœ€å·¦è¾¹ï¼‰ï¼Œæ¯ä¸ªç‚¹çœ‹è‡ªå·±åˆ°æœ€å³è¾¹çš„æœ€å¤§å€¼ï¼Œä¸æ–­ä¼ ï¼Œåˆ°æœ€å·¦å¯è·å¾—æ•´æ¡æ¨ªçº¿æœ€å¤§ï¼›æ‰¾topmostï¼ˆæŠŠverticalæœ€å¤§å€¼ä¼ åˆ°æœ€ä¸Šé¢ï¼‰ï¼Œæ¯ä¸ªç‚¹çœ‹è‡ªå·±åˆ°æœ€ä¸‹ï¼Œä¸æ–­ä¼ ï¼Œåˆ°æœ€ä¸Šåˆ™å¯è·å¾—æ•´æ¡çº¿æœ€å¤§
```

æ¨¡å—&ç¤ºæ„å›¾
![](Figures/ade09a091bb57b300358c745de23118416db2913.png)
![](Figures/ca0373639965a7c36e025eeef214af59716f68eb.png)<u>**cascade corner pooling**</u> å¢åŠ è§’ç‚¹çš„ç‰¹å¾ï¼Œç›¸æ¯”corner poolingå¢åŠ å†…éƒ¨ï¼Œä½¿å…¶ä¸å¯¹è¾¹ç¼˜æ•æ„Ÿ
æ²¿ç€è¾¹ç¼˜æ‰¾è¾¹ç¼˜æœ€å¤§å€¼ï¼Œå†ä»è¾¹ç¼˜æœ€å¤§å€¼çš„ä½ç½® *å‘å†…æ‰¾å†…éƒ¨æœ€å¤§å€¼* ï¼Œæœ€åä¸¤ä¸ªæœ€å¤§å€¼ç›¸åŠ 
æ¨¡å—&ç¤ºæ„å›¾
![](Figures/cc6e39c86f14d245507fb1c4c2f691d87dc5a3c7.png)![](Figures/caa1d357e9dd10d63c81d52dcd69b0d045d9b9a3.png)

> Q: how to classification?  
> Need RoI align?  

---

## FCOS: Fully Convolutional One-Stage Object Detection

> anchor-free æ¶ˆé™¤anchorï¼Œå‡å°‘IoUçš„è®¡ç®—å’ŒGTæ¡†çš„åŒ¹é…ã€‚å¯ä»¥ä»£æ›¿äºŒé˜¶æ®µçš„RPN

æŒ‰ç…§åƒç´ è¿›è¡Œé¢„æµ‹<u>**per-pixel prediction**</u>ï¼Œé¢„æµ‹æ¯ä¸€ä¸ªåƒç´ ç‚¹çš„å››ä¸ªç»´åº¦çš„æ¡† `(Left, Top, Right, Bottom)`ğŸ‘‡
![](Figures/433cf8512698164d44e845d1d67fcaf628708061.png)

* å¯¹äºç‰¹å¾å›¾ä¸Šæ¯ä¸€ä¸ªç‚¹ï¼Œå¯¹åº”ä¸€ä¸ªåŸå›¾ä¸Šçš„æ¡†ã€‚ç›´æ¥æŠŠç‰¹å¾å›¾ä¸Šçš„åƒç´ ç‚¹çœ‹æˆè®­ç»ƒæ ·æœ¬è€Œä¸æ˜¯åœ¨ç‚¹ä¸Šé“ºä¸åŒé•¿å®½æ¯”å’Œå¤§å°çš„anchoræ¡†

* å¯¹äºä¸€ä¸ªç‚¹è½åœ¨å¤šä¸ªGTæ¡†ä¸­ï¼ˆambiguous samplesï¼‰é€‰æ‹©æœ€å°çš„bboxä½œä¸ºtargetã€‚åŒæ—¶é€šè¿‡multi-level predictionæ¥å‡å°‘æ•°é‡ã€‚ğŸ‘‡
  ![](Figures/376354c1dd3cfbe7facc6debbe8335f845c44e19.png)ğŸ‘†which bbox this location should regressï¼Ÿ

* FCOSå¯ä»¥åˆ©ç”¨å°½å¯èƒ½å¤šçš„æ ·æœ¬ï¼ˆç‰¹å¾å›¾ä¸Šçš„ç‚¹å½¢æˆçš„æ¡†ï¼‰è€Œä¸åªæ˜¯IoUè¶³å¤Ÿå¤§çš„anchor æ¥è¿›è¡Œè®­ç»ƒã€‚æ¯ä¸ªç‚¹éƒ½å»å­¦ä¹ å¯¹æ¡†çš„é¢„æµ‹ï¼Œå¤šä¸ªç‚¹å…±åŒäº§ç”Ÿå¤šä¸ªç±»ä¼¼çš„æ¡†ï¼Œç„¶åNMSé€‰æ‹©æœ€å¤§çš„

* å¯¹äºambiguous samplesï¼Œé‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œæ¯å±‚é™åˆ¶4D vecä¸­æœ€å¤§å€¼çš„å¤§å°ï¼ˆ<u>**é™åˆ¶æ¯å±‚ç‰¹å¾å›¾äº§ç”Ÿçš„bboxçš„å¤§å°**</u>ï¼‰ï¼Œæ»¡è¶³$m_i < \max(l,t,r,b) < m_{i+1}$ã€‚å¯¹äºåŒä¸€ä¸ªç‚¹ä¸Šå¤šä¸ªæ¡†ï¼Œå› é™åˆ¶ï¼Œæ‰€ä»¥åœ¨ä¸åŒå°ºåº¦ç‰¹å¾å›¾ä¸Šæ„æˆçš„æ¡†è¿›è¡Œregressï¼Œä¸€ä¸ªfeature mapä¸Šä¸€ä¸ªç‚¹åªè´Ÿè´£å›ºå®šå°ºåº¦çš„æ¡†å›å½’ã€‚å¦‚æœè¿˜å‡ºç°é‡å¤ï¼Œåˆ™é€‰æ‹©å°ºå¯¸æœ€å°

* é˜²æ­¢è¿œç¦»ç‰©ä½“ä¸­å¿ƒçš„ç‚¹äº§ç”Ÿè´¨é‡å·®çš„æ¡†ï¼Œcenter-lossã€‚
  
  $centerness^*=\sqrt{\frac{\min(l^*,r^*)}{\max(l^*,r^*)}\times \frac{\min(t^*,b^*)}{\max(t^*,b^*)}}$
  
  ä½¿<u>**å·¦å’Œå³ï¼Œä¸Šå’Œä¸‹çš„é•¿åº¦å°½å¯èƒ½ç›¸ç­‰**</u>ã€‚æµ‹è¯•æ—¶centerness-weighted classification confidenceï¼ŒæŠ‘åˆ¶åè¿œæ¡†ğŸ‘‡
  ![](Figures/d39a52432c2c3225142d3500869baf40e3c73b58.png)
  ç½‘ç»œç»“æ„ğŸ‘‡
  ![](Figures/8b5702a9ed96bc002a55b079d5b531be343ca142.png)

- - - -

## Mask RCNN

å‚è€ƒ[https://zhuanlan.zhihu.com/p/37998710](https://zhuanlan.zhihu.com/p/37998710)

#### ç½‘ç»œç»“æ„

![](Figures/2020-02-17-23-33-00-image.png)

#### RoI Align

![](Figures/2020-02-17-23-32-25-image.png)

![](Figures/2020-02-17-23-33-41-image.png) ğŸ‘†lossè®¡ç®—æ—¶`w*h*c`çš„maskè¾“å‡ºï¼Œåªè®¡ç®—<u>åˆ†ç±»åˆ†æ”¯é¢„æµ‹çš„ç±»åˆ«</u>å¯¹åº”channelçš„sigmodè¾“å‡ºä½œä¸ºæŸå¤±ã€Œ**è¯­ä¹‰maské¢„æµ‹ä¸åˆ†ç±»é¢„æµ‹è§£è€¦**ã€

---

## HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions

> light-weight 

åŒ…å«ä¸¤ä¸ªéƒ¨åˆ† _spatial contraction-expansion_ å’Œ _channel expansion-contraction_ ï¼Œç‹¬ç«‹ä½œç”¨åœ¨ç‰¹å¾å›¾çš„ orthogonal dimensionã€‚å‰è€…é€šè¿‡å‡å°‘ç‰¹å¾å›¾å¤§å°å‡å°‘è®¡ç®—é‡ï¼Œåè€…é€šè¿‡æå‡informative featureæå‡æ€§èƒ½

> mobileneté€šè¿‡åˆ†ç¦»æˆ*point-wise*å’Œ*depth-wise*æ¥åˆ†åˆ«ä¸å˜å°ºå¯¸å˜é€šé“æ•°å’Œä¸å˜é€šé“æ•°å˜å°ºå¯¸(up/down sampling)     so called *depthwise separable conv*  
> shuffleneté€šè¿‡group convå‡å°‘é€šé“ä¸Šçš„è®¡ç®—é‡ï¼Œchannel-shuffleæ¥å¢åŠ ä¸åŒé€šé“ä¹‹é—´çš„è¿æ¥  
> ğŸ‘†previous work focus on **channel transformation**, introduce **spatial feature dim(size)**  

* æå‡é€šé“æ•°å¯ä»¥æå‡ä¿¡æ¯ï¼Œä½†æ˜¯å¢åŠ è®¡ç®—å¼€é”€. æå‡º*Two reciprocal components work on orthogonal dimension*.<u>**é€šé“æ•°æ‰©å±•æ—¶ï¼Œå°ºå¯¸å‡å°‘**</u>
* ç›¸æ¯”mobilenetå¢åŠ **spatial contractionç¼©å°-expansionæ”¾å¤§åˆ°è¾“å…¥çš„å¤§å°**ï¼Œç±»ä¼¼Squeeze-Excitation Networkçš„å…ˆç¼©åæ”¾çš„æ€è·¯
  æ¨¡å—ï¼Œå¯¹æ¯”mobilenetğŸ‘‡
  ![](Figures/7a0feef0d97e42bab4044301662546ba8c93851c.png)
  è®¡ç®—é‡ğŸ‘‡
  ![](Figures/a43e53f7f1c22a59e7b1079ac38e14d39292c07b.png)
* å¢åŠ residual pathï¼Œå‡å°‘ä¸»å¹²è®¡ç®—and **feature reuse**. Inverted residual with harmonious bottleneckğŸ‘‡
  ![](Figures/6c1529ac24e533703ca3c5dcdc70f721512e2ac1.png)
* For Object detection: use **MobileNet V2 SSD** utilize the **warm-up** strategy which linearly ramps up the learning rate from a close-to-zero one 1e-6 to the normal initial learning rate of 1e-3 during the first 5 epochs. 

---

### SSDç½‘ç»œæ—¶é—´

1. ç½‘ç»œè¿è¡Œæ—¶é—´ï¼š0.002-0.003s åœ¨GPUè¿è¡Œ
2. detectï¼ˆNMSä¸ºä¸»ï¼‰è¿è¡Œæ—¶é—´ï¼š0.013-0.016s åªåœ¨ _CPU_ ä¸Šè¿è¡Œ <u>**ç“¶é¢ˆ**</u>

å°ºåº¦å‡å°‘ï¼Œaspect ratioå‡å°‘
Shallow feature map only for small
Deep ONLY large
ä½ç§©ç®€åŒ–
PointRend

<u>**ç‰¹å¾èåˆ
cascade**</u>

---

## Cascade RPN

<u>Single anchor per location + multi-stage refinement</u>

#### Motivation

predefined anchoråœ¨GTå’Œanchorå¯¹é½æ—¶é™åˆ¶æ€§èƒ½/åå·®ï¼Œ(#toread RoIPool RoIAlign)

1. **single anchor** + incorporates **criteria** of anchor/anchor-free in defining **positive** boxes
2. **adaptive convolution** to maintain the alignment between anchor boxes and features

Iterative RPNæ¯æ¬¡æŠŠanchoré›†åˆçœ‹ä½œæ–°çš„anchorè¿›è¡Œrefineï¼Œå¯¼è‡´æ¯æ¬¡è¿­ä»£åanchorä½ç½®å’Œå½¢çŠ¶å‘ç”Ÿå˜åŒ–ï¼Œanchorå’Œè¡¨ç¤ºanchorç‰¹å¾ä¸åŒ¹é…ã€Œ **anchorä¸­å¿ƒç‚¹çš„ç‰¹å¾(å³è¡¨ç¤ºanchorçš„ç‰¹å¾)ä¸å‘ç”Ÿå˜åŒ–ï¼Œä½†æ˜¯anchorçš„ä½ç½®å‘ç”Ÿå˜åŒ–** ï¼Œmismatchã€
ğŸ‘†ä½¿ç”¨deformable convè§£å†³ï¼Œbut _no constraint to enforce_ ğŸ™…â€â™‚ï¸

#### Adaptive Convolution

å·ç§¯é‡‡æ ·çš„æ—¶å€™å¢åŠ offset field
`offset = center offset + shape offset`ä¸­å¿ƒçš„åç§»å’Œå½¢çŠ¶åç§»(ç”±anchorå½¢çŠ¶å’Œkernelå†³å®š)

![](Figures/d5cd3e96c3f74c043b11af85568fd780d34c03b6.png)

ğŸ‘†å¯¹æ¯”deformable convï¼šåç§»é‡ç”±anchorå’Œkernelå†³å®šï¼Œéç½‘ç»œå­¦ä¹ â¡ï¸anchorå’Œfeatureå¯¹é½

#### Sample Discrimination Metrics

æ¯ä¸ªä½ç½®åªæœ‰ä¸€ä¸ªanchorï¼Œç„¶åè¿­ä»£refine

> Determining whether a training sample is pos/neg as the use of anchor/anchor-free is adversarial ä¸¤ç§æ–¹æ³•å†³å®šæ­£è´Ÿæ ·æœ¬çš„æ–¹æ³•ä¸åŒ

ğŸ‘†å³anchor-freeçš„å†³å®šæ–¹å¼å®½æ¾/æ•°é‡å¤šï¼Œanchor-basedæ ‡å‡†ä¸¥æ ¼/æ•°é‡å°‘
Stage 1: anchor-freeâ¡ï¸æ›´å¤šæ­£æ ·æœ¬ã€Œè§£å†³æ­£è´Ÿæ ·æœ¬ä¸åŒ¹é…ã€
Stage 2: amchor-basedâ¡ï¸ä¸¥æ ¼ï¼Œæ•°é‡å‡å°‘ï¼ŒIoUé«˜
`anchor-free`æŒ‡FCOSï¼Œä¸­å¿ƒç‚¹åœ¨ç‰©ä½“å†…ä¸ºpos anchor
`anchor-based`æŒ‡Faster RCNNï¼ŒIoU threshold

#### Cascade RPN

å‰ä¸€ä¸ªé˜¶æ®µçš„è¾“å‡ºbridgeåˆ°åä¸€ä¸ªé˜¶æ®µ
ç”±anchorè®¡ç®—å‡ºoffset `o`ï¼Œå†å’Œfeature `x`è¾“å…¥regressorè®¡ç®—æ–°çš„anchor ( $\sigma$å°±æ˜¯anchorå›å½’çš„ç›®æ ‡ eg. $(tx-ax)/aw$ )

![](Figures/407d8090a6d3898e6971c85df18ba81b99684580.png)

---

## DetNet

> ä¸ºæ£€æµ‹ä»»åŠ¡è®¾è®¡backbone  

ç°æœ‰çš„ImageNet backboneï¼š 1. ç½‘ç»œstageéœ€è¦å¢åŠ ï¼Œä¸”æœªåœ¨imagenetè®­è¿‡ 2. down-sampleå’ŒstrideæŸå¤±ç©ºé—´ä¿¡æ¯ï¼Œå¤§ç›®æ ‡è¾¹ç•Œæ¨¡ç³Š 3. å°ç›®æ ‡ã€Œç©ºé—´åˆ†è¾¨ç‡ä½ã€

---

## TridentNet

> Scale variation $\to$ Different <u>**receptive fields**</u>  

å¤šåˆ†æ”¯ç½‘ç»œï¼Œåˆ†æ”¯ç»“æ„ç›¸åŒæƒé‡å…±äº«ï¼Œæ¯ä¸ªåˆ†æ”¯ä¸åŒçš„æ„Ÿå—é‡å¯¹åº”æ£€æµ‹ä¸åŒå°ºåº¦èŒƒå›´çš„ç‰©ä½“
ä¸åŒæ„Ÿå—é‡ä½¿ç”¨`dilated_conv`å®ç°ğŸ‘‰å‚æ•°ç›¸åŒ
æƒé‡å…±äº«ï¼šå‡å°‘å‚æ•°é‡ï¼Œinferenceæ—¶åªé€‰æ‹©ä¸€ä¸ªä¸»åˆ†æ”¯
![](Figures/ac7b0f86651341d53a16ca349d1b6deb5414d9c7.png)

<u>Image Pyramid</u> (Multi-scale training&testing): time-consuming
<u>Feature Pyramid</u>: use different params to predict different scale (not uniform) 

**trident block**ğŸ‘‡

![](Figures/118a7989f6f926f41e2581e0c102f9c6d3c665e0.png)**Scale-aware Training Scheme**ï¼šæ¯ä¸ªbranchåªå¯¹é•¿å®½åœ¨ä¸€å®šèŒƒå›´çš„proposalè¿›è¡Œè®­ç»ƒã€Œä¸€å¼ å›¾ç‰‡ä½¿ç”¨ä¸åŒbranch(ä¸åŒdilate rate)è®­ç»ƒä¸åŒå°ºåº¦çš„proposalã€
å…¶ä»–å‚æ•°ç›¸åŒ(make sense?) 

**é¢„æµ‹**ï¼šè®¡ç®—æ¯ä¸ªåˆ†æ”¯çš„é¢„æµ‹è¾“å‡ºï¼Œfilter outæ‰è¶…è¿‡å°ºå¯¸èŒƒå›´çš„box **TridentNet Fast**ï¼šé¢„æµ‹åªé‡‡ç”¨å•åˆ†æ”¯$\to$ä¸­é—´åˆ†æ”¯é¢„æµ‹ï¼Œå¾—ç›Šäºä¸‰åˆ†æ”¯æƒé‡å…±äº«ï¼Œæ•ˆæœæ¥è¿‘

---

## SNIPER: Efficient Multi-Scale Training

> è§£å†³å¤šå°ºåº¦é—®é¢˜, ä¸æ„å»ºfeature pyramid, <u>**å¤šå°ºåº¦è®­ç»ƒç­–ç•¥**</u>, å°ºå¯¸é€‚åº”ç½‘ç»œ

**Scale Invariant**: *RCNN*å°†proposalç¼©æ”¾åˆ°åŒä¸€ä¸ªå°ºåº¦ï¼Œæ£€æµ‹ç½‘ç»œåªéœ€è¦å­¦ä¹ ä¸€ç§å°ºåº¦çš„æ£€æµ‹ã€‚è€Œä¸ºäº†é€‚åº”ä¸åŒå°ºåº¦ï¼Œå¤šå°ºåº¦è®­ç»ƒçš„*Faster RCNN*å¯¹æ•´ä¸ªå›¾ç‰‡è¿›è¡Œæ”¾ç¼©ï¼Œproposalä¹Ÿæ”¾å¤§ç¼©å°ï¼Œæ£€æµ‹ç½‘ç»œå­¦ä¹ é€‚åº”å¤šç§å°ºåº¦ã€‚ <u>é€šè¿‡ç½‘ç»œcapacityè®°å¿†ä¸åŒscaleçš„ç‰©ä½“</u> ï¼Œæµªè´¹capacity

> *Process context regions around GT instances(chips) at appropriate scale*  
> æˆªå–å›ºå®šå°ºå¯¸çš„chip(eg `3x3, 5x5, 7x7`)å¯¹åº”ä¸åŒå°ºåº¦ï¼Œç„¶åresizeåˆ°ç›¸åŒå¤§å°(low-res)å»è®­ç»ƒ  
> å°ç›®æ ‡zoom-inï¼Œå¤§ç›®æ ‡zoom-out  

#### Pos chips

![](Figures/ff21eb5b057a444a8bb5207b3cc2b71dc5fdde7b.png)
ğŸ‘†chipä»æœ€å°çš„coveræŸä¸ªGT boxå¼€å§‹ï¼Œç›´åˆ°æœ€å¤šçš„boxè¢«è¿™ä¸ªchip coveråˆ°
ã€Œchipå°ºå¯¸ä¸å˜ï¼Œå›´ç»•coverè¿™ä¸ªGTboxè½¬ï¼Œç›´åˆ°æœ€å¤§åŒ–coverçš„boxæ•°é‡ã€

1. æ¯ä¸ªboxè‡³å°‘è¢«ä¸€ä¸ªchip cover
2. ä¸€ä¸ªç‰©ä½“å¯èƒ½è¢«å¤šä¸ªchip cover
3. ä¸€ä¸ªç‰©ä½“åœ¨ä¸åŒå°ºåº¦chipä¸­å¯èƒ½valid or not
4. æˆªæ–­çš„ç‰©ä½“ä¿ç•™

#### Neg chips

![](Figures/58422bc9dea8dd52de0f8214533a4c54c7d97cc6.png)ğŸ‘†åªæœ‰pos chipsä¼šå¯¼è‡´ç½‘ç»œåªå¯¹GTé™„è¿‘å°èŒƒå›´çš„å›¾ç‰‡è®­ç»ƒ *iconic*ï¼Œç¼ºä¹ <u>èƒŒæ™¯</u> ã€‚å¢åŠ **éš¾æ ·æœ¬**ä½œä¸ºneg chips
Metrics:

1. å¦‚æœåŒºåŸŸæ²¡æœ‰proposalï¼Œè®¤ä¸ºæ˜¯easy backgroundï¼Œå¿½ç•¥
2. å»æ‰è¢«pos chip coverçš„proposalã€Œproposalå’ŒGTæ¥è¿‘ï¼Œæ˜“äºåŒºåˆ†ã€
3. è´ªå¿ƒé€‰æ‹©è‡³å°‘cover Mä¸ªå‰©ä½™proposalçš„ä½œä¸ºneg chips

è®­ç»ƒæ—¶å¯æ§åˆ¶neg chipæ•°é‡ï¼Œç±»ä¼¼OHEM
<u>åˆ†è¾¨ç‡å’Œå‡†ç¡®ç‡å…³ç³»å¯èƒ½ä¸å¤§ï¼Œè¿‡å¤šcontextå¯èƒ½ä¸å¿…è¦</u>
Ref: [ç›®æ ‡æ£€æµ‹-SNIPER-Efficient Multi-Scale Training-è®ºæ–‡ç¬”è®° | arleyzhang](https://arleyzhang.github.io/articles/f0c1556d/)

---

## HRNet: Deep High-Resolution Representation Learning for Visual Recognition

> å¤„ç†è¿‡ç¨‹ä¸­ä¿æŒé«˜åˆ†è¾¨ç‡ã€Œposition-sensitive taskã€  
> maintain high-res representation through the whole process  
> ä¸åŒäºskip connectionï¼šé«˜åˆ†è¾¨åˆ†æ”¯å¹³è¡Œconvï¼Œé€šè¿‡fusionè€Œä¸æ˜¯addèåˆé«˜ä½åˆ†æ”¯ï¼Œå¤šåˆ†è¾¨ç‡è¾“å‡º  
> ä¸åŒäºç‰¹å¾é‡‘å­—å¡”ï¼šé«˜ä½åˆ†è¾¨ç‡å¹³è¡Œè®¡ç®—ï¼ˆlow-reså¢åŠ åˆ†è¾¨ç‡ä¸‹convè®¡ç®—ï¼Œä¸æ˜¯é€šè¿‡high-resä¸€æ¬¡å·ç§¯downsampleå¾—åˆ°ï¼Œ**é€æ­¥å¹³è¡Œ**è®¡ç®—å¢åŠ ï¼‰  

å…ˆå‰ç½‘ç»œï¼šencode high $\to$ lowï¼Œrecover low $\to$ high
æå‡ºç½‘ç»œï¼šè¿ç®—æ—¶**ä¿æŒé«˜åˆ†è¾¨ç‡**åˆ†æ”¯ï¼Œ**å¹³è¡Œ**çš„åŠ å…¥ä½åˆ†è¾¨ç‡åˆ†æ”¯ï¼›**multi-res fusion**

#### Parallel multi-res conv

![](Figures/24dc1e9896b61be4bef3511cd8a870ed0fd0e31c.png)ğŸ‘†æ¯ä¸ªstage <u>é€æ­¥åŠ å…¥ä¸€ä¸ªä½åˆ†è¾¨ç‡(eg 1/2)</u> åˆ†æ”¯ï¼Œä¸”ä¿æŒåŸæœ‰åˆ†è¾¨ç‡åˆ†æ”¯
ç±»ä¼¼ <u>group conv</u> ï¼Œé€šé“åˆ†åˆ« $\to$ åˆ†è¾¨ç‡åˆ†åˆ«

#### Repeated multi-res fusion

æ¯ä¸ªstage(4ä¸ªunit/block)äº¤æ¢ä¸åŒåˆ†è¾¨ç‡çš„ä¿¡æ¯
![](Figures/e4187388dd2d925fbbd3c4f199e3d5e8c830eb90.png)ğŸ‘†high $\to$ low: stride conv; low $\to$ high: bilinear upsampling + 1x1 conv
an <u>extra</u> output for lower res output![](Figures/7ac095e7bf586fccf3c9b8de1caf0658163c37eb.png)ğŸ‘†èåˆç±»ä¼¼FC

#### Multi-res representation head/ä¸åŒä»»åŠ¡ä¸åŒè¾“å‡ºæ¨¡å¼

![](Figures/a2036193f4b7513cffb8bb02a77b3e3cbf9ab787.png)(a) å…³é”®ç‚¹æ£€æµ‹ (b) segmentation (c) object detection

---

## Region Proposal by Guided Anchoring

> æ›´å¥½çš„anchorï¼Œ**æ”¹è¿›äº§ç”Ÿanchorçš„è¿‡ç¨‹**ã€Œéå¯†é“ºã€  
> anchorä¸feature: <u>**alignment**</u> + <u>**consistency**</u> 

ä¸¤ä¸ªåˆ†æ”¯åˆ†åˆ«å¯¹anchorçš„ä¸­å¿ƒç‚¹å’Œé•¿å®½è¿›è¡Œé¢„æµ‹ï¼Œé˜²æ­¢offsetåç§»è¿‡å¤§ï¼Œanchorå’Œç‚¹çš„featureä¸å¯¹åº”
é‡‡ç”¨**deformable conv**ä½¿featureçš„èŒƒå›´å’Œanchorçš„å½¢çŠ¶å¯¹åº”ï¼Œæ¯ä¸ªä½ç½®anchorå½¢çŠ¶ä¸åŒè€Œcaptureä¸åŒçš„ç‰¹å¾ã€ŒåŠ offsetä»¥é€‚åº”anchorå½¢çŠ¶ã€
![](Figures/ec5348cb3efb80de6adb90ab09fac616f97681bd.png)

#### Guided Anchoring

$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$
**åˆ†ä¸¤æ­¥äº§ç”Ÿanchor**ã€Œå‡å°åŒæ—¶é¢„æµ‹xywhæ—¶å‡ºç°çš„åç§»ä¸å¯¹åº”ã€

1. locationï¼šé¢„æµ‹objectnessï¼Œä¹‹åé‡‡ç”¨mask_conv **å‡å°‘åŒºåŸŸè®¡ç®—**
   åªå¯¹ç‰©ä½“çš„ä¸­å¿ƒ(åŠé™„è¿‘)ä¸ºposè®­ç»ƒï¼Œé¢„æµ‹ç‰©ä½“ä¸­å¿ƒã€Œè¾¹ç¼˜ä¸å®¹æ˜“å›å½’æ¡†ã€
2. shapeï¼šé¢„æµ‹æ¯ä¸ªä½ç½®ä¸Šçš„best shapeï¼Œä½ç½®ä¸å˜åªå˜é•¿å®½ï¼Œä¸ä¼šmisalign
   é¢„æµ‹$w=k\times e^{dw}$ï¼Œé¢„æµ‹dwï¼Œè€Œä¸æ˜¯wï¼ŒèŒƒå›´æ›´å¤§ğŸ‘‡
   $w=\sigma \cdot s \cdot e^{dw}, \; h=\sigma \cdot s \cdot e^{dh}$

é€‰æ‹©é«˜äºthreshçš„locationä¸­ï¼Œæ¦‚ç‡æœ€é«˜çš„shapeï¼Œäº§ç”Ÿanchor

#### Feature adaptation

**consistency**: æ¯ç‚¹å¯¹åº”çš„anchoré•¿å®½ä¸åŒï¼Œæ‰€ä»¥å­¦ä¹ åˆ°ç‰¹å¾å¯¹åº”åŒºåŸŸçš„é•¿å®½ä¹Ÿåº”è¯¥ä¸åŒ
$\mathbf{f}'_i=\mathcal{N}_T(\mathbf{f}_i,w_i,h_i)$
åŸºäºå¯¹åº”anchorçš„é•¿å®½ï¼Œæ”¹å˜ç‰¹å¾ï¼ˆxyä¸å˜ï¼Œä½ç½®branchåªé¢„æµ‹objectness scoreï¼‰
ğŸ‘†ä½¿ç”¨deformable convolutionå®ç°

#### Anchor shape target

è®­ç»ƒæ—¶anchorå’Œgt boxçš„åŒ¹é…ï¼Œè®­ç»ƒç›®æ ‡ã€‚whä¸ºå˜é‡ï¼Œæ— æ³•è®¡ç®—IoU

$\mathbf{vIoU}(a_{\mathbf{wh}},\mathbf{gt})=\max\limits_{w>0,h>0}{\mathbf{IoU}_{normal}(a_{\mathbf{wh}},\mathbf{gt})}$

æ–¹æ³•ï¼šSampleå¸¸è§çš„whç»„åˆï¼Œè®¡ç®—å’ŒGTçš„IoUï¼Œå¾—åˆ°vIoUğŸ‘†ï¼Œä½œä¸ºanchorå’Œgt IoUçš„ä¼°è®¡ï¼Œä¹‹åé‡‡ç”¨å¸¸è§anchoråˆ†é…æ–¹æ³•ç¡®å®šè®­ç»ƒç›®æ ‡

#### High quality proposal

ç”±äºç”Ÿæˆçš„anchoræ›´å¥½ï¼Œposæ ·æœ¬æ•°é‡æ›´å¤šã€‚è®­ç»ƒæ ·æœ¬åˆ†å¸ƒç¬¦åˆproposalåˆ†å¸ƒ
è®¾ç½® <u>æ›´é«˜æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹</u> ï¼ŒåŒæ—¶ <u>æ›´å°‘æ ·æœ¬</u> æ•°é‡ï¼Œå³ <u>æ›´é«˜IoU threshold</u>

---

## Soft NMS

> è§£å†³å¯†é›† _ç›¸é‚»_ ç‰©ä½“çš„æ£€æµ‹æ¡†é‡å IoUå¤§ï¼Œå¯èƒ½åœ¨NMSè¿‡ç¨‹ä¸­ <u>**è¯¯åˆ **</u>  
> å¯†é›†ç‰©ä½“æ£€æµ‹æœ‰æå‡  

#### NMS

æŒ‰ç…§ç½®ä¿¡åº¦æ’åºï¼Œé€‰æ‹©æœ€å¤§çš„box iä¿ç•™ã€‚å…¶ä½™boxä¸­ï¼Œä¸Içš„IoU>thresholdçš„åˆ é™¤(ç½®ä¿¡åº¦ç½®ä¸º0)ã€‚å†ä»å‰©ä¸‹boxé€‰æ‹©æœ€å¤§ä¿ç•™ï¼Œé‡å¤
![](Figures/98b46d0bb819b13145c34910d32fda6c602d7192.jpg)

#### Soft NMS

é‡å IoUè¶Šå¤§ï¼Œç½®ä¿¡åº¦ä¸‹é™è¶Šå¤š
ç½®ä¿¡åº¦ç½®ä¸º0å˜ä¸ºæ›´æ–°IoU>thresholdæ¡†çš„ç½®ä¿¡åº¦
![](Figures/ec46758beec67fa960bc978c8fa18304cceee89d.jpg)
æˆ–
![](Figures/89ba0e23898e18e892d209b76c3c2b6602fe3e0e.jpg)
Ref: [NMSä¸soft NMS - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/42018282)

---

## Matrix Nets: A New Deep Architecture for Object Detection (xNets)

> FPNå¤„ç†ä¸åŒå¤§å°çš„ç‰©ä½“(ç‰¹å¾é‡‘å­—å¡”)  
> ğŸ‘‡æœ¬æ–‡å¢åŠ ä¸åŒé•¿å®½æ¯”ç‰©ä½“çš„å¤„ç† (å¤§å°é‡‘å­—å¡”+aspect ratioé‡‘å­—å¡”)  

![](Figures/f96e9e976eb7720d970f1810c8396bf598967513.png)
é«˜åº¦ï¼Œå®½åº¦å‡åŠã€‚å·¦ä¸‹å³ä¸Šå‰ªæ(ç‰©ä½“ä¸å¸¸è§)
æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œç›¸æ¯”CenterNetå‚æ•°é‡å‡å°‘
Ref: [å‚æ•°å°‘ä¸€åŠã€é€Ÿåº¦å¿«3å€ï¼šæœ€æ–°ç›®æ ‡æ£€æµ‹æ ¸å¿ƒæ¶æ„æ¥äº†](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650768067&idx=2&sn=7133cf90287c91b297d857a57bdd6481&chksm=871a46bdb06dcfab0f4092acc96db5b8dc88c34937e7ea1a5e813e7d53a165d2549db0640068&scene=21#wechat_redirect)

---

## IoU-Net

**Add localization confidence in NMS**

#### NMS

1. é€‰æ‹©æœ€å¤§classification confidenceçš„æ¡†$b_j$ï¼ŒåŠ å…¥é›†åˆ$S$ã€‚
2. å…¶ä»–æ‰€æœ‰ä¸å†é›†åˆä¸­çš„æ¡†ï¼Œå¦‚æœå’Œ$b_j$çš„IoUå¤§äºthresholdï¼Œåˆ™åˆ å»ï¼Œç®€åŒ–é‡å¤æ¡†ã€‚
3. é‡å¤çŸ¥é“æ²¡æœ‰æ¡†ï¼Œ$S$ä¸ºç»“æœã€‚

ä½¿ç”¨åˆ†ç±»ç½®ä¿¡åº¦ä½œä¸ºæœ€å¼€å§‹é€‰æ‹©æ¡†çš„ä¾æ®ï¼ŒIoUç”¨äºè®¡ç®—åˆ†ç±»ç½®ä¿¡åº¦æœ€å¤§çš„æ¡†å’Œå…¶ä»–æ¡†ä¹‹é—´çš„é‡åˆåº¦ï¼Œåˆ å»æ¡†ã€‚

è€ŒIoU-Netä½¿ç”¨é¢„æµ‹çš„æ¡†å’ŒGTçš„é‡åˆIoUï¼Œå³å®šä½ç½®ä¿¡åº¦ï¼Œé€‰æ‹©æœ€å¤§ä½œä¸ºä¾æ®ã€‚åœ¨inferenceé˜¶æ®µå‘æŒ¥ä½œç”¨ã€‚

#### é¢„æµ‹IoU

![15689424793639](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689424793639.jpg)
é€šè¿‡ç½‘ç»œé¢„æµ‹IoUï¼šä½¿ç”¨FPNä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæç‰¹å¾ã€‚ä½¿ç”¨PrRoI poolingæ›¿ä»£RoI pooling

#### IoU Guided NMS

![15689426602716](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689426602716.jpg)
Rank all detection bbox on localization confidence.

#### Consider bounding box refinement as optimization

![15689722991024](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689722991024.jpg)
é€šè¿‡é¢„æµ‹IoUå¹¶äº§ç”Ÿæ¢¯åº¦ï¼Œæ›´æ–°bounding boxï¼Œå¹¶é€šè¿‡åˆ¤æ–­åˆ†æ•°çš„æå‡å’Œå·®å€¼æ¥æ›´æ–°è¾¹ç•Œæ¡†
// ToRead

#### Precise RoI pooling

ä½¿ç”¨åŒçº¿æ€§æ’å€¼æ¥è¿ç»­åŒ–ç‰¹å¾å›¾ï¼Œä»»æ„è¿ç»­åæ ‡(x,y)å¤„éƒ½æ˜¯è¿ç»­çš„
$f(x,y)=\Sigma_{i,j}IC(x,y,i,j)\times w_{i,j}$
$IC(x,y,i,j)=max(0,1-|x-i|)\times max(0,1-|y-j|)$æ˜¯æ’å€¼ç³»æ•°ï¼Œxyè¿ç»­ï¼Œijä¸ºåæ ‡åƒç´ ç‚¹ã€‚RoIçš„ä¸€ä¸ªbinè¡¨ç¤ºä¸ºå·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡å¯¹ã€‚é€šè¿‡äºŒé‡ç§¯åˆ†è¿›è¡Œæ± åŒ–ï¼ˆåŠ æƒæ±‚å’Œï¼‰
$PrPool(\{(x_1, y_1),\;(x_2, y_2)\},\;F)=\frac{\int_{y_1}^{y_2}\int_{x_1}^{x_2}f(x,y)dxdy}{(x_2-x_1)\times (y_2-y_1)}$

![15689719523883](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689719523883.jpg)

ä½¿ç”¨ResNet-FPNä½œä¸ºéª¨å¹²ç½‘ç»œï¼ŒRoI poolingæ¢æˆPrRoI poolingã€‚åŒæ—¶IoUé¢„æµ‹åˆ†æ”¯å¯ä»¥å’ŒR-CNNçš„åˆ†ç±»å’Œè¾¹ç•Œæ¡†å›å½’åˆ†æ”¯å¹¶è¡Œå·¥ä½œ.

---

## FreeAnchor: Learning to Match Anchors for Visual Object Detection

Related å¯¹äºanchorç”Ÿæˆ/åˆ†é…/é€‰æ‹©çš„æ”¹è¿›ï¼šGuided Anchoring, IoU-Net, MetaAnchor [MetaAnchor - ç®€ä¹¦](https://www.jianshu.com/p/a24d814613eb)

> å¯¹äºanchorå’Œobjectçš„<u>**åŒ¹é…æ–¹å¼**</u>çš„æ”¹è¿›ï¼ŒLearn to match 

ä¹‹å‰é‡‡ç”¨IoUæœ€å¤§çš„anchorè¿›è¡Œåˆ†é…ï¼šç»†é•¿ç‰©ä½“ï¼Œæœ€representativeçš„ç‰¹å¾ä¸åœ¨ç‰©ä½“ä¸­å¿ƒï¼ŒIoUæœ€å¤§$\neq$æœ€representative

Assignç­–ç•¥éœ€è¦æ»¡è¶³ï¼š

1. **Recall**: æ¯ä¸ªç‰©ä½“éƒ½èƒ½åˆ†é…ä¸€ä¸ªanchor

2. **Precision**: åŒºåˆ†background anchor

3. **Compatible NMS**: é«˜åˆ†ç±»åˆ†æ•°çš„anchoræœ‰å¥½çš„localization

**matchingè¿‡ç¨‹çœ‹ä½œMLEè¿‡ç¨‹**ï¼Œæ¯ä¸ªç‰©ä½“ä»bag of anchorä¸­é€‰likelihood probabilityæœ€å¤§çš„

#### Maximum Likelihood Estimationåˆ†æç°æœ‰detector

è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œ$C_{i,j}$è¡¨ç¤ºj anchorå’Œi ç‰©ä½“åŒ¹é…ã€Œassign using IoU criterionã€$\mathcal{L}(\theta)=\sum\limits_{a_j\in A_+}\sum\limits_{b_i\in B}C_{i,j}\mathcal{L}_{i,j}^{cls}(\theta)+\beta\sum\limits_{a_j\in A_+}\sum\limits_{b_i\in B}C_{i,j}\mathcal{L}_{i,j}^{loc}(\theta)+\sum\limits_{a_j\in A_-}\mathcal{L}_j^{bg}(\theta)$

æŠŠè®­ç»ƒæŸå¤±å‡½æ•°çœ‹ä½œä¼¼ç„¶æ¦‚ç‡

$\begin{aligned}
\mathcal{P}(\theta) &=\mathbf{e^{-\mathcal{L}(\theta)}} \\
&=\prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} e^{-\mathcal{L}_{i j}^{c l_{j}}(\theta)}\right) \prod_{a_{j} \in B_{+}}\left(\sum_{b_{i} \in B} C_{i j} e^{-\beta \mathcal{L}_{j_{j}}^{l o c}(\theta)}\right) \prod_{a_{j} \in A_{-} \atop a_{j} \in A_{-}} \mathcal{P}_{j}^{b g}(\theta) \\
&=\prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} \mathcal{P}_{i j}^{c l s}(\theta)\right) \prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} \mathcal{P}_{i j}^{l o c}(\theta)\right) \prod_{a_{j} \in A_{-}} \mathcal{P}_{j}^{b g}(\theta)
\end{aligned}$

<u>æ˜ å°„éå¸¸å·§å¦™ï¼Œä½¿$[0,+\infty)$çš„æŸå¤±æ˜ å°„åˆ°$(0,1]$ï¼Œè€Œä¸”æŸå¤±è¶Šå°ï¼Œ$\mathcal{P}(\theta)$è¶Šå¤§</u>

å› æ­¤ï¼Œæœ€å°åŒ–æŸå¤±çš„ç›®æ ‡è½¬æ¢ä¸ºæœ€å¤§åŒ–ä¼¼ç„¶æ¦‚ç‡

#### æ”¹è¿›detectionä¼¼ç„¶å‡½æ•°

ç›®æ ‡ recallï¼Œprecisionï¼Œcompatible

**Recall**ï¼šæ¯ä¸ªobjæ„å»ºbag of anchorï¼Œæœ€å¤§åŒ–å…¶ä¸­anchorçš„clså’Œlocä¼¼ç„¶ã€‚æ¯ä¸ªobjä¸€å®šå­˜åœ¨ä¸€ä¸ªanchorå¯¹åº”

$\mathcal{P}_{\text {recall}}(\theta)=\prod_{i} \max _{a_{j} \in A_{i}}\left(\mathcal{P}_{i j}^{c l s}(\theta) \mathcal{P}_{i j}^{l o c}(\theta)\right)$

**Precision**ï¼šå³å¯¹anchoråŒºåˆ†å‰èƒŒæ™¯ï¼ŒæŠŠèƒŒæ™¯anchoråˆ†å‡º

$\mathcal{P}_{\text {precision}}(\theta)=\prod_{i}\left(1-P\left\{a_{j} \in A_{-}\right\}\left(1-\mathcal{P}_{j}^{b g}(\theta)\right)\right)$

å…¶ä¸­$P\{a_{j} \in A_{-}\}=1-\max\limits_iP\{a_j\to b_i\}$ è¡¨ç¤ºanchor jä¸matchä»»ä½•ç‰©ä½“ã€‚å³anchorä¸matchä»»ä½•objæ¦‚ç‡è¶Šé«˜ï¼Œanchorä¸å±äºèƒŒæ™¯çš„æ¦‚ç‡è¶Šä½(1-)ï¼Œæ‰å¯ä»¥æœ€å¤§$\mathcal{P}_{\text {precision}}(\theta)$

**Compatible**: $P\{a_j\to b_i\}$è¡¨ç¤ºj anchoråŒ¹é…i objæ¦‚ç‡ï¼ŒNMSæŒ‰ç…§clsåˆ†æ•°é€‰ã€‚æ‰€ä»¥æ”¹æˆlocåˆ†æ•°ã€Œi j çš„IoUã€è¶Šå¤§ï¼ŒåŒ¹é…æ¦‚ç‡è¶Šé«˜ï¼ŒPä¸ºå…³äºIoUçš„<u>saturated linear</u>å‡½æ•°ã€‚æ­¥éª¤å­˜åœ¨äº$\mathcal{P}_{\text {precision}}(\theta)$ä¸­

![](Figures/2020-02-16-19-21-00-image.png)

æ¨ªåæ ‡ä¸ºIoU

**ä¼¼ç„¶å‡½æ•°**:    Jointly maximize

$\mathcal{P}'(\theta)=\mathcal{P}_{recall}(\theta)\times\mathcal{P}_{precision}(\theta)$

#### æ”¹è¿›ä¼¼ç„¶å‡½æ•°æ¨å‡ºMatching Mechanism

è®­ç»ƒæŸå¤±$\mathcal{L}'(\theta)=-\log\mathcal{P}'(\theta)$ï¼Œä½¿ç”¨FocalLoss

å…¶ä¸­æœ‰maxæ“ä½œï¼Œä½†éšæœºåˆå§‹åŒ–çš„ç½‘ç»œï¼Œæ‰€æœ‰anchorå¾—åˆ†éƒ½ä½ï¼Œmaxæ²¡æœ‰æ„ä¹‰

æ”¹ç”¨**Mean-max**å‡½æ•°ï¼š$\operatorname{Mean}-\max (X)=\frac{\sum_{x_{j} \in X} \frac{x_{j}}{1-x_{j}}}{\sum_{x_{j} \in X} \frac{1}{1-x_{j}}}$

è®­ç»ƒä¸å……åˆ†æ—¶æ¥è¿‘meanï¼Œä½¿ç”¨bagä¸­æ‰€æœ‰anchorè®­ç»ƒ

è®­ç»ƒå……åˆ†æ—¶æ¥è¿‘maxï¼Œé€‰æ‹©æœ€å¥½çš„anchorè®­ç»ƒ

![](Figures/2020-02-16-20-55-42-image.png)

å¯è§†åŒ–ï¼Œanchor assign confident (laptop)

![](Figures/2020-02-16-20-57-56-image.png)

ç›¸æ¯”baselineæœ‰æå‡3%. ä½¿ç”¨ResNeXt-64x4d-101ï¼Œ**ä¸ºmulti-scale

![](Figures/2020-02-16-21-02-22-image.png)

Ref: [https://www.aminer.cn/research_report/5dedbde4af66005a4482453f?download=false](https://www.aminer.cn/research_report/5dedbde4af66005a4482453f?download=false)

---

## å¯†é›†å°ç›®æ ‡

Paper: <u>Benchmark for Generic Product Detection: A strong baseline for Dense Object Detection</u>

![](Figures/2020-02-21-21-03-09-image.png)

<u>Scale Match for Tiny Person Detection</u>Â Â Â Â [method+dataset]

---

## Aligndet: Revisiting Feature Alignment for One-stage Object Detection

---

## FPN & Variants

æ‰€æœ‰FPNéƒ½ä½¿ç”¨backboneçš„å¤šå±‚ç‰¹å¾å›¾ï¼ˆç»è¿‡`1x1`å·ç§¯ï¼‰ğŸ‘‡$C_{2..5}$

![](Figures/2020-02-22-16-21-13-image.png)

#### Top-down FPN

ç»å…¸FPNï¼Œä»æœ€é«˜å±‚ç‰¹å¾(semanticï¼Œlow-res)ç»è¿‡upsampleï¼Œå’Œå„åŒä¸€çº§çš„ç‰¹å¾å›¾ç›¸åŠ 

ç»™åº•å±‚ç‰¹å¾å¼•å…¥é«˜å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œç›Šäºå°ç›®æ ‡æ£€æµ‹ï¼ˆä½å±‚ç‰¹å¾å›¾ï¼‰

![](Figures/2020-02-22-16-25-08-image.png)

å…¬å¼$F_{i}^{t}=\mathbf{W}_{i+1}^{\mathrm{t}} \otimes\left(U\left(F_{i+1}^{t}\right)+C_{i}\right)$

#### Bottom-up FPN

ä»æœ€åº•å±‚(high-res)å‘ä¸Šé€æ¬¡äº§ç”ŸFPNå±‚ï¼Œå‘é«˜å±‚ç‰¹å¾å›¾ä¼ æ’­ä½å±‚çš„ç©ºé—´ç»†èŠ‚ä¿¡æ¯(spatial)

ä»ä½åˆ°é«˜ï¼Œèåˆã€Œæœ¬å±‚ç‰¹å¾ï¼Œé«˜ä¸€å±‚ç‰¹å¾ï¼Œä¸Šä¸€å±‚FPNã€

![](Figures/2020-02-22-16-42-55-image.png)

å…¬å¼$F_{i}^{b}=\mathbf{W}_{\mathbf{i}}^{\mathbf{b}} \otimes\left(D\left(F_{i-1}^{b}\right)+C_{i}+U\left(C_{i+1}\right)\right)$

$D\to$ downsample, $U\to$ upsample

#### Fusing-splitting FPN

ä¸Šè¿°ä¸¤ä¸ªFPNé¡ºåºé€æ¬¡äº§ç”Ÿï¼Œå…ˆäº§ç”Ÿçš„å±‚ä¼šå¯¹ä¹‹åå±‚å½±å“(unfair)

é¦–å…ˆåˆ†ç»„**fuse**é«˜å±‚å’Œä½å±‚çš„ä¸´è¿‘ä¸¤ç»„ç‰¹å¾

$\alpha_s=C_4+U(C_5), \alpha_l=D(C_2)+C_3$

ç„¶å**merge**é«˜å±‚å’Œä½å±‚çš„ç‰¹å¾

$\beta_s=\mathbf{W_s^f}\otimes \mathrm{cat}(\alpha_s,D(\alpha_l))$

$\beta_l=\mathbf{W_l^f}\otimes \mathrm{cat}(U(\alpha_s),\alpha_l)$

å†**split**äº§ç”Ÿä¸åŒå±‚çš„ç‰¹å¾

$F_2^f=U(\beta_l), F_3^f=\beta_l$

$F_4^f=\beta_s, F_5^f=D(\beta_s)$

![](Figures/4acfff9cac36f72acc7a0cc716f33e27be53fd47.png)

Ref: *MFPN: A NOVEL MIXTURE FEATURE PYRAMID NETWORK OF MULTIPLE
ARCHITECTURES FOR OBJECT DETECTION*

---

# 
