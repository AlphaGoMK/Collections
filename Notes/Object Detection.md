# Object Detection

## mAP

> AP is averaged over all categories. Traditionally, this is called â€œmean average precisionâ€ (mAP). We make no distinction between AP and mAP (and likewise AR and mAR) and assume the difference is clear from context.  

[mAP (mean Average Precision) for Object Detection - Jonathan Hui - Medium](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)

---

## CornerNet

> anchor-free

#### Motivation

1. anchor boxå¤ªå¤šï¼Œåªæœ‰å°‘éƒ¨åˆ†å’ŒGTé‡åˆ
2. anchor é€‰æ‹©éœ€è¦äººä¸ºè®¾è®¡ï¼ˆæ•°é‡ï¼Œå°ºå¯¸ï¼Œæ¯”ä¾‹ï¼‰ï¼Œä¸åŒå°ºåº¦anchorè®¾ç½®ä¸åŒ

ä¸¤ä¸ªéƒ¨åˆ†ï¼š 1) æ£€æµ‹è§’ç½‘ç»œï¼Œå·¦ä¸Š+å³ä¸‹ 2) åµŒå…¥ç½‘ç»œï¼Œç”¨äºåŒ¹é…è§’ç‚¹

#### Method

![](Figures/1e682da0e78dfe82e953f541d1580f86f14af8f6.png)

**æ£€æµ‹cornerç½‘ç»œ**ï¼šæç‰¹å¾ä¹‹åï¼Œç»è¿‡corner poolingäº§ç”Ÿæ¯ä¸ªç±»åˆ«çš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„heat mapã€‚ä¸ºæ­£è´Ÿæ ·æœ¬åŒ¹é…ï¼Œåªæƒ©ç½šGTä¸€å®šèŒƒå›´å¤–çš„é¢„æµ‹ç‚¹ï¼ˆé€šè¿‡IoU thresholdé™åˆ¶radiusï¼‰
**è®¡ç®—åµŒå…¥embedç½‘ç»œ**ï¼šç”¨äºåŒ¹é…åŒæ¡†çš„å·¦ä¸Šå³ä¸‹ã€‚æŸå¤±å‡½æ•°ï¼šåŒæ¡†å·¦ä¸Šå’Œå³ä¸‹æ¥è¿‘ï¼ˆvarianceå°ï¼‰ï¼Œä¸åŒæ¡†çš„å¹³å‡embedè·ç¦»å¤§ã€‚
<u>**Corner pooling** </u>ï¼šè§£å†³è§’ç‚¹ç‰¹å¾å°‘ï¼Œå–ä¸€æ¡çº¿ä¸Šçš„æœ€å¤§å€¼pooling
![](Figures/63eebde8b0adc4b30eacb0eb14e2b291b78b0e87.png)
*Hourglass network*ï¼šæç‰¹å¾

**two stageå¾—åˆ°é¢„æµ‹æ¡†ä¹‹åé‡‡ç”¨RoI pooling/alignæå–æ£€æµ‹æ¡†å†…éƒ¨çš„ä¿¡æ¯ï¼Œåªæœ‰å†…éƒ¨ä¿¡æ¯(approx.)çš„ç‰¹å¾å†è¿›è¡Œä¸€æ¬¡æ¡†å®šå’Œåˆ†ç±»ï¼ˆrefineæ­¥éª¤ï¼‰ï¼›è€Œä¸€é˜¶æ®µçš„æ–¹æ³•åœ¨æå–åˆ°ç‰¹å¾ä¹‹ååˆ†æˆä¸¤æ”¯è¿›è¡Œæ¡†å®šå’Œåˆ†ç±»ï¼Œæ²¡æœ‰å¯¹è¿‘ä¼¼çš„åªåŒ…å«æ¡†å†…ç‰©ä½“çš„ç‰¹å¾(å±€éƒ¨ç‰¹å¾)è¿›è¡Œå†ä¸€æ¬¡æå–ï¼Œæ‰€ä»¥ç²¾åº¦è¾ƒå·®**

-----

## CenterNet

> anchor-free

#### Motivation

CornerNetåªç”¨åˆ°è¾¹ç¼˜çš„ç‰¹å¾ä¿¡æ¯ï¼Œæ²¡æœ‰ç”¨åˆ°å†…éƒ¨çš„ç‰¹å¾ä¿¡æ¯ï¼ˆé€ æˆä¸æ­¢å¯¹ç‰©ä½“çš„è¾¹ç¼˜æ•æ„Ÿï¼Œä¹Ÿå¯¹èƒŒæ™¯çš„è¾¹ç¼˜æ•æ„Ÿï¼‰ã€‚å†…éƒ¨ä¿¡æ¯å¯¹äºå†³å®šä¸¤ä¸ªkeypointæ˜¯å¦æ˜¯åŒä¸€ä¸ªæ¡†æœ‰å¸®åŠ©ã€‚
CenterNeté¢„æµ‹ä¸‰å…ƒç»„ï¼šå·¦ä¸Šï¼Œå³ä¸‹ï¼Œä¸­å¿ƒ

#### Method

![](Figures/d9ab741df8f3e313182e9e5f3f47cc6d89eba5c4.png)
äºŒåˆ†æ”¯ï¼šäº§ç”Ÿcornerç‚¹å¹¶matchå½¢æˆæ¡†ï¼›äº§ç”Ÿcenterç‚¹ã€‚å¦‚æœcenterç‚¹åœ¨æ¡†çš„central regionï¼Œè®¡ç®—æ¡†ï¼Œå¦åˆ™åˆ é™¤
<u>**central region**</u> ç¡®å®šåˆ¤å®šçš„ä¸­å¿ƒåŒºåŸŸçš„å¤§å°ï¼šå¤§æ¡†åå°ï¼Œå°æ¡†åå¤§ï¼Œæ•´ä½“çº¿æ€§

$\left\{\begin{array}{l}
{\operatorname{ctl}_{\mathrm{x}}=\frac{(n+1) \mathrm{tl}_{\mathrm{x}}+(n-1) \mathrm{br}_{\mathrm{x}}}{2 n}} \\
{\operatorname{ctl}_{\mathrm{y}}=\frac{(n+1) \mathrm{tl}_{\mathrm{y}}+(n-1) \mathrm{br}_{\mathrm{y}}}{2 n}} \\
{\operatorname{cbr}_{\mathrm{x}}=\frac{(n-1) \mathrm{tl}_{\mathrm{x}}+(n+1) \mathrm{br}_{\mathrm{x}}}{2 n}} \\
{\operatorname{cbr}_{\mathrm{y}}=\frac{(n-1) \mathrm{tl}_{\mathrm{y}}+(n+1) \mathrm{br}_{\mathrm{y}}}{2 n}}
\end{array}\right.$
Nç¦»æ•£å˜åŒ–ï¼Œè¿‡thresholdåå˜ç³»æ•°nã€‚å°$\to$threshold$\to$å¤§ï¼Œ3$\to$5

#### Enrich center&corner information

<u>**center pooling**</u> ç”¨åœ¨é¢„æµ‹ä¸­å¿ƒç‚¹æ—¶ï¼Œå¢åŠ ä¸­å¿ƒç‚¹çš„recognizableç‰¹å¾

```
ä¾‹å¦‚æ‰¾leftmostçš„ï¼ˆå³æŠŠhorizontalæœ€å¤§å€¼ä¼ åˆ°æœ€å·¦è¾¹ï¼‰ï¼Œæ¯ä¸ªç‚¹çœ‹è‡ªå·±åˆ°æœ€å³è¾¹çš„æœ€å¤§å€¼ï¼Œä¸æ–­ä¼ ï¼Œåˆ°æœ€å·¦å¯è·å¾—æ•´æ¡æ¨ªçº¿æœ€å¤§ï¼›æ‰¾topmostï¼ˆæŠŠverticalæœ€å¤§å€¼ä¼ åˆ°æœ€ä¸Šé¢ï¼‰ï¼Œæ¯ä¸ªç‚¹çœ‹è‡ªå·±åˆ°æœ€ä¸‹ï¼Œä¸æ–­ä¼ ï¼Œåˆ°æœ€ä¸Šåˆ™å¯è·å¾—æ•´æ¡çº¿æœ€å¤§
```

æ¨¡å—&ç¤ºæ„å›¾
![](Figures/ade09a091bb57b300358c745de23118416db2913.png)
![](Figures/ca0373639965a7c36e025eeef214af59716f68eb.png)

è¾“å‡ºmapè¡¨ç¤ºæ˜¯å¦ä¸ºcenterç‚¹ï¼Œç„¶åæ‰¾æ¨ªå‘å’Œçºµå‘æœ€å¤§å€¼

<u>**cascade corner pooling**</u> å¢åŠ è§’ç‚¹çš„ç‰¹å¾ï¼Œç›¸æ¯”corner poolingå¢åŠ å†…éƒ¨ï¼Œä½¿å…¶ä¸å¯¹è¾¹ç¼˜æ•æ„Ÿ
æ²¿ç€è¾¹ç¼˜æ‰¾è¾¹ç¼˜æœ€å¤§å€¼ï¼Œå†ä»è¾¹ç¼˜æœ€å¤§å€¼çš„ä½ç½® *å‘å†…æ‰¾å†…éƒ¨æœ€å¤§å€¼* ï¼Œæœ€åä¸¤ä¸ªæœ€å¤§å€¼ç›¸åŠ 
æ¨¡å—&ç¤ºæ„å›¾
![](Figures/cc6e39c86f14d245507fb1c4c2f691d87dc5a3c7.png)![](Figures/caa1d357e9dd10d63c81d52dcd69b0d045d9b9a3.png)

> Q: how to classification?  
> Need RoI align?  

---

## FCOS: Fully Convolutional One-Stage Object Detection

> anchor-free æ¶ˆé™¤anchorï¼Œå‡å°‘IoUçš„è®¡ç®—å’ŒGTæ¡†çš„åŒ¹é…ã€‚å¯ä»¥ä»£æ›¿äºŒé˜¶æ®µçš„RPN

æŒ‰ç…§åƒç´ è¿›è¡Œé¢„æµ‹<u>**per-pixel prediction**</u>ï¼Œé¢„æµ‹æ¯ä¸€ä¸ªåƒç´ ç‚¹çš„å››ä¸ªç»´åº¦çš„æ¡† `(Left, Top, Right, Bottom)`ğŸ‘‡
![](Figures/433cf8512698164d44e845d1d67fcaf628708061.png)

* å¯¹äºç‰¹å¾å›¾ä¸Šæ¯ä¸€ä¸ªç‚¹ï¼Œå¯¹åº”ä¸€ä¸ªåŸå›¾ä¸Šçš„æ¡†ã€‚ç›´æ¥æŠŠç‰¹å¾å›¾ä¸Šçš„åƒç´ ç‚¹çœ‹æˆè®­ç»ƒæ ·æœ¬è€Œä¸æ˜¯åœ¨ç‚¹ä¸Šé“ºä¸åŒé•¿å®½æ¯”å’Œå¤§å°çš„anchoræ¡†

* å¯¹äºä¸€ä¸ªç‚¹è½åœ¨å¤šä¸ªGTæ¡†ä¸­ï¼ˆambiguous samplesï¼‰é€‰æ‹©æœ€å°çš„bboxä½œä¸ºtargetã€‚åŒæ—¶é€šè¿‡multi-level predictionæ¥å‡å°‘æ•°é‡ã€‚ğŸ‘‡
  ![](Figures/376354c1dd3cfbe7facc6debbe8335f845c44e19.png)ğŸ‘†which bbox this location should regressï¼Ÿ

* FCOSå¯ä»¥åˆ©ç”¨å°½å¯èƒ½å¤šçš„æ ·æœ¬ï¼ˆç‰¹å¾å›¾ä¸Šçš„ç‚¹å½¢æˆçš„æ¡†ï¼‰è€Œä¸åªæ˜¯IoUè¶³å¤Ÿå¤§çš„anchor æ¥è¿›è¡Œè®­ç»ƒã€‚æ¯ä¸ªç‚¹éƒ½å»å­¦ä¹ å¯¹æ¡†çš„é¢„æµ‹ï¼Œå¤šä¸ªç‚¹å…±åŒäº§ç”Ÿå¤šä¸ªç±»ä¼¼çš„æ¡†ï¼Œç„¶åNMSé€‰æ‹©æœ€å¤§çš„

* å¯¹äºambiguous samplesï¼Œé‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾å›¾ï¼Œæ¯å±‚é™åˆ¶4D vecä¸­æœ€å¤§å€¼çš„å¤§å°ï¼ˆ<u>**é™åˆ¶æ¯å±‚ç‰¹å¾å›¾äº§ç”Ÿçš„bboxçš„å¤§å°**</u>ï¼‰ï¼Œæ»¡è¶³$m_i < \max(l,t,r,b) < m_{i+1}$ã€‚å¯¹äºåŒä¸€ä¸ªç‚¹ä¸Šå¤šä¸ªæ¡†ï¼Œå› é™åˆ¶ï¼Œæ‰€ä»¥åœ¨ä¸åŒå°ºåº¦ç‰¹å¾å›¾ä¸Šæ„æˆçš„æ¡†è¿›è¡Œregressï¼Œä¸€ä¸ªfeature mapä¸Šä¸€ä¸ªç‚¹åªè´Ÿè´£å›ºå®šå°ºåº¦çš„æ¡†å›å½’ã€‚å¦‚æœè¿˜å‡ºç°é‡å¤ï¼Œåˆ™é€‰æ‹©å°ºå¯¸æœ€å°

* é˜²æ­¢è¿œç¦»ç‰©ä½“ä¸­å¿ƒçš„ç‚¹äº§ç”Ÿè´¨é‡å·®çš„æ¡†ï¼Œcenter-lossã€‚$centerness^*=\sqrt{\frac{\min(l^*,r^*)}{\max(l^*,r^*)}\times \frac{\min(t^*,b^*)}{\max(t^*,b^*)}}$
  
  ä½¿<u>**å·¦å’Œå³ï¼Œä¸Šå’Œä¸‹çš„é•¿åº¦å°½å¯èƒ½ç›¸ç­‰**</u>ã€‚æµ‹è¯•æ—¶centerness-weighted classification confidenceï¼ŒæŠ‘åˆ¶åè¿œæ¡†ğŸ‘‡
  ![](Figures/d39a52432c2c3225142d3500869baf40e3c73b58.png)
  ç½‘ç»œç»“æ„ğŸ‘‡
  ![](Figures/8b5702a9ed96bc002a55b079d5b531be343ca142.png)

- - - -

## CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection

![image-20200428214951683](Figures/image-20200428214951683.png)

---

## SaccadeNet: A Fast and Accurate Object Detector

![image-20200428220100664](Figures/image-20200428220100664.png)

åŒæ—¶é¢„æµ‹ä¸­å¿ƒ`Center Attentive Module`å’Œè§’ç‚¹`Attention Transitive Module`ï¼Œå¾—åˆ°ç²—æ¡†ï¼Œä½¿ç”¨`Aggregation Attentive Module`åŒçº¿æ€§æ’å€¼é‡æ–°é‡‡æ ·feature mapï¼Œå¾—åˆ°ç²¾ç»†æ¡†ï¼Œ<u>è½»é‡çº§è¾¹æ¡†ç»†åŒ–</u>ã€‚`Corner Attentive Module`è¾…åŠ©è®­ç»ƒã€‚

`Center-AM`<u>è·ç¦»æƒ©ç½š</u>è®­ç»ƒï¼Œé‡‡ç”¨Gaussian heat mapä½œä¸ºGT $e^{\frac{\left\|X-X_{k}\right\|^{2}}{2 \sigma^{2}}}$

ç›¸æ¯”CornerNetå¢åŠ äº†ä¸­å¿ƒç‰¹å¾ï¼Œç›¸æ¯”FCOSå¢åŠ äº†è¾¹ç¼˜ç‰¹å¾ï¼Œç›¸æ¯”CenterNetåŠ é€Ÿ

## Mask RCNN

å‚è€ƒ[https://zhuanlan.zhihu.com/p/37998710](https://zhuanlan.zhihu.com/p/37998710)

#### ç½‘ç»œç»“æ„

![](Figures/2020-02-17-23-33-00-image.png)

#### RoI Align

![](Figures/2020-02-17-23-32-25-image.png)

![](Figures/2020-02-17-23-33-41-image.png) ğŸ‘†lossè®¡ç®—æ—¶`w*h*c`çš„maskè¾“å‡ºï¼Œåªè®¡ç®—<u>åˆ†ç±»åˆ†æ”¯é¢„æµ‹çš„ç±»åˆ«</u>å¯¹åº”channelçš„sigmodè¾“å‡ºä½œä¸ºæŸå¤±ã€Œ**è¯­ä¹‰maské¢„æµ‹ä¸åˆ†ç±»é¢„æµ‹è§£è€¦**ã€

---

## HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions

> light-weight 

åŒ…å«ä¸¤ä¸ªéƒ¨åˆ† _spatial contraction-expansion_ å’Œ _channel expansion-contraction_ ï¼Œç‹¬ç«‹ä½œç”¨åœ¨ç‰¹å¾å›¾çš„ orthogonal dimensionã€‚å‰è€…é€šè¿‡å‡å°‘ç‰¹å¾å›¾å¤§å°å‡å°‘è®¡ç®—é‡ï¼Œåè€…é€šè¿‡æå‡informative featureæå‡æ€§èƒ½

> mobileneté€šè¿‡åˆ†ç¦»æˆ*point-wise*å’Œ*depth-wise*æ¥åˆ†åˆ«ä¸å˜å°ºå¯¸å˜é€šé“æ•°å’Œä¸å˜é€šé“æ•°å˜å°ºå¯¸(up/down sampling)     so called *depthwise separable conv*  
> shuffleneté€šè¿‡group convå‡å°‘é€šé“ä¸Šçš„è®¡ç®—é‡ï¼Œchannel-shuffleæ¥å¢åŠ ä¸åŒé€šé“ä¹‹é—´çš„è¿æ¥  
> ğŸ‘†previous work focus on **channel transformation**, introduce **spatial feature dim(size)**  

* æå‡é€šé“æ•°å¯ä»¥æå‡ä¿¡æ¯ï¼Œä½†æ˜¯å¢åŠ è®¡ç®—å¼€é”€. æå‡º*Two reciprocal components work on orthogonal dimension*.<u>**é€šé“æ•°æ‰©å±•æ—¶ï¼Œå°ºå¯¸å‡å°‘**</u>
* ç›¸æ¯”mobilenetå¢åŠ **spatial contractionç¼©å°-expansionæ”¾å¤§åˆ°è¾“å…¥çš„å¤§å°**ï¼Œç±»ä¼¼Squeeze-Excitation Networkçš„å…ˆç¼©åæ”¾çš„æ€è·¯
  æ¨¡å—ï¼Œå¯¹æ¯”mobilenetğŸ‘‡
  ![](Figures/7a0feef0d97e42bab4044301662546ba8c93851c.png)
  è®¡ç®—é‡ğŸ‘‡
  ![](Figures/a43e53f7f1c22a59e7b1079ac38e14d39292c07b.png)
* å¢åŠ residual pathï¼Œå‡å°‘ä¸»å¹²è®¡ç®—and **feature reuse**. Inverted residual with harmonious bottleneckğŸ‘‡
  ![](Figures/6c1529ac24e533703ca3c5dcdc70f721512e2ac1.png)
* For Object detection: use **MobileNet V2 SSD** utilize the **warm-up** strategy which linearly ramps up the learning rate from a close-to-zero one 1e-6 to the normal initial learning rate of 1e-3 during the first 5 epochs. 

---

### SSDç½‘ç»œæ—¶é—´

1. ç½‘ç»œè¿è¡Œæ—¶é—´ï¼š0.002-0.003s åœ¨GPUè¿è¡Œ
2. detectï¼ˆNMSä¸ºä¸»ï¼‰è¿è¡Œæ—¶é—´ï¼š0.013-0.016s åªåœ¨ _CPU_ ä¸Šè¿è¡Œ <u>**ç“¶é¢ˆ**</u>

å°ºåº¦å‡å°‘ï¼Œaspect ratioå‡å°‘
Shallow feature map only for small
Deep ONLY large
ä½ç§©ç®€åŒ–
PointRend

<u>**ç‰¹å¾èåˆ
cascade**</u>

---

## Cascade RPN

<u>Single anchor per location + multi-stage refinement</u>

> ä¸€æ¬¡å›å½’ä¸åˆ°ï¼ˆè·ç¦»å¤ªè¿œï¼‰ï¼Œå¤šæ¬¡å›å½’
>
> å›å½’å¤šæ¬¡åanchorç‚¹å¤„çš„ç‰¹å¾å’Œç§»åŠ¨anchoræ‰€åœ¨ä½ç½®ç‰¹å¾ä¸åŒ¹é…$\to$deformable conv
>
> åŒ¹é…çš„anchorä½ç½®ä¸å˜(è¿˜æ˜¯æœ€åˆå§‹ç‚¹å¯¹åº”çš„anchor)ï¼Œä½†æ˜¯æå–ç‰¹å¾çš„ä½ç½®æ”¹å˜ğŸ‘‡ğŸ‘‡

![å›¾åƒ](Figures/%E5%9B%BE%E5%83%8F.jpeg)

#### Motivation

predefined anchoråœ¨GTå’Œanchorå¯¹é½æ—¶é™åˆ¶æ€§èƒ½/åå·®ï¼Œ(#toread RoIPool RoIAlign)

1. **single anchor** + incorporates **criteria** of anchor/anchor-free in defining **positive** boxes
2. **adaptive convolution** to maintain the alignment between anchor boxes and features

Iterative RPNæ¯æ¬¡æŠŠanchoré›†åˆ**çœ‹ä½œæ–°çš„anchor**è¿›è¡Œrefineï¼Œå¯¼è‡´æ¯æ¬¡è¿­ä»£åanchorä½ç½®å’Œå½¢çŠ¶å‘ç”Ÿå˜åŒ–ï¼Œanchorå’Œè¡¨ç¤ºanchorç‰¹å¾ä¸åŒ¹é…ã€Œ **anchorä¸­å¿ƒç‚¹çš„ç‰¹å¾(å³è¡¨ç¤ºanchorçš„ç‰¹å¾)ä¸å‘ç”Ÿå˜åŒ–ï¼Œä½†æ˜¯anchorçš„ä½ç½®å‘ç”Ÿå˜åŒ–** ï¼Œmismatchã€
ğŸ‘†ä½¿ç”¨deformable convè§£å†³ï¼Œbut _no constraint to enforce_ ğŸ™…â€â™‚ï¸

#### Adaptive Convolution

å·ç§¯é‡‡æ ·çš„æ—¶å€™å¢åŠ offset field
`offset = center offset + shape offset`ä¸­å¿ƒçš„åç§»å’Œå½¢çŠ¶åç§»(ç”±anchorå½¢çŠ¶å’Œkernelå†³å®š)

![](Figures/d5cd3e96c3f74c043b11af85568fd780d34c03b6.png)

ğŸ‘†å¯¹æ¯”deformable convï¼šåç§»é‡ç”±anchorå’Œkernelå†³å®šï¼Œéç½‘ç»œå­¦ä¹ â¡ï¸anchorå’Œfeatureå¯¹é½

#### Sample Discrimination Metrics

æ¯ä¸ªä½ç½®åªæœ‰ä¸€ä¸ªanchorï¼Œç„¶åè¿­ä»£refine

> Determining whether a training sample is pos/neg as the use of anchor/anchor-free is adversarial ä¸¤ç§æ–¹æ³•å†³å®šæ­£è´Ÿæ ·æœ¬çš„æ–¹æ³•ä¸åŒ

ğŸ‘†å³anchor-freeçš„å†³å®šæ–¹å¼å®½æ¾/æ•°é‡å¤šï¼Œanchor-basedæ ‡å‡†ä¸¥æ ¼/æ•°é‡å°‘
Stage 1: anchor-freeâ¡ï¸æ›´å¤šæ­£æ ·æœ¬ã€Œè§£å†³æ­£è´Ÿæ ·æœ¬ä¸åŒ¹é…ã€
Stage 2: amchor-basedâ¡ï¸ä¸¥æ ¼ï¼Œæ•°é‡å‡å°‘ï¼ŒIoUé«˜
`anchor-free`æŒ‡FCOSï¼Œä¸­å¿ƒç‚¹åœ¨ç‰©ä½“å†…ä¸ºpos anchor
`anchor-based`æŒ‡Faster RCNNï¼ŒIoU threshold

#### Cascade RPN

å‰ä¸€ä¸ªé˜¶æ®µçš„è¾“å‡ºbridgeåˆ°åä¸€ä¸ªé˜¶æ®µ
ç”±anchorè®¡ç®—å‡ºoffset `o`ï¼Œå†å’Œfeature `x`è¾“å…¥regressorè®¡ç®—æ–°çš„anchor ( $\sigma$å°±æ˜¯anchorå›å½’çš„ç›®æ ‡ eg. $(tx-ax)/aw$ )

![](Figures/407d8090a6d3898e6971c85df18ba81b99684580.png)

---

## DetNet

> ä¸ºæ£€æµ‹ä»»åŠ¡è®¾è®¡backbone  

ç°æœ‰çš„ImageNet backboneï¼š 1. ç½‘ç»œstageéœ€è¦å¢åŠ ï¼Œä¸”æœªåœ¨imagenetè®­è¿‡ 2. down-sampleå’ŒstrideæŸå¤±ç©ºé—´ä¿¡æ¯ï¼Œå¤§ç›®æ ‡è¾¹ç•Œæ¨¡ç³Š 3. å°ç›®æ ‡ã€Œç©ºé—´åˆ†è¾¨ç‡ä½ã€

---

## TridentNet

> Scale variation $\to$ Different <u>**receptive fields**</u>  

å¤šåˆ†æ”¯ç½‘ç»œï¼Œåˆ†æ”¯ç»“æ„ç›¸åŒæƒé‡å…±äº«ï¼Œæ¯ä¸ªåˆ†æ”¯ä¸åŒçš„æ„Ÿå—é‡å¯¹åº”æ£€æµ‹ä¸åŒå°ºåº¦èŒƒå›´çš„ç‰©ä½“
ä¸åŒæ„Ÿå—é‡ä½¿ç”¨`dilated_conv`å®ç°ğŸ‘‰å‚æ•°ç›¸åŒ
æƒé‡å…±äº«ï¼šå‡å°‘å‚æ•°é‡ï¼Œinferenceæ—¶åªé€‰æ‹©ä¸€ä¸ªä¸»åˆ†æ”¯
![](Figures/ac7b0f86651341d53a16ca349d1b6deb5414d9c7.png)

<u>Image Pyramid</u> (Multi-scale training&testing): time-consuming
<u>Feature Pyramid</u>: use different params to predict different scale (not uniform) 

**trident block**ğŸ‘‡

![](Figures/118a7989f6f926f41e2581e0c102f9c6d3c665e0.png)**Scale-aware Training Scheme**ï¼šæ¯ä¸ªbranchåªå¯¹é•¿å®½åœ¨ä¸€å®šèŒƒå›´çš„proposalè¿›è¡Œè®­ç»ƒã€Œä¸€å¼ å›¾ç‰‡ä½¿ç”¨ä¸åŒbranch(ä¸åŒdilate rate)è®­ç»ƒä¸åŒå°ºåº¦çš„proposalã€
å…¶ä»–å‚æ•°ç›¸åŒ(make sense?) 

**é¢„æµ‹**ï¼šè®¡ç®—æ¯ä¸ªåˆ†æ”¯çš„é¢„æµ‹è¾“å‡ºï¼Œfilter outæ‰è¶…è¿‡å°ºå¯¸èŒƒå›´çš„box **TridentNet Fast**ï¼šé¢„æµ‹åªé‡‡ç”¨å•åˆ†æ”¯$\to$ä¸­é—´åˆ†æ”¯é¢„æµ‹ï¼Œå¾—ç›Šäºä¸‰åˆ†æ”¯æƒé‡å…±äº«ï¼Œæ•ˆæœæ¥è¿‘

---

## SNIPER: Efficient Multi-Scale Training

> è§£å†³å¤šå°ºåº¦é—®é¢˜, ä¸æ„å»ºfeature pyramid, <u>**å¤šå°ºåº¦è®­ç»ƒç­–ç•¥**</u>, å°ºå¯¸é€‚åº”ç½‘ç»œ

**Scale Invariant**: *RCNN*å°†proposalç¼©æ”¾åˆ°åŒä¸€ä¸ªå°ºåº¦ï¼Œæ£€æµ‹ç½‘ç»œåªéœ€è¦å­¦ä¹ ä¸€ç§å°ºåº¦çš„æ£€æµ‹ã€‚è€Œä¸ºäº†é€‚åº”ä¸åŒå°ºåº¦ï¼Œå¤šå°ºåº¦è®­ç»ƒçš„*Faster RCNN*å¯¹æ•´ä¸ªå›¾ç‰‡è¿›è¡Œæ”¾ç¼©ï¼Œproposalä¹Ÿæ”¾å¤§ç¼©å°ï¼Œæ£€æµ‹ç½‘ç»œå­¦ä¹ é€‚åº”å¤šç§å°ºåº¦ã€‚ <u>é€šè¿‡ç½‘ç»œcapacityè®°å¿†ä¸åŒscaleçš„ç‰©ä½“</u> ï¼Œæµªè´¹capacity

> *Process context regions around GT instances(chips) at appropriate scale*  
> æˆªå–å›ºå®šå°ºå¯¸çš„chip(eg `3x3, 5x5, 7x7`)å¯¹åº”ä¸åŒå°ºåº¦ï¼Œç„¶åresizeåˆ°ç›¸åŒå¤§å°(low-res)å»è®­ç»ƒ  
> å°ç›®æ ‡zoom-inï¼Œå¤§ç›®æ ‡zoom-out  

#### Pos chips

![](Figures/ff21eb5b057a444a8bb5207b3cc2b71dc5fdde7b.png)
ğŸ‘†chipä»æœ€å°çš„coveræŸä¸ªGT boxå¼€å§‹ï¼Œç›´åˆ°æœ€å¤šçš„boxè¢«è¿™ä¸ªchip coveråˆ°
ã€Œchipå°ºå¯¸ä¸å˜ï¼Œå›´ç»•coverè¿™ä¸ªGTboxè½¬ï¼Œç›´åˆ°æœ€å¤§åŒ–coverçš„boxæ•°é‡ã€

1. æ¯ä¸ªboxè‡³å°‘è¢«ä¸€ä¸ªchip cover
2. ä¸€ä¸ªç‰©ä½“å¯èƒ½è¢«å¤šä¸ªchip cover
3. ä¸€ä¸ªç‰©ä½“åœ¨ä¸åŒå°ºåº¦chipä¸­å¯èƒ½valid or not
4. æˆªæ–­çš„ç‰©ä½“ä¿ç•™

#### Neg chips

![](Figures/58422bc9dea8dd52de0f8214533a4c54c7d97cc6.png)ğŸ‘†åªæœ‰pos chipsä¼šå¯¼è‡´ç½‘ç»œåªå¯¹GTé™„è¿‘å°èŒƒå›´çš„å›¾ç‰‡è®­ç»ƒ *iconic*ï¼Œç¼ºä¹ <u>èƒŒæ™¯</u> ã€‚å¢åŠ **éš¾æ ·æœ¬**ä½œä¸ºneg chips
Metrics:

1. å¦‚æœåŒºåŸŸæ²¡æœ‰proposalï¼Œè®¤ä¸ºæ˜¯easy backgroundï¼Œå¿½ç•¥
2. å»æ‰è¢«pos chip coverçš„proposalã€Œproposalå’ŒGTæ¥è¿‘ï¼Œæ˜“äºåŒºåˆ†ã€
3. è´ªå¿ƒé€‰æ‹©è‡³å°‘cover Mä¸ªå‰©ä½™proposalçš„ä½œä¸ºneg chips

è®­ç»ƒæ—¶å¯æ§åˆ¶neg chipæ•°é‡ï¼Œç±»ä¼¼OHEM
<u>åˆ†è¾¨ç‡å’Œå‡†ç¡®ç‡å…³ç³»å¯èƒ½ä¸å¤§ï¼Œè¿‡å¤šcontextå¯èƒ½ä¸å¿…è¦</u>
Ref: [ç›®æ ‡æ£€æµ‹-SNIPER-Efficient Multi-Scale Training-è®ºæ–‡ç¬”è®° | arleyzhang](https://arleyzhang.github.io/articles/f0c1556d/)

---

## Stitcher: Feedback-driven Data Provider for Object Detection

> å°ç›®æ ‡ï¼Œç²˜è´´æ„é€ è®­ç»ƒæ ·æœ¬

å°ç›®æ ‡æ•°æ®é›†ä¸­åˆ†å¸ƒä¸å‡åŒ€(41.4%çš„å°ç›®æ ‡åªå‡ºç°åœ¨52.3%çš„å›¾ç‰‡ä¸­)ï¼Œå°ç›®æ ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è´¡çŒ®çš„lossä½ï¼Œå­¦ä¸å¥½

æŠŠå›¾ç‰‡ç¼©å°ï¼Œæ‹¼æ¥åœ¨ä¸€èµ·ï¼ˆå’ŒSNIPERåˆ‡å‰²ç›¸åï¼‰

![image-20200620175602128](Figures/image-20200620175602128.png)

**æŠŠå¤§ç‰©ä½“å’Œä¸­ç‰©ä½“éƒ½å˜æˆå°ç‰©ä½“ï¼Œå¢åŠ å°å°ºåº¦çš„åˆ†å¸ƒ**

å°ç›®æ ‡ï¼šæ£€æµ‹æ—¶æ”¾å¤§âŒï¼Œè®­ç»ƒæ—¶ç¼©å°âœ…

lossä½œä¸ºåé¦ˆä¿¡å·ï¼Œ**å°ç›®æ ‡äº§ç”Ÿlossä¸è¶³($r^t_s<\tau$)åˆ™ä¸‹ä¸ªiteré‡‡ç”¨stitchï¼Œç¼ºå•¥è¡¥å•¥**

![image-20200620175826519](Figures/image-20200620175826519.png)

![image-20200620175926012](Figures/image-20200620175926012.png)

---

## HRNet: Deep High-Resolution Representation Learning for Visual Recognition

> å¤„ç†è¿‡ç¨‹ä¸­ä¿æŒé«˜åˆ†è¾¨ç‡ã€Œposition-sensitive taskã€  
> maintain high-res representation through the whole process  
> ä¸åŒäºskip connectionï¼šé«˜åˆ†è¾¨åˆ†æ”¯å¹³è¡Œconvï¼Œé€šè¿‡fusionè€Œä¸æ˜¯addèåˆé«˜ä½åˆ†æ”¯ï¼Œå¤šåˆ†è¾¨ç‡è¾“å‡º  
> ä¸åŒäºç‰¹å¾é‡‘å­—å¡”ï¼šé«˜ä½åˆ†è¾¨ç‡å¹³è¡Œè®¡ç®—ï¼ˆlow-reså¢åŠ åˆ†è¾¨ç‡ä¸‹convè®¡ç®—ï¼Œä¸æ˜¯é€šè¿‡high-resä¸€æ¬¡å·ç§¯downsampleå¾—åˆ°ï¼Œ**é€æ­¥å¹³è¡Œ**è®¡ç®—å¢åŠ ï¼‰  

å…ˆå‰ç½‘ç»œï¼šencode high $\to$ lowï¼Œrecover low $\to$ high
æå‡ºç½‘ç»œï¼šè¿ç®—æ—¶**ä¿æŒé«˜åˆ†è¾¨ç‡**åˆ†æ”¯ï¼Œ**å¹³è¡Œ**çš„åŠ å…¥ä½åˆ†è¾¨ç‡åˆ†æ”¯ï¼›**multi-res fusion**

#### Parallel multi-res conv

![](Figures/24dc1e9896b61be4bef3511cd8a870ed0fd0e31c.png)ğŸ‘†æ¯ä¸ªstage <u>é€æ­¥åŠ å…¥ä¸€ä¸ªä½åˆ†è¾¨ç‡(eg 1/2)</u> åˆ†æ”¯ï¼Œä¸”ä¿æŒåŸæœ‰åˆ†è¾¨ç‡åˆ†æ”¯
ç±»ä¼¼ <u>group conv</u> ï¼Œé€šé“åˆ†åˆ« $\to$ åˆ†è¾¨ç‡åˆ†åˆ«

#### Repeated multi-res fusion

æ¯ä¸ªstage(4ä¸ªunit/block)äº¤æ¢ä¸åŒåˆ†è¾¨ç‡çš„ä¿¡æ¯
![](Figures/e4187388dd2d925fbbd3c4f199e3d5e8c830eb90.png)ğŸ‘†high $\to$ low: stride conv; low $\to$ high: bilinear upsampling + 1x1 conv
an <u>extra</u> output for lower res output![](Figures/7ac095e7bf586fccf3c9b8de1caf0658163c37eb.png)ğŸ‘†èåˆç±»ä¼¼FC

#### Multi-res representation head/ä¸åŒä»»åŠ¡ä¸åŒè¾“å‡ºæ¨¡å¼

![](Figures/a2036193f4b7513cffb8bb02a77b3e3cbf9ab787.png)(a) å…³é”®ç‚¹æ£€æµ‹ (b) segmentation (c) object detection

---

## Region Proposal by Guided Anchoring

> æ›´å¥½çš„anchorï¼Œ**æ”¹è¿›äº§ç”Ÿanchorçš„è¿‡ç¨‹**ã€Œéå¯†é“ºã€  
> anchorä¸feature: <u>**alignment**</u> + <u>**consistency**</u> 

ä¸¤ä¸ªåˆ†æ”¯åˆ†åˆ«å¯¹anchorçš„ä¸­å¿ƒç‚¹å’Œé•¿å®½è¿›è¡Œé¢„æµ‹ï¼Œé˜²æ­¢offsetåç§»è¿‡å¤§ï¼Œanchorå’Œç‚¹çš„featureä¸å¯¹åº”
é‡‡ç”¨**deformable conv**ä½¿featureçš„èŒƒå›´å’Œanchorçš„å½¢çŠ¶å¯¹åº”ï¼Œæ¯ä¸ªä½ç½®anchorå½¢çŠ¶ä¸åŒè€Œcaptureä¸åŒçš„ç‰¹å¾ã€ŒåŠ offsetä»¥é€‚åº”anchorå½¢çŠ¶ã€
![](Figures/ec5348cb3efb80de6adb90ab09fac616f97681bd.png)

#### Guided Anchoring

$p(x,y,w,h|I)=p(x,y|I)p(w,h|x,y,I)$
**åˆ†ä¸¤æ­¥äº§ç”Ÿanchor**ã€Œå‡å°åŒæ—¶é¢„æµ‹xywhæ—¶å‡ºç°çš„åç§»ä¸å¯¹åº”ã€

1. locationï¼šé¢„æµ‹objectnessï¼Œä¹‹åé‡‡ç”¨mask_conv **å‡å°‘åŒºåŸŸè®¡ç®—**
   åªå¯¹ç‰©ä½“çš„ä¸­å¿ƒ(åŠé™„è¿‘)ä¸ºposè®­ç»ƒï¼Œé¢„æµ‹ç‰©ä½“ä¸­å¿ƒã€Œè¾¹ç¼˜ä¸å®¹æ˜“å›å½’æ¡†ã€
2. shapeï¼šé¢„æµ‹æ¯ä¸ªä½ç½®ä¸Šçš„best shapeï¼Œä½ç½®ä¸å˜åªå˜é•¿å®½ï¼Œä¸ä¼šmisalign
   é¢„æµ‹$w=k\times e^{dw}$ï¼Œé¢„æµ‹dwï¼Œè€Œä¸æ˜¯wï¼ŒèŒƒå›´æ›´å¤§ğŸ‘‡
   $w=\sigma \cdot s \cdot e^{dw}, \; h=\sigma \cdot s \cdot e^{dh}$

é€‰æ‹©é«˜äºthreshçš„locationä¸­ï¼Œæ¦‚ç‡æœ€é«˜çš„shapeï¼Œäº§ç”Ÿanchor

#### Feature adaptation

**consistency**: æ¯ç‚¹å¯¹åº”çš„anchoré•¿å®½ä¸åŒï¼Œæ‰€ä»¥å­¦ä¹ åˆ°ç‰¹å¾å¯¹åº”åŒºåŸŸçš„é•¿å®½ä¹Ÿåº”è¯¥ä¸åŒ
$\mathbf{f}'_i=\mathcal{N}_T(\mathbf{f}_i,w_i,h_i)$
åŸºäºå¯¹åº”anchorçš„é•¿å®½ï¼Œæ”¹å˜ç‰¹å¾ï¼ˆxyä¸å˜ï¼Œä½ç½®branchåªé¢„æµ‹objectness scoreï¼‰
ğŸ‘†ä½¿ç”¨deformable convolutionå®ç°

#### Anchor shape target

è®­ç»ƒæ—¶anchorå’Œgt boxçš„åŒ¹é…ï¼Œè®­ç»ƒç›®æ ‡ã€‚whä¸ºå˜é‡ï¼Œæ— æ³•è®¡ç®—IoU

$\mathbf{vIoU}(a_{\mathbf{wh}},\mathbf{gt})=\max\limits_{w>0,h>0}{\mathbf{IoU}_{normal}(a_{\mathbf{wh}},\mathbf{gt})}$

æ–¹æ³•ï¼šSampleå¸¸è§çš„whç»„åˆï¼Œè®¡ç®—å’ŒGTçš„IoUï¼Œå¾—åˆ°vIoUğŸ‘†ï¼Œä½œä¸ºanchorå’Œgt IoUçš„ä¼°è®¡ï¼Œä¹‹åé‡‡ç”¨å¸¸è§anchoråˆ†é…æ–¹æ³•ç¡®å®šè®­ç»ƒç›®æ ‡

#### High quality proposal

ç”±äºç”Ÿæˆçš„anchoræ›´å¥½ï¼Œposæ ·æœ¬æ•°é‡æ›´å¤šã€‚è®­ç»ƒæ ·æœ¬åˆ†å¸ƒç¬¦åˆproposalåˆ†å¸ƒ
è®¾ç½® <u>æ›´é«˜æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹</u> ï¼ŒåŒæ—¶ <u>æ›´å°‘æ ·æœ¬</u> æ•°é‡ï¼Œå³ <u>æ›´é«˜IoU threshold</u>

---

## Soft NMS

> è§£å†³å¯†é›† _ç›¸é‚»_ ç‰©ä½“çš„æ£€æµ‹æ¡†é‡å IoUå¤§ï¼Œå¯èƒ½åœ¨NMSè¿‡ç¨‹ä¸­ <u>**è¯¯åˆ **</u>  
> å¯†é›†ç‰©ä½“æ£€æµ‹æœ‰æå‡  

#### NMS

æŒ‰ç…§ç½®ä¿¡åº¦æ’åºï¼Œé€‰æ‹©æœ€å¤§çš„box iä¿ç•™ã€‚å…¶ä½™boxä¸­ï¼Œä¸Içš„IoU>thresholdçš„åˆ é™¤(ç½®ä¿¡åº¦ç½®ä¸º0)ã€‚å†ä»å‰©ä¸‹boxé€‰æ‹©æœ€å¤§ä¿ç•™ï¼Œé‡å¤
![](Figures/98b46d0bb819b13145c34910d32fda6c602d7192.jpg)

#### Soft NMS

é‡å IoUè¶Šå¤§ï¼Œç½®ä¿¡åº¦ä¸‹é™è¶Šå¤š
ç½®ä¿¡åº¦ç½®ä¸º0å˜ä¸ºæ›´æ–°IoU>thresholdæ¡†çš„ç½®ä¿¡åº¦
![](Figures/ec46758beec67fa960bc978c8fa18304cceee89d.jpg)
æˆ–
![](Figures/89ba0e23898e18e892d209b76c3c2b6602fe3e0e.jpg)
Ref: [NMSä¸soft NMS - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/42018282)

---

## Adaptive NMS Refining Pedestrian Detection in a Crowd

> å¯†é›†åœºæ™¯ä¸‹NMSè¯¯åˆ 
>
> é€šè¿‡é¢„æµ‹crowdç¨‹åº¦åŠ¨æ€é€‰æ‹©threshold

å¯†é›†ä½ç½®æé«˜IoUé˜ˆå€¼ä¿ç•™ä¸´è¿‘æ¡†ï¼Œç¨€ç–ä½ç½®é™ä½IoUé˜ˆå€¼åˆ é™¤å†—ä½™æ¡†

å¯¹äº**æ¯ä¸ªç‰©ä½“**å®šä¹‰object density $$d_{i}:=\max _{b_{j} \in \mathcal{G}, i \neq j} \operatorname{iou}\left(b_{i}, b_{j}\right)$$ 

é˜ˆå€¼è®¡ç®—è¿‡ç¨‹ $$N_{\mathcal{M}}:=\max \left(N_{t}, d_{\mathcal{M}}\right)$$

NMSè¿‡ç¨‹ $$s_{i}=\left\{\begin{array}{ll}s_{i}, & \operatorname{iou}\left(\mathcal{M}, b_{i}\right)<N_{\mathcal{M}} \\ s_{i} f\left(\operatorname{iou}\left(\mathcal{M}, b_{i}\right)\right), & \text { iou }\left(\mathcal{M}, b_{i}\right) \geq N_{\mathcal{M}}\end{array}\right.$$

æµ‹è¯•æ—¶densityé€šè¿‡ç½‘ç»œ`density subnet`é¢„æµ‹ï¼Œobjectness map + bboxé¢„æµ‹`concat`ä½œä¸ºè¾“å…¥ï¼Œ`5x5`å·ç§¯(ä¸´è¿‘ç‰©ä½“çš„ä¿¡æ¯)

![image-20200428211808639](Figures/image-20200428211808639.png)

![image-20200428211838035](Figures/image-20200428211838035.png)

åœ¨<u>cityperson</u>å’Œ<u>crowdhuman</u>å¯†é›†æ•°æ®é›†æ•ˆæœå¥½

Ref: https://www.starlg.cn/2019/05/20/Adaptive-NMS/

---

## Fast NMS

![image-20200510104231573](Figures/image-20200510104231573.png)

æŒ‰ç…§confé¡ºåºæ„å»ºIoUçŸ©é˜µï¼Œè½¬ä¸ºä¸Šä¸‰è§’ã€‚å¯¹$B_i$ï¼Œå¦‚æœ$\exists x_{i,j}>\epsilon$ï¼Œåˆ™å»æ‰$B_i$ï¼Œé€Ÿåº¦å¿«

é—®é¢˜ï¼šæ²¡æœ‰å»æ‰$B_i$æ—¶æŠŠä¹‹åçš„$x_{i\;\cdot}$å¤±æ•ˆï¼Œã€Œæ¨ªå‘ä¼ æ’­ã€ï¼Œå¯èƒ½å¤šåˆ é™¤æ¡†ã€‚$B_i$è¢«åˆ é™¤åï¼Œä¹‹åçš„æ¡†å’Œ$B_i$çš„IoUä»è¢«è€ƒè™‘è®¡ç®—

## Cluster NMS

*Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation*

#### Cluster-NMS

æ”¹è¿›`Fast NMS`ï¼Œå¢åŠ remove row $B_i$çš„æ“ä½œ

![image-20200510121557382](Figures/image-20200510121557382.png)

$b^{t-1}$è¡¨ç¤º$t-1$ iterçš„NMS indicatorï¼Œtæ¬¡iteræ—¶å¯¹$C^t$è¿›è¡ŒNMS

$A=diag(b)$è¡¨ç¤ºæ ¹æ®ä¸Šæ¬¡NMSç»“æœ(indicator)ï¼Œå¯¹å·²ç»è¢«supressedçš„æ¡†å»æ‰ï¼ˆiè¡Œç½®0ï¼Œä¸è€ƒè™‘ $x_{i\;\cdot}$ ï¼Œå’Œiæ¡†çš„IoUä¸è®¡ç®—ï¼‰

$T=1$åŒ`Fast NMS`ï¼Œ$T=N$åŒ`NMS`

æ²¡æœ‰é‡åˆçš„boxå¯ä»¥åˆ†æˆå¤šä¸ª**cluster**å¹¶è¡Œå¤„ç†

#### Score penalty reweigh + Cluster NMS

ç±»ä¼¼`Soft NMS`ä¸­ä¸æ˜¯ç›´æ¥å»é™¤box ã€Œhardã€ï¼Œå˜æˆå¯¹scoreè¿›è¡Œreweightã€Œsoftã€ï¼Œæ„æˆ`Cluster-NMS_S`

$s_{j}=s_{j} \prod\limits_{i} e^{-\frac{(\boldsymbol{A} \times \boldsymbol{X})_{i j}^{2}}{\sigma}}$

jå’Œå…¶ä»–boxçš„IoUè¶Šå¤§ï¼Œscoreé™ä½è¶Šå¤š

ä¸åŒäº`Soft NMS`ï¼Œåªä¼šè¢« <u>å’Œæ›´é«˜confçš„boxæœ‰å¤§IoU</u> è€Œå—åˆ°æƒ©ç½šï¼Œç”±äºæ˜¯ä¸Šä¸‰è§’ï¼Œåªè®¡ç®—å’Œæ›´é å‰boxçš„IoU

#### Normalized central point distance + Cluster NMS

å¢åŠ åŒ`DIoU`ç±»ä¼¼çš„ä¸­å¿ƒç‚¹è·ç¦»$D$ï¼Œæ„æˆ`Cluster-NMS_S+D`

$s_{j}=s_{j} \prod\limits_{i} \min \{e^{-\frac{(\boldsymbol{A} \times \boldsymbol{X})_{i j}^{2}}{\sigma}}+D^{\beta}, 1\}$

#### Weighted NMS + Cluster NMS

`Weighted NMS`æ ¹æ®IoUå’ŒconfåŠ æƒ`merge`é‡å æ¡†ï¼Œè¾“å‡º**å…¨æ–°çš„æ¡†**ã€Œé€Ÿåº¦æ…¢ã€

$\mathcal{B}=\frac{1}{\sum_{j} w_{j}} \sum_{\mathcal{B}_{j} \in \Lambda} w_{j} \mathcal{B}_{j}$

$\mathcal{B}$æ˜¯åŠ æƒèåˆåçš„å…¨æ–°çš„æ¡†ï¼Œ$\Lambda=\left\{\mathcal{B}_{j} | x_{i j} \geq \varepsilon, \forall i\right\}$ä¸ºé‡å æ¡†ï¼Œæƒé‡$w_{j}=s_{j} I o U\left(\mathcal{B}, \mathcal{B}_{j}\right)$ï¼Œ**weighted combination**

confä»é«˜åˆ°ä½ï¼Œæ‰¾åˆ°IoU>thresholdçš„æ¡†ï¼Œæ ¹æ®IoUè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°èåˆæ¡†ï¼›å†å¯¹å…¶ä»–IoU<thresholdçš„æ¡†è®¡ç®— (https://github.com/sanch7/Weighted-NMS/blob/master/weighted_nms.py)

`Cluster-NMS_W`:

$\mathcal{C'}=\mathcal{s} \otimes \mathcal{C}$

$\mathcal{B}=\mathcal{C'}\times \mathcal{B}$

![image-20200510164152776](Figures/image-20200510164152776.png)

## Feature NMS: NMS by Learning Feature Embeddings

**å¯†é›†é‡å åœºæ™¯**ä¸‹ï¼Œåªé€šè¿‡IoUä¸èƒ½åˆ¤æ–­æ˜¯å¦æ˜¯å¯¹åŒä¸€ä¸ªç‰©ä½“çš„é¢„æµ‹ã€‚<u>**å¢åŠ feature vec è·ç¦»åˆ¤æ–­æ˜¯å¦æ˜¯åŒä¸€ä¸ªç‰©ä½“çš„é¢„æµ‹**</u>ï¼Œè·ç¦»å°åˆ é™¤

![image-20200520113710926](Figures/image-20200520113710926.png)

å½“IoUæ— æ³•åˆ¤æ–­æ—¶ä½¿ç”¨embeddingåˆ¤æ–­æ˜¯å¦æ˜¯åŒä¸€ä¸ªç‰©ä½“

è®­ç»ƒä½¿ç”¨<u>Margin Loss</u>: $L=\frac{\sum_{i \in \mathcal{A}} \sum_{j \in \mathcal{A} \backslash\{i\}} L^{\prime}(i, j)}{|\mathcal{A}| \cdot(|\mathcal{A}|-1)}$

å…¶ä¸­ï¼Œpairwise loss: $L^{\prime}(i, j)=\left\{\begin{array}{ll}\max \left(0,\left\|\mathbf{f}_{i}, \mathbf{f}_{j}\right\|_{2}-(\beta-\alpha)\right), & \text { if } o b j(i)=o b j(j) \\ \max \left(0,(\beta+\alpha)-\left\|\mathbf{f}_{i}, \mathbf{f}_{j}\right\|_{2}\right), & \text { otherwise }\end{array}\right.$

---

## Matrix Nets: A New Deep Architecture for Object Detection (xNets)

> FPNå¤„ç†ä¸åŒå¤§å°çš„ç‰©ä½“(ç‰¹å¾é‡‘å­—å¡”)  
> ğŸ‘‡æœ¬æ–‡å¢åŠ ä¸åŒé•¿å®½æ¯”ç‰©ä½“çš„å¤„ç† (å¤§å°é‡‘å­—å¡”+aspect ratioé‡‘å­—å¡”)  

![](Figures/f96e9e976eb7720d970f1810c8396bf598967513.png)
é«˜åº¦ï¼Œå®½åº¦å‡åŠã€‚å·¦ä¸‹å³ä¸Šå‰ªæ(ç‰©ä½“ä¸å¸¸è§)
æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼Œç›¸æ¯”CenterNetå‚æ•°é‡å‡å°‘
Ref: [å‚æ•°å°‘ä¸€åŠã€é€Ÿåº¦å¿«3å€ï¼šæœ€æ–°ç›®æ ‡æ£€æµ‹æ ¸å¿ƒæ¶æ„æ¥äº†](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650768067&idx=2&sn=7133cf90287c91b297d857a57bdd6481&chksm=871a46bdb06dcfab0f4092acc96db5b8dc88c34937e7ea1a5e813e7d53a165d2549db0640068&scene=21#wechat_redirect)

---

## IoU-Net

> Add localization confidence in NMS
>
> å¯ä»¥çœ‹ä½œä¸€ç§ç²¾ç»†åŒ–çš„å‰èƒŒæ™¯åˆ†ç±»ï¼ˆsoftï¼‰

#### NMS

1. é€‰æ‹©æœ€å¤§classification confidenceçš„æ¡†$b_j$ï¼ŒåŠ å…¥é›†åˆ$S$ã€‚
2. å…¶ä»–æ‰€æœ‰ä¸å†é›†åˆä¸­çš„æ¡†ï¼Œå¦‚æœå’Œ$b_j$çš„IoUå¤§äºthresholdï¼Œåˆ™åˆ å»ï¼Œç®€åŒ–é‡å¤æ¡†ã€‚
3. é‡å¤çŸ¥é“æ²¡æœ‰æ¡†ï¼Œ$S$ä¸ºç»“æœã€‚

ä½¿ç”¨åˆ†ç±»ç½®ä¿¡åº¦ä½œä¸ºæœ€å¼€å§‹é€‰æ‹©æ¡†çš„ä¾æ®ï¼ŒIoUç”¨äºè®¡ç®—åˆ†ç±»ç½®ä¿¡åº¦æœ€å¤§çš„æ¡†å’Œå…¶ä»–æ¡†ä¹‹é—´çš„é‡åˆåº¦ï¼Œåˆ å»æ¡†ã€‚

è€ŒIoU-Netä½¿ç”¨é¢„æµ‹çš„æ¡†å’ŒGTçš„é‡åˆIoUï¼Œå³å®šä½ç½®ä¿¡åº¦ï¼Œé€‰æ‹©æœ€å¤§ä½œä¸ºä¾æ®ã€‚åœ¨inferenceé˜¶æ®µå‘æŒ¥ä½œç”¨ã€‚

#### é¢„æµ‹IoU

![15689424793639](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689424793639.jpg)
é€šè¿‡ç½‘ç»œé¢„æµ‹IoUï¼šä½¿ç”¨FPNä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œæç‰¹å¾ã€‚ä½¿ç”¨PrRoI poolingæ›¿ä»£RoI pooling

#### IoU Guided NMS

![15689426602716](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689426602716.jpg)
Rank all detection bbox on localization confidence.

é€‰æ‹©IoUæœ€å¤§çš„æ¡†ï¼Œå…¶ä»–æ¡†é‡å å¤§äºthresçš„æ¡†åªä½¿ç”¨ä»–çš„æœ€å¤§conf scoreä½œä¸ºIoUæœ€å¤§æ¡†çš„confã€Œæ ¹æ®IoUé€‰æ‹©ï¼Œæœ€å¤§scoreä¿®æ­£confã€

#### Consider bounding box refinement as optimization

![15689722991024](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689722991024.jpg)
é€šè¿‡é¢„æµ‹IoUå¹¶äº§ç”Ÿæ¢¯åº¦ï¼Œæ›´æ–°bounding boxï¼Œå¹¶é€šè¿‡åˆ¤æ–­åˆ†æ•°çš„æå‡å’Œå·®å€¼æ¥æ›´æ–°è¾¹ç•Œæ¡†
// ToRead

#### Precise RoI pooling

ä½¿ç”¨åŒçº¿æ€§æ’å€¼æ¥è¿ç»­åŒ–ç‰¹å¾å›¾ï¼Œä»»æ„è¿ç»­åæ ‡(x,y)å¤„éƒ½æ˜¯è¿ç»­çš„
$f(x,y)=\Sigma_{i,j}IC(x,y,i,j)\times w_{i,j}$
$IC(x,y,i,j)=max(0,1-|x-i|)\times max(0,1-|y-j|)$æ˜¯æ’å€¼ç³»æ•°ï¼Œxyè¿ç»­ï¼Œijä¸ºåæ ‡åƒç´ ç‚¹ã€‚RoIçš„ä¸€ä¸ªbinè¡¨ç¤ºä¸ºå·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡å¯¹ã€‚é€šè¿‡äºŒé‡ç§¯åˆ†è¿›è¡Œæ± åŒ–ï¼ˆåŠ æƒæ±‚å’Œï¼‰
$PrPool(\{(x_1, y_1),\;(x_2, y_2)\},\;F)=\frac{\int_{y_1}^{y_2}\int_{x_1}^{x_2}f(x,y)dxdy}{(x_2-x_1)\times (y_2-y_1)}$

![15689719523883](https://images-1256050009.cos.ap-beijing.myqcloud.com/15689719523883.jpg)

ä½¿ç”¨ResNet-FPNä½œä¸ºéª¨å¹²ç½‘ç»œï¼ŒRoI poolingæ¢æˆPrRoI poolingã€‚åŒæ—¶IoUé¢„æµ‹åˆ†æ”¯å¯ä»¥å’ŒR-CNNçš„åˆ†ç±»å’Œè¾¹ç•Œæ¡†å›å½’åˆ†æ”¯å¹¶è¡Œå·¥ä½œ.

---

## FreeAnchor: Learning to Match Anchors for Visual Object Detection

Related å¯¹äºanchorç”Ÿæˆ/åˆ†é…/é€‰æ‹©çš„æ”¹è¿›ï¼šGuided Anchoring, IoU-Net, MetaAnchor [MetaAnchor - ç®€ä¹¦](https://www.jianshu.com/p/a24d814613eb)

> å¯¹äºanchorå’Œobjectçš„<u>**åŒ¹é…æ–¹å¼**</u>çš„æ”¹è¿›ï¼ŒLearn to match 

ä¹‹å‰é‡‡ç”¨IoUæœ€å¤§çš„anchorè¿›è¡Œåˆ†é…ï¼šç»†é•¿ç‰©ä½“ï¼Œæœ€representativeçš„ç‰¹å¾ä¸åœ¨ç‰©ä½“ä¸­å¿ƒï¼ŒIoUæœ€å¤§$\neq$æœ€representative

Assignç­–ç•¥éœ€è¦æ»¡è¶³ï¼š

1. **Recall**: æ¯ä¸ªç‰©ä½“éƒ½èƒ½åˆ†é…ä¸€ä¸ªanchor

2. **Precision**: åŒºåˆ†background anchor

3. **Compatible NMS**: é«˜åˆ†ç±»åˆ†æ•°çš„anchoræœ‰å¥½çš„localization

**matchingè¿‡ç¨‹çœ‹ä½œMLEè¿‡ç¨‹**ï¼Œæ¯ä¸ªç‰©ä½“ä»bag of anchorä¸­é€‰likelihood probabilityæœ€å¤§çš„

#### Maximum Likelihood Estimationåˆ†æç°æœ‰detector

è®­ç»ƒæŸå¤±å‡½æ•°ï¼Œ$C_{i,j}$è¡¨ç¤ºj anchorå’Œi ç‰©ä½“åŒ¹é…ã€Œassign using IoU criterionã€$\mathcal{L}(\theta)=\sum\limits_{a_j\in A_+}\sum\limits_{b_i\in B}C_{i,j}\mathcal{L}_{i,j}^{cls}(\theta)+\beta\sum\limits_{a_j\in A_+}\sum\limits_{b_i\in B}C_{i,j}\mathcal{L}_{i,j}^{loc}(\theta)+\sum\limits_{a_j\in A_-}\mathcal{L}_j^{bg}(\theta)$

æŠŠè®­ç»ƒæŸå¤±å‡½æ•°çœ‹ä½œä¼¼ç„¶æ¦‚ç‡

$\begin{aligned}
\mathcal{P}(\theta) &=\mathbf{e^{-\mathcal{L}(\theta)}} \\
&=\prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} e^{-\mathcal{L}_{i j}^{c l_{j}}(\theta)}\right) \prod_{a_{j} \in B_{+}}\left(\sum_{b_{i} \in B} C_{i j} e^{-\beta \mathcal{L}_{j_{j}}^{l o c}(\theta)}\right) \prod_{a_{j} \in A_{-} \atop a_{j} \in A_{-}} \mathcal{P}_{j}^{b g}(\theta) \\
&=\prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} \mathcal{P}_{i j}^{c l s}(\theta)\right) \prod_{a_{j} \in A_{+}}\left(\sum_{b_{i} \in B} C_{i j} \mathcal{P}_{i j}^{l o c}(\theta)\right) \prod_{a_{j} \in A_{-}} \mathcal{P}_{j}^{b g}(\theta)
\end{aligned}$

<u>æ˜ å°„éå¸¸å·§å¦™ï¼Œä½¿$[0,+\infty)$çš„æŸå¤±æ˜ å°„åˆ°$(0,1]$ï¼Œè€Œä¸”æŸå¤±è¶Šå°ï¼Œ$\mathcal{P}(\theta)$è¶Šå¤§</u>

å› æ­¤ï¼Œæœ€å°åŒ–æŸå¤±çš„ç›®æ ‡è½¬æ¢ä¸ºæœ€å¤§åŒ–ä¼¼ç„¶æ¦‚ç‡

#### æ”¹è¿›detectionä¼¼ç„¶å‡½æ•°

ç›®æ ‡ recallï¼Œprecisionï¼Œcompatible

**Recall**ï¼šæ¯ä¸ªobjæ„å»ºbag of anchorï¼Œæœ€å¤§åŒ–å…¶ä¸­anchorçš„clså’Œlocä¼¼ç„¶ã€‚æ¯ä¸ªobjä¸€å®šå­˜åœ¨ä¸€ä¸ªanchorå¯¹åº”

$\mathcal{P}_{\text {recall}}(\theta)=\prod_{i} \max _{a_{j} \in A_{i}}\left(\mathcal{P}_{i j}^{c l s}(\theta) \mathcal{P}_{i j}^{l o c}(\theta)\right)$

**Precision**ï¼šå³å¯¹anchoråŒºåˆ†å‰èƒŒæ™¯ï¼ŒæŠŠèƒŒæ™¯anchoråˆ†å‡º

$\mathcal{P}_{\text {precision}}(\theta)=\prod_{i}\left(1-P\left\{a_{j} \in A_{-}\right\}\left(1-\mathcal{P}_{j}^{b g}(\theta)\right)\right)$

å…¶ä¸­$P\{a_{j} \in A_{-}\}=1-\max\limits_iP\{a_j\to b_i\}$ è¡¨ç¤ºanchor jä¸matchä»»ä½•ç‰©ä½“ã€‚å³anchorä¸matchä»»ä½•objæ¦‚ç‡è¶Šé«˜ï¼Œanchorä¸å±äºèƒŒæ™¯çš„æ¦‚ç‡è¶Šä½(1-)ï¼Œæ‰å¯ä»¥æœ€å¤§$\mathcal{P}_{\text {precision}}(\theta)$

**Compatible**: $P\{a_j\to b_i\}$è¡¨ç¤ºj anchoråŒ¹é…i objæ¦‚ç‡ï¼ŒNMSæŒ‰ç…§clsåˆ†æ•°é€‰ã€‚æ‰€ä»¥æ”¹æˆlocåˆ†æ•°ã€Œi j çš„IoUã€è¶Šå¤§ï¼ŒåŒ¹é…æ¦‚ç‡è¶Šé«˜ï¼ŒPä¸ºå…³äºIoUçš„<u>saturated linear</u>å‡½æ•°ã€‚æ­¥éª¤å­˜åœ¨äº$\mathcal{P}_{\text {precision}}(\theta)$ä¸­

![](Figures/2020-02-16-19-21-00-image.png)

æ¨ªåæ ‡ä¸ºIoU

**ä¼¼ç„¶å‡½æ•°**:    Jointly maximize

$\mathcal{P}'(\theta)=\mathcal{P}_{recall}(\theta)\times\mathcal{P}_{precision}(\theta)$

#### æ”¹è¿›ä¼¼ç„¶å‡½æ•°æ¨å‡ºMatching Mechanism

è®­ç»ƒæŸå¤±$\mathcal{L}'(\theta)=-\log\mathcal{P}'(\theta)$ï¼Œä½¿ç”¨FocalLoss

å…¶ä¸­æœ‰maxæ“ä½œï¼Œä½†éšæœºåˆå§‹åŒ–çš„ç½‘ç»œï¼Œæ‰€æœ‰anchorå¾—åˆ†éƒ½ä½ï¼Œmaxæ²¡æœ‰æ„ä¹‰

æ”¹ç”¨**Mean-max**å‡½æ•°ï¼š$\operatorname{Mean}-\max (X)=\frac{\sum_{x_{j} \in X} \frac{x_{j}}{1-x_{j}}}{\sum_{x_{j} \in X} \frac{1}{1-x_{j}}}$

è®­ç»ƒä¸å……åˆ†æ—¶æ¥è¿‘meanï¼Œä½¿ç”¨bagä¸­æ‰€æœ‰anchorè®­ç»ƒ

è®­ç»ƒå……åˆ†æ—¶æ¥è¿‘maxï¼Œé€‰æ‹©æœ€å¥½çš„anchorè®­ç»ƒ

![](Figures/2020-02-16-20-55-42-image.png)

å¯è§†åŒ–ï¼Œanchor assign confident (laptop)

![](Figures/2020-02-16-20-57-56-image.png)

ç›¸æ¯”baselineæœ‰æå‡3%. ä½¿ç”¨ResNeXt-64x4d-101ï¼Œ**ä¸ºmulti-scale

![](Figures/2020-02-16-21-02-22-image.png)

Ref: [https://www.aminer.cn/research_report/5dedbde4af66005a4482453f?download=false](https://www.aminer.cn/research_report/5dedbde4af66005a4482453f?download=false)

---

## å¯†é›†å°ç›®æ ‡

Paper: <u>Benchmark for Generic Product Detection: A strong baseline for Dense Object Detection</u>

![](Figures/2020-02-21-21-03-09-image.png)

<u>Scale Match for Tiny Person Detection</u>Â Â Â Â [method+dataset]

---

## Aligndet: Revisiting Feature Alignment for One-stage Object Detection

---

## FPN & Variants

æ‰€æœ‰FPNéƒ½ä½¿ç”¨backboneçš„å¤šå±‚ç‰¹å¾å›¾ï¼ˆç»è¿‡`1x1`å·ç§¯ï¼‰ğŸ‘‡$C_{2..5}$

![](Figures/2020-02-22-16-21-13-image.png)

#### Top-down FPN

ç»å…¸FPNï¼Œä»æœ€é«˜å±‚ç‰¹å¾(semanticï¼Œlow-res)ç»è¿‡upsampleï¼Œå’Œå„åŒä¸€çº§çš„ç‰¹å¾å›¾ç›¸åŠ 

ç»™åº•å±‚ç‰¹å¾å¼•å…¥é«˜å±‚è¯­ä¹‰ä¿¡æ¯ï¼Œç›Šäºå°ç›®æ ‡æ£€æµ‹ï¼ˆä½å±‚ç‰¹å¾å›¾ï¼‰

![](Figures/2020-02-22-16-25-08-image.png)

å…¬å¼$F_{i}^{t}=\mathbf{W}_{i+1}^{\mathrm{t}} \otimes\left(U\left(F_{i+1}^{t}\right)+C_{i}\right)$

#### Bottom-up FPN

ä»æœ€åº•å±‚(high-res)å‘ä¸Šé€æ¬¡äº§ç”ŸFPNå±‚ï¼Œå‘é«˜å±‚ç‰¹å¾å›¾ä¼ æ’­ä½å±‚çš„ç©ºé—´ç»†èŠ‚ä¿¡æ¯(spatial)

ä»ä½åˆ°é«˜ï¼Œèåˆã€Œæœ¬å±‚ç‰¹å¾ï¼Œé«˜ä¸€å±‚ç‰¹å¾ï¼Œä¸Šä¸€å±‚FPNã€

![](Figures/2020-02-22-16-42-55-image.png)

å…¬å¼$F_{i}^{b}=\mathbf{W}_{\mathbf{i}}^{\mathbf{b}} \otimes\left(D\left(F_{i-1}^{b}\right)+C_{i}+U\left(C_{i+1}\right)\right)$

$D\to$ downsample, $U\to$ upsample

#### Fusing-splitting FPN

ä¸Šè¿°ä¸¤ä¸ªFPNé¡ºåºé€æ¬¡äº§ç”Ÿï¼Œå…ˆäº§ç”Ÿçš„å±‚ä¼šå¯¹ä¹‹åå±‚å½±å“(unfair)

é¦–å…ˆåˆ†ç»„**fuse**é«˜å±‚å’Œä½å±‚çš„ä¸´è¿‘ä¸¤ç»„ç‰¹å¾

$\alpha_s=C_4+U(C_5), \alpha_l=D(C_2)+C_3$

ç„¶å**merge**é«˜å±‚å’Œä½å±‚çš„ç‰¹å¾

$\beta_s=\mathbf{W_s^f}\otimes \mathrm{cat}(\alpha_s,D(\alpha_l))$

$\beta_l=\mathbf{W_l^f}\otimes \mathrm{cat}(U(\alpha_s),\alpha_l)$

å†**split**äº§ç”Ÿä¸åŒå±‚çš„ç‰¹å¾

$F_2^f=U(\beta_l), F_3^f=\beta_l$

$F_4^f=\beta_s, F_5^f=D(\beta_s)$

![](Figures/4acfff9cac36f72acc7a0cc716f33e27be53fd47.png)

Ref: *MFPN: A NOVEL MIXTURE FEATURE PYRAMID NETWORK OF MULTIPLE
ARCHITECTURES FOR OBJECT DETECTION*

---

## Learning Data Augmentation Strategies for Object Detection

> **æ•°æ®å¢å¼º**ï¼Œé€šè¿‡æœç´¢æ¥**combine** transformations

æ•°æ®å¢å¼ºè§’åº¦ï¼š1. Learn a **generator** to create data 2. Learn a set of **transformations** applied to existing data(æœ¬æ–‡)

å¸¸ç”¨transformerï¼šimage mirrorï¼Œmulti-scale trainingï¼Œcrop-and-erase (occlude)ï¼Œcut-and-paste

è‡ªåŠ¨å­¦ä¹ æ•°æ®é›†å¯¹åº”çš„æ•°æ®å¢å¼ºæ–¹å¼ï¼šAutoAugment

Policy searché—®é¢˜ï¼šK=5ä¸ªsub-policiesï¼Œæ¯ä¸ªåŒ…å«N=2ä¸ªæ“ä½œã€‚è®­ç»ƒæ—¶éšæœºé€‰æ‹©sub-policyï¼Œé¡ºåºæ‰§è¡ŒNã€‚

æ“ä½œä¸¤ä¸ªå‚æ•°ã€Œæ‰§è¡Œæ“ä½œçš„æ¦‚ç‡ï¼Œæ“ä½œå¤§å°ç¨‹åº¦ã€ğŸ‘‡

![image-20200228183031352](/Users/mk/Library/Application Support/typora-user-images/image-20200228183031352.png)

![image-20200228192437277](/Users/mk/Library/Application Support/typora-user-images/image-20200228192437277.png)

#### Transform

1. **Color operations**: æ”¹å˜é¢œè‰²é€šé“ï¼Œobjä½ç½®ä¸å˜ `equalize, contrast brightness`
2. **Geometric ops**: æ”¹å˜objä½ç½®å’Œå¤§å° `rotate, ShearX, TranslationY`
3. **Bounding box ops.**: åªæ”¹å˜bboxå†…çš„å›¾åƒ `BBox_Only_Equalize, BBox_Only_Rotate, BBox_Only_FlipLR`

#### Results

`Rotate` æ—‹è½¬å›¾ç‰‡å’Œbboxï¼ˆ<u>best</u>ï¼‰

`Equalize` ç›´æ–¹å›¾å‡è¡¡åŒ–(Histogram equalization)ï¼Œå¹³è¡¡ä¸åŒç°åº¦åƒç´ å‡ºç°æ¦‚ç‡ï¼Œå¢å¤§å¯¹æ¯”åº¦ğŸ‘‡![A histogram which is zero apart from a central area containing strong peaks is transformed by stretching the peaked area to fill the entire x-axis.](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Histogrammeinebnung.png/300px-Histogrammeinebnung.png)

`BBox_Only_TranslateY` bboxå†…å‚ç›´å˜æ¢ï¼Œä¸Šä¸‹ç¿»è½¬

![image-20200228190923807](/Users/mk/Library/Application Support/typora-user-images/image-20200228190923807.png)

---

## YOLO

åˆ†æ ¼å­(grids)ï¼Œæ¯ä¸ªæ ¼å­åªé¢„æµ‹è§„å®šæ•°é‡bboxï¼Œåªæœ‰å½“gt boxçš„ä¸­å¿ƒç‚¹è½å…¥gridå†…æ—¶ï¼Œæ­¤gridè´Ÿè´£é¢„æµ‹è¿™ä¸ªgtã€‚(*æ½œåœ¨é—®é¢˜ï¼šå¯†é›†ç‰©ä½“ï¼Œå¤šä¸ªä¸­å¿ƒç‚¹è½å…¥åŒä¸€ä¸ªgridï¼Œæ¼æ£€*)

![img](Figures/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg)

ç½‘ç»œè¾“å‡º`(x,y,w,h), box_confidence_score`ï¼Œè¡¨ç¤ºnormalizedé•¿å®½å’Œ<u>ä¸­å¿ƒç‚¹</u>offsetï¼Œä»¥åŠç½®ä¿¡åº¦ã€Œè¡¨ç¤ºobjectnesså’Œä½ç½®å‡†ç¡®æ€§ã€

![img](Figures/1*e0VY6U1_WMF2KBoKQNZvkQ.png)

åˆ†grid$\to$æ¯ä¸ªgridäº§ç”Ÿkä¸ªé¢„æµ‹$\to$ä¿ç•™é«˜box_conf_scoreé¢„æµ‹

å…¶ä¸­$box\_conf\_score=P(object)\cdot IoU$ï¼Œ$conditional\_class\_prob=P(class_i|object)$

**class confidence score**: $box\_conf\_score\times cond\_cls\_prob=P(class_i)\cdot IoU$

è¡¨ç¤ºåˆ†ç±»å’Œå›å½’çš„å‡†ç¡®ç‡

![img](Figures/1*9ER4GVUtQGVA2Y0skC9OQQ.png)

**ç½‘ç»œç»“æ„**ğŸ‘†ï¼šä¸‹é‡‡æ ·+å…¨è¿æ¥å›å½’é¢„æµ‹

**Loss function**ï¼šåŒ…æ‹¬åˆ†ç±»æŸå¤±ï¼Œä½ç½®æŸå¤±ï¼Œobjectness ï¼ˆ$S\times S$grids, $B$ bbox each grids ï¼‰

1. **Classification loss**ï¼šç±»åˆ«ï¼Œcond_cls_prob

   $\sum_{i=0}^{S^{2}} \mathbb{1}_{i}^{\mathrm{obj}} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_{i}(c)\right)^{2}$

2. **Localization loss**ï¼šåªè®¡ç®—åŒ¹é…äº†gtçš„grid

   $\begin{array}{c}
   \lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_{i}\right)^{2}+\left(y_{i}-\hat{y}_{i}\right)^{2}\right] \\
   \quad+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_{i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_{i}})^{2}\right]
   \end{array}$

   where $\mathbb{1}^{\mathrm{obj}}_{ij}=1$è¡¨ç¤ºgrid içš„ç¬¬jä¸ªboxè´Ÿè´£é¢„æµ‹ç‰©ä½“ï¼Œé¢„æµ‹æ ¹å·æ¥ä½¿å¤§å°ç‰©ä½“è¯¯å·®å€¼å¯¹losså‡½æ•°è´¡çŒ®ä¸åŒã€Œè§å¹³æ–¹æ ¹å‡½æ•°ï¼Œxå°å¢é•¿å¿«ï¼Œxå¤§å¢é•¿æ…¢ã€‚å°å€¼è¯¯å·®å¢é•¿å¿«ï¼Œå¤§å€¼è¯¯å·®å¢é•¿æ…¢ã€

3. **Confidence loss**ï¼šobjectnessï¼ŒåŒºåˆ†å‰èƒŒæ™¯ï¼Œä½¿ç”¨$box\_conf\_score$å³$C_i$è®¡ç®—

   $\sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{obj}}\left(C_{i}-\hat{C}_{i}\right)^{2}$å’Œ$\lambda_{\mathrm{noobj}}\sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{i j}^{\mathrm{noobj}}\left(C_{i}-\hat{C}_{i}\right)^{2}$

é‡‡ç”¨**NMS**å»æ‰é‡å¤æ¡†

**æ²¡æœ‰RPNå¯ä»¥è®©ç½‘ç»œè·å¾—æ›´å¤šcontextï¼Œåˆ©äºåˆ†ç±»(fewer false pos.)**

### YOLOv2

**BN**ï¼Œ**é«˜åˆ†è¾¨ç‡** (`224x224`ä¸Špretrain backboneï¼Œ`448x448`ä¸Šfinetune)

**anchor** boxï¼Œå¯¹gridå†…Bä¸ªboxå¢åŠ å…ˆéªŒçŸ¥è¯†ï¼Œè§„å®šåˆå§‹scaleå’Œshapeï¼Œ<u>focus on a specific shape</u>ï¼Œè®­ç»ƒæ›´ç¨³å®š   ğŸ‘‡ä»å·¦åˆ°å³

**Anchoræœºåˆ¶é€šè¿‡é¢„å®šä¹‰scaleå’Œshapeæ¥å¼•å…¥å…ˆéªŒçŸ¥è¯†**ï¼Œbbox has strong patterns

![image-20200302192028409](Figures/image-20200302192028409.png)

å»æ‰FCå±‚ï¼Œä½¿ç”¨`1x1 conv`æ”¹å˜é€šé“ä¸º`7x7x((4+1+20)x5)`ï¼Œgridå†…5ä¸ªanchor

![img](Figures/1*UsqjfoW3sLkmyXKQ0Hyo8A.png)

ç‰¹å¾å›¾å¥‡æ•°åˆ†è¾¨ç‡ï¼šå¤§ç‰©ä½“å¤„äºå›¾ç‰‡ä¸­å¿ƒï¼Œå¥‡æ•°æ›´å¥½åˆ†which grid

å»æ‰pooling

anchor**èšç±»**ç¡®å®špredefined scale&shapeï¼ˆğŸ‘‡æ•°æ®ç‚¹ä¹‹é—´<u>è·ç¦»è¡¨ç¤ºIoU</u>å¤§å°ï¼Œä½ç½®æ— æ„ä¹‰ï¼‰æ¯ç±»anchoré…ç½®çœ‹ä½œä¸€ä¸ªcluster

![img](Figures/1*esSmI0UaMr-GrqkUGMg0hA.jpeg)

é¢„æµ‹offset `[tx, ty, tw, th]`å‡å°‘ç½‘ç»œé¢„æµ‹å–å€¼èŒƒå›´ï¼Œå¢å¤§å¯è¡¨ç¤ºçš„æ•°å€¼èŒƒå›´ğŸ‘‡

![img](Figures/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg)

å¢åŠ **passthrough**ï¼Œç±»ä¼¼skip-connectionã€‚å’Œæµ…å±‚ç‰¹å¾å›¾`concat`é¢„æµ‹å°ç›®æ ‡

**Multi-Scale Training**é‡‡ç”¨å¤šä¸ªå°ºåº¦è®­ç»ƒé€‚åº”å°ºåº¦å˜åŒ– `320x320, 352x352,..., 608x608` 10ä¸ªbatchçš„ä¸åŒå°ºåº¦å›¾ç‰‡è®­ç»ƒ

ä½¿ç”¨**DarkNet**ä½œä¸ºbackboneğŸ‘‡

![img](Figures/1*NBnDpz8fitkhcdnkgF2bvg.png)

### YOLO9000

ä½¿ç”¨hierarichical classificationè®­ç»ƒyoloï¼Œä½¿ç”¨WordTreeå°†åˆ†ç±»å’Œæ£€æµ‹æ•°æ®é›†æ··åˆè®­ç»ƒï¼Œåˆ†9418ç±»

### YOLOv3

**Multi-label classification**ï¼šè¾“å‡ºä¸€ä¸ªlabel idè€Œä¸æ˜¯catç»´å‘é‡ï¼Œç›´æ¥è¾“å‡ºexclusive outputï¼Œä½¿ç”¨binary cross-entropy lossè®­ç»ƒ

æ¯ä¸ªç›®æ ‡åªåŒ¹é…ä¸€ä¸ªanchorï¼Œæ²¡æœ‰åŒ¹é…çš„anchorä¸è®¡ç®—clså’ŒlocæŸå¤±ï¼Œåªè®¡ç®—objectness

**FPN**ï¼š<u>åœ¨3ä¸ªå°ºåº¦çš„ç‰¹å¾å›¾ä¸Šé¢„æµ‹ï¼Œæ¯ä¸ªgridé¢„æµ‹3ä¸ªanchor</u>ï¼Œä¸€å…±9ç§anchoré…ç½®

**Residual+DarkNet53**ï¼šå·ç§¯å¢åŠ skip-connection

![img](Figures/1*biRYJyCSv-UTbTQTa4Afqg.png)

**FPN**

![img](Figures/70.jpeg)

Ref: https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088

---

## IoU

è¯„ä»·é¢„æµ‹å’Œgtçš„è·ç¦»ï¼Œå›å½’ç›®æ ‡ã€‚å…·æœ‰**å°ºåº¦ä¸å˜å½¢**

$\operatorname{IoU}=\frac{|A\cap B|}{|A\cup B|}$

è®­ç»ƒ$L_{IoU}=1-\operatorname{IoU}$

é—®é¢˜ï¼š

1. <u>æ²¡æœ‰é‡å </u>æ—¶ IoU=0ï¼Œæ²¡æœ‰æ¢¯åº¦æ— æ³•ç”¨IoU lossè®­ç»ƒ

2. æ— æ³•å¾ˆå¥½åæ˜ æ–¹å‘ä¸ä¸€è‡´æ—¶é‡å 

   ![image-20200303220421986](Figures/image-20200303220421986.png)

#### GIoU (Generalized)

$\operatorname{GIoU}=\operatorname{IoU}-\frac{|C\backslash(A\cap B)|}{|C|}$

å…¶ä¸­$C$ä¸ºåŒ…å«Aå’ŒBçš„æœ€å°å‡¸å¤šè¾¹å½¢(enclosing convex)ï¼Œå¤šä¸ºçŸ©å½¢

Aå’ŒBä¸é‡åˆæ—¶ä¹Ÿå¯ä»¥ä¼˜åŒ–ï¼ŒèŒƒå›´$[-1,1]$ï¼Œä¸é‡åˆæ—¶ä¸ºè´Ÿæ•°(provide moving direction)

å…³æ³¨**å½¢çŠ¶ä¹‹é—´ç¼éš™å‡å°**ï¼Œå¦‚ğŸ‘†2,3ä¸­ç¼éš™å¯¼è‡´GIoUæ›´å°

![img](Figures/v2-a6311e2e892658c42d52b939cb467975_1440w.jpg)

ğŸ‘†<font color="#00dd00">ä¸é‡å æ ·æœ¬</font>ï¼ŒIoU=0ï¼Œè€ŒGIoUä¸ºè´Ÿå€¼ï¼Œæœ‰æ¢¯åº¦

ğŸ‘†<font color="#666600">é‡å æ ·æœ¬</font>ï¼Œä¸æ–­ä¼˜åŒ–è¿‡ç¨‹ä¸­GIoU$\to$IoU

<u>ä½¿ç”¨IoU Loss $L_{IoU}$æˆ–$L_{GIoU}$è®­ç»ƒ</u>ï¼Œç›¸æ¯”Sommoth-L1å’ŒMSEèƒ½å¸¦æ¥æ€§èƒ½æå‡

#### DIoU (Distance)

> å¥½çš„bounding box regres. æ ‡å‡†éœ€è¦è€ƒè™‘ä¸‰ä¸ªå› ç´ ï¼š**Overlap, Center Distance, Aspect Ratio**

ç›´æ¥ä¼˜åŒ–ä¸¤ä¸ªæ¡†ä¸­å¿ƒç‚¹çš„è·ç¦»

$\operatorname{DIoU}=\operatorname{IoU}-\frac{\rho^2(b,b^{gt})}{c^2}$

å…¶ä¸­$\rho$ (or å›¾ä¸­$d$)è¡¨ç¤ºä¸­å¿ƒç‚¹çš„æ¬§å¼è·ç¦»ï¼ˆ**L1å¯ä»¥å—ï¼Ÿ**ï¼‰ï¼Œ$c$è¡¨ç¤ºåŒ…å«ä¸¤ä¸ªæ¡†çš„æœ€å°é—­åŒ…åŒºåŸŸçš„å¯¹è§’çº¿è·ç¦»ğŸ‘‡

![image-20200303230451277](Figures/image-20200303230451277.png)

ç›¸æ¯”GIoUï¼šGIoUæ›´å¼ºè°ƒå¯¹é½ï¼Œåªè¦å¯¹é½ä¹‹åæ²¡æœ‰æ¢¯åº¦ï¼Œå¦‚ğŸ‘‡ï¼Œé¢„æµ‹è“è‰²æ¡†å’ŒGTç»¿è‰²å¯¹é½ï¼Œæ²¡æœ‰ç¼éš™ï¼ŒGIoU term=0ã€Œä¸€æ¡†åŒ…æ‹¬å¦ä¸€çš„æƒ…å†µã€

![image-20200303230816475](Figures/image-20200303230816475.png)

è€ŒDIoUç›´æ¥ä¼˜åŒ–æ¡†ä¸­å¿ƒçš„é‡åˆï¼Œè·ç¦»è¿‘ğŸ‘‡

![image-20200303230931086](Figures/image-20200303230931086.png)

![image-20200303232843528](Figures/image-20200303232843528.png)

ğŸ‘†IoUä¸é‡å lossé«˜ï¼ŒGIoUæ­£å‚ç›´æ°´å¹³æ–¹å‘lossé«˜ï¼ˆå¦‚ğŸ‘†ğŸ‘†ğŸ‘†å›¾ï¼‰ï¼ŒDIoUè¾ƒä½

DIoUä¹Ÿå¯ç”¨äºNMSä¸­DIoU-NMS

#### CIoU (Complete)

$\operatorname{CIoU}=\operatorname{IoU}-\frac{\rho^2(b,b^{gt})}{c^2}-\alpha v$

$v=\frac{4}{\pi^2}(\arctan{\frac{w^{gt}}{h^{gt}}}-\arctan{\frac{w}{h}})^2$

$\alpha=\frac{v}{(1-\operatorname{IoU})+v}$

ä½¿ç”¨å„ç§IoU lossè®­ç»ƒğŸ‘‡

![image-20200303233112636](Figures/image-20200303233112636.png)

---

## HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces

> åœ¨çº¿åŒ¹é…ï¼Œå…ˆå›å½’å‡ºæ¡†ï¼Œå†anchor-targetåŒ¹é…è®¡ç®—loss

#### å®éªŒ/Motivation

![image-20200316221449869](Figures/image-20200316221449869.png)

ğŸ‘†anchorå¤§ï¼Œæ¯ä¸ªäººè„¸åŒ¹é…åˆ°çš„æ•°é‡å˜å¤šï¼Œä½†æ˜¯åŒ¹é…åˆ°äººè„¸å æ‰€æœ‰äººè„¸ä¸­å æ¯”ä¸‹é™ï¼Œäººè„¸recallä¸‹é™

ğŸ‘†anchorå°ï¼Œæ¯ä¸ªäººè„¸åŒ¹é…åˆ°çš„anchoræ•°é‡ä¸‹é™ï¼Œä½†æ˜¯å¤§å¤šæ•°äººè„¸éƒ½æœ‰åŒ¹é…anchorï¼Œäººè„¸recallä¸Šå‡

![image-20200316221821820](Figures/image-20200316221821820.png)

ğŸ‘†0.35ä¸ºanchorå’Œtarget matchçš„thresholdï¼Œæ‰€ä»¥89%çš„anchoréƒ½æ²¡æœ‰è¢«match

![image-20200316221942813](Figures/image-20200316221942813.png)

==å…³é”®== çºµåæ ‡ä»£è¡¨faceæ•°é‡

match 1ä¸ªanchorçš„faceæœ‰2492å¼ ï¼Œanchorèƒ½äº§ç”Ÿçš„æ­£ç¡®é¢„æµ‹æ¡†ï¼ˆ`IoU>0.5`ï¼ŒCorrectly Predicted Bounding Boxï¼‰å¯¹åº”çš„äººè„¸æœ‰1968å¼  $\to$ <u>*å¤§å¤šæ•°äººè„¸éƒ½èƒ½é€šè¿‡anchoräº§ç”Ÿä¸€ä¸ªIoUé«˜çš„é¢„æµ‹æ¡†*</u>

é¢„æµ‹æ¡†ç»è¿‡NMSä¹‹åèƒ½ä¿ç•™ä¸‹æ¥çš„äººè„¸åªæœ‰343å¼  $\to$ *<u>å¤§å¤šæ•°äººè„¸ç»è¿‡anchoräº§ç”Ÿçš„é¢„æµ‹æ¡†éƒ½è¢«NMSè¿‡æ»¤æ‰ï¼Œè€Œå¯¼è‡´è¿™äº›faceæ¼æ£€</u>*

ä½†æ˜¯NMSåªåˆ é™¤ä¸€ä¸ªä½ç½®é‡å¤çš„æ¡†(IoUè¿‡å¤§)ï¼Œå¯¹äºæ¼æ£€çš„äººè„¸ï¼Œåªè¦æœ‰æ¡†coverï¼Œå°±ä¸€å®šä¼šä¿ç•™ï¼ŒNMSååŒä¸€ä¸ªä½ç½®è‡³å°‘ä¿ç•™ä¸€ä¸ªscoreæœ€å¤§çš„æ¡† $\to$ NMSåå¯¼è‡´æ¼æ£€çš„1625å¼ faceï¼Œäº§ç”Ÿäº†CPBB(`IoU>0.5`)ï¼Œä½†æ˜¯NMSè¢«åˆ æ‰ $\to$ IoUè¶³å¤Ÿå¤§ï¼Œä½†æ˜¯å¾—åˆ†å¤ªä½ï¼ˆä½äº`cls_threshold`)ï¼ŒNMSæ—¶è¿‡ä½scoreçš„å¿½ç•¥æ‰(ä¸ä¼šè€ƒè™‘IoU) $\to$ *<u>æ˜¯ç”±äºè®­ç»ƒçš„æ—¶å€™æ­¤anchoræ²¡æœ‰matchï¼Œåˆ†ç±»åˆ†æ”¯è®­ç»ƒç›®æ ‡ä¸ºBGï¼Œåˆ†ç±»ç½‘ç»œé™ä½äº†score</u>*ã€Œæœ¬è´¨ä¸ºIoUå’Œscoreçš„mismatchã€

ğŸ‘†**ç»“è®ºï¼šä½IoUè€Œæ²¡æœ‰è¢«matchçš„anchorä¹Ÿèƒ½äº§ç”Ÿå¾ˆå¥½çš„é¢„æµ‹æ¡†(CPBB)**ï¼Œéœ€è¦è¢«matchä¸ºç‰©ä½“ï¼Œæé«˜å…¶åˆ†ç±»scoreã€‚è¿™äº›anchorè´Ÿè´£çš„faceå¤šä¸ºouter face(éš¾æ ·ä¾‹)ï¼Œä¸Šè¿°ä¹Ÿä¸ºouter faceæ¼æ£€çš„åŸå› ï¼ˆä½IoUæ¡†è¢«unmatchï¼Œæ— æ³•è®­ç»ƒï¼Œåˆ†ç±»ç½‘ç»œä¸èƒ½åˆ†ç±»ä¸ºé«˜åˆ†ï¼‰

#### HAMBox (Online High-quality Anchor Mining)

é€‰æ‹©å¤§anchorï¼Œé€šè¿‡OHAMæ¥è¿›è¡Œå¼¥è¡¥æ²¡æœ‰anchor matchçš„face

**ä¼ ç»Ÿmatchç­–ç•¥**ï¼šä¸€ä¸ªface/targeté¦–å…ˆmatchæ‰€æœ‰<u>å’Œå®ƒIoUå¤§äºthreshold</u>çš„anchorã€‚æ­¤åï¼Œå¯¹äºæ²¡æœ‰anchorå’Œå®ƒIoUé«˜äºé˜ˆå€¼çš„faceï¼Œé€‰æ‹©<u>å’Œå®ƒIoUæœ€å¤§</u>çš„anchoråŒ¹é…è¿›è¡Œè¡¥å……ï¼ˆonly oneï¼‰

**OHAM**ï¼š1) matchæ‰€æœ‰anchor IoUå¤§äºthresholdçš„faceï¼Œå¯¹äºæ²¡æœ‰anchoråŒ¹é…çš„faceï¼Œä¸è¿›è¡Œcompensate. 2) å¯¹æ‰€æœ‰æ¡†å›å½’è®¡ç®—bbox. 3) å¯¹æ‰€æœ‰æ²¡æœ‰åŒ¹é…anchorçš„faceï¼Œè®¡ç®—é¢„æµ‹æ¡†å’Œfaceçš„IoUï¼Œå¯¹å…¶è¿›è¡Œå¼¥è¡¥ï¼Œ$\operatorname{IoU}>\operatorname{threshold'}$ ã€Œæ²¡æœ‰åŒ¹é…æˆ–åŒ¹é…æ•°é‡ä¸è¶³ (K anchor bag)ã€

**è®¡ç®—Losså­¦ä¹ æ—¶ï¼Œä½¿ç”¨å›å½’åçš„bboxå’ŒtargetåŒ¹é…ï¼Œæ¥å¼¥è¡¥ç”¨åŸå§‹anchorå’ŒtargetåŒ¹é…çš„æ•°é‡ä¸è¶³é—®é¢˜**

#### Regression-aware Focal Loss

$\begin{array}{l}
L_{c l s}\left(p_{i}\right)=\frac{1}{N_{c o m}} \sum_{i \in \psi} F_{i} L_{f l}\left(p_{i}, g_{i}^{*}\right) \\
+\frac{1}{N_{n o r m}} \sum_{i \in \Omega}\left(1_{\left(l_{i}^{*}=0\right)} 1_{\left(F_{i}<0.5\right)}+1_{\left(l_{i}^{*}=1\right)}\right) L_{f l}\left(p_{i}, l_{i}^{*}\right)
\end{array}$

å…¶ä¸­$\psi$è¡¨ç¤ºå¼¥è¡¥çš„anchorï¼Œ$N_{com}$ä¸ºæ•°é‡ï¼Œpé¢„æµ‹labelï¼Œg=gt

$\Omega$ä¸ºmatched anchorå’Œunmatched low-quality (IoU<0.5) anchorã€Œå³unmatched hq anchorä¸è¿›è¡Œè®­ç»ƒï¼Œåº”è¯¥hqä»æœªè¢«matchè¡¨ç¤º<u>ç®€å•æ ·ä¾‹ä¸Šå¤šä½™(>K)çš„anchor</u>ã€ï¼Œ$N_{norm}$ä¸ºæ•°é‡ï¼Œ$F_i$è¡¨ç¤ºIoUï¼Œ$l_i^*$è¡¨ç¤ºç¬¬ä¸€æ¬¡matchçš„label

$\begin{aligned}
L_{l o c}\left(x_{i}\right) &=\frac{1}{N_{c o m}} \sum_{i \in \psi} L_{S m o o t h L 1}\left(x_{i}, x_{i}^{*}\right) \\
&+\frac{1}{N_{n o r m}} \sum_{i \in \Omega} L_{S m o o t h L 1}\left(x_{i}, x_{i}^{*}\right)
\end{aligned}$

![image-20200316225903344](Figures/image-20200316225903344.png)

---

## [RFB-Net] Receptive Field Block Net for Accurate and Fast Object Detection

> ä¸åŒæ„Ÿå—é‡ï¼Œå¯¹åº”ä¸åŒæ‰©å¼ (dilation)
>
> **ä¸æ˜¯å¤šå°ºåº¦çš„ç‰¹å¾å›¾ï¼Œè€Œæ˜¯ä¸åŒæ„Ÿå—é‡å¤§å°çš„ç‰¹å¾å›¾**

![image-20200321165433401](Figures/image-20200321165433401.png)

ä¹‹å‰åªå˜æ„Ÿå—é‡(receptive field/kernel size)ï¼Œæˆ–è€…åªå˜æ‰©å¼ å°ºåº¦(ASPP)

RFBæå‡ºæ„Ÿå—é‡å’Œæ‰©å¼ å°ºåº¦åº”è¯¥åŒæ—¶å˜åŒ–ã€Œç›¸äº’å½±å“ã€

ğŸ‘‡åœˆåªè¡¨ç¤ºæ„Ÿå—é‡å¤§å°ï¼Œ<u>å¤§çš„kernelå¯¹åº”å¤§çš„dilateï¼Œä½¿æ„Ÿå—é‡æ›´å¤§</u>

![image-20200321165623460](Figures/image-20200321165623460.png)

å®ç°ä¸Š

![image-20200321165853355](Figures/image-20200321165853355.png)

ä½¿ç”¨ä¸¤ä¸ª`3x3`ä»£æ›¿`5x5`ã€‚**æ³¨æ„paddingï¼Œæ‰€æœ‰éƒ½ä¸ºsame size(k=3, p=1)ï¼Œæ¯ä¸ªåˆ†æ”¯äº§ç”Ÿçš„ç‰¹å¾å›¾å¤§å°ç›¸åŒ**

```python
self.branch0 = nn.Sequential(
                Conv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),
                Conv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual, dilation=visual, relu=False)
                )
        self.branch1 = nn.Sequential(
                Conv(in_planes, inter_planes, kernel_size=1, stride=1),
                Conv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),
                Conv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual+1, dilation=visual+1, relu=False)
                )
        self.branch2 = nn.Sequential(
                Conv(in_planes, inter_planes, kernel_size=1, stride=1),
                Conv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),
                Conv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),
                Conv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=2*visual+1, dilation=2*visual+1, relu=False)
                )
```

---

## [ASFF] Learning Spatial Fusion for Single Shot Object Detection

> å¤šå°ºåº¦ç‰¹å¾å›¾èåˆ

![image-20200321192646888](Figures/image-20200321192646888.png)

ç‰¹å¾é¦–å…ˆç»è¿‡resizeï¼Œå†èåˆã€‚*resizeå¯ä½¿ç”¨deconv/conv, æ’å€¼/pooling*

$\mathbf{y}_{i j}^{l}=\alpha_{i j}^{l} \cdot \mathbf{x}_{i j}^{1 \rightarrow l}+\beta_{i j}^{l} \cdot \mathbf{x}_{i j}^{2 \rightarrow l}+\gamma_{i j}^{l} \cdot \mathbf{x}_{i j}^{3 \rightarrow l}$

Where $\alpha_{i j}^{l}=\frac{e^{\lambda_{\alpha_{i j}}^{l}}}{e^{\lambda_{\alpha_{i j}}^{l}}+e^{\lambda_{\beta_{i j}}^{l}}+e^{\lambda_{\gamma_{i j}}^{l}}}$, etc

and $\lambda_{\alpha}^l$, $\lambda_{\beta}^l$, $\lambda_{\gamma}^l$ computed (`1x1` conv) from $\mathbf{x}^{1\to l}$, $\mathbf{x}^{2\to l}$, $\mathbf{x}^{3\to l}$ **respectively**

<u>å¯ä»¥çœ‹ä½œäº§ç”Ÿä¸€ä¸ªæ¡†feature pyramidå¤šä¸ªç‰¹å¾å›¾éƒ½ç”¨åˆ°ï¼Œä¹‹å‰åªç”¨ä¸€ä¸ªç‰¹å¾å›¾äº§ç”Ÿä¸€ä¸ªæ¡†</u>

<u>**è®­ç»ƒtricks**ï¼šmixup algorithm, cos lr, sync bn, bag of freebies</u>

---

## Accelerating Object Detection by Erasing Background Activation

> Objectness-aware object detection, äº§ç”ŸFG/BGçš„maskï¼Œåªå¯¹maskåŒºåŸŸè®¡ç®—

å›¾ç‰‡åªæœ‰å°éƒ¨åˆ†æœ‰ç‰©ä½“ï¼ŒèƒŒæ™¯åŒºåŸŸä¸éœ€è¦ç‰¹å¾æå–è®¡ç®—ï¼Œ**åªå¯¹å‰æ™¯maskåŒºåŸŸè®¡ç®—ç‰¹å¾**ï¼Œåˆ†ç±»&å›å½’(æ¬¡è¦)å‡ºbbox

![image-20200324230634744](Figures/image-20200324230634744.png)

OMGç½‘ç»œä¸ºFast-SCNNğŸ‘‡

![img](Figures/20190313161834869.png)

ä½¿ç”¨element-wise mulæ¥zero-outèƒŒæ™¯çš„feature map

OMGä¸­æœ‰`argmax`æ“ä½œï¼Œä¸ºäº†end2end trainingï¼Œå¯ä»¥1) ä½¿ç”¨`soft-argmax`ä»£æ›¿`argmax`è®­ç»ƒ 2) ä½¿ç”¨<u>surrogate gradient</u>ï¼ŒFPä½¿ç”¨`argmax`ï¼ŒBPæ—¶ä½¿ç”¨`soft-argmax`ä»£æ›¿è·å¾—è¿‘ä¼¼æ¢¯åº¦

$\operatorname{soft-argmax}(x)=\sum_{i} \frac{e^{\beta x_{i}}}{\sum_{j} e^{\beta x_{j}}} i$

**å®éªŒ**ï¼š å¯¹æ¯”MACï¼Œä½¿ç”¨ä¸åŒmaskç›‘ç£

---

## Libra R-CNN: Towards Balanced Learning for Object Detection

> è®­ç»ƒæ–¹å¼ï¼Œä¸å¹³è¡¡é—®é¢˜ï¼šhard example IoUåˆ†å¸ƒä¸å¹³è¡¡ï¼Œmulti-level/res featureèåˆä¸å¹³è¡¡ï¼Œä¸åŒlossæ ·æœ¬äº§ç”Ÿçš„æ¢¯åº¦ä¸å¹³è¡¡
>
> æœ‰æ¢¯åº¦åæ¨losså‡½æ•°è®¾è®¡

**ç›®æ ‡æ£€æµ‹å™¨è®­ç»ƒç›®æ ‡**ï¼š

1. Selected region samples are representative
2. Extracted visual feature are fully utilized
3. Designed objective function is optimal

å¸¸è§è®­ç»ƒæœ‰ä¸‰å±‚æ¬¡çš„imbalance

1. <u>Sample-level</u>: hard exampleéœ€è¦å¤šè®­ç»ƒï¼Œä½†OHEMå¯¹å™ªå£°æ•æ„Ÿ
2. <u>Feature-level</u>: pyramidä¸åŒlevel/resçš„ç‰¹å¾å¤„ç†æ·±åº¦ä¸åŒï¼Œé«˜å±‚å¤„ç†å¤šï¼Œæµ…/åº•å±‚ç‰¹å¾å¤„ç†å°‘
3. <u>Objective-level</u>: cls/regä¸¤ä¸ªä»»åŠ¡æŸå¤±å‡½æ•°åè°ƒ

#### IoU-balanced Sampling

æ ¹æ®æ ·æœ¬å’ŒGTçš„IoUï¼Œåˆ†æˆå¤šä¸ªbinï¼Œæ¯ä¸ªbinå‡åŒ€é‡‡æ ·

ç¬¬k binä¸­æ¯ä¸ªæ ·æœ¬é‡‡æ ·æ¦‚ç‡$p_k=\frac{N}{K}*\frac{1}{M_k},\;k\in[0,K)$

<u>**æ ·æœ¬å„IoUå‡åŒ€åˆ†å¸ƒ**</u>

#### Balanced Feature Pyramid

ä½¿ç”¨åŒæ ·æ·±çš„ç½‘ç»œæ¥å¤„ç†ä¸åŒå±‚çš„ç‰¹å¾

resizeä¸åŒå±‚featureï¼Œ**å–å¹³å‡**ï¼›ä½¿ç”¨Gaussian **non-local attention**å¢å¼ºèåˆçš„ç‰¹å¾ã€‚åœ¨resizeä¼šåŸå…ˆçš„å°ºåº¦å¢å¼ºmulti-scaleç‰¹å¾ğŸ‘‡

![image-20200325222120052](Figures/image-20200325222120052.png)

#### Balanced L1 Loss

==**ä»æ¢¯åº¦çš„è§’åº¦è€ƒè™‘**== promoting the crucial gradient: ç²¾ç¡®çš„æ ·æœ¬(inlier)çš„æ¢¯åº¦æ›´é‡è¦ï¼Œå¢å¼ºlosså°çš„æ ·æœ¬çš„æ¢¯åº¦(<u>æ­£ç¡®çš„æ¢¯åº¦ï¼Œæ•°é‡å°‘è¦å¢å¼º</u>)ï¼Œå‡å¼±losså¤§æ ·æœ¬çš„æ¢¯åº¦(<u>éš¾è®­ç»ƒï¼Œå¤§æ¢¯åº¦å¯¼è‡´è®­ç»ƒä¸ç¨³å®š</u>)ğŸ‘‡

![image-20200325232936319](Figures/image-20200325232936319.png)

$\frac{\partial L_{b}}{\partial x}=\left\{\begin{array}{ll}
\alpha \ln (b|x|+1) & \text { if }|x|<1 \\
\gamma & \text { otherwise }
\end{array}\right.$

ğŸ‘†å¢å¼ºå°lossçš„æ¢¯åº¦ï¼Œå¤§lossçš„æ¢¯åº¦clipã€‚<u>**å¤§å°lossæ ·æœ¬äº§ç”Ÿçš„æ¢¯åº¦å¹³è¡¡**</u>

$(x,y)\to Loss \to gradient$ï¼Œ**é€šè¿‡åˆ†ææ¢¯åº¦ï¼Œåæ¨å›lossè®¾è®¡**

ç§¯åˆ†å¯å¾—ğŸ‘‡

$L_{b}(x)=\left\{\begin{array}{ll}
\frac{\alpha}{b}(b|x|+1) \ln (b|x|+1)-\alpha|x| & \text { if }|x|<1 \\
\gamma|x|+C & \text { otherwise }
\end{array}\right.$

å…¶ä¸­$x$ä¸ºGTå’Œpredçš„bboxåæ ‡å·®è·ï¼Œ$\gamma$ä¸ºclipç•Œ (å¤§äº$\gamma$ï¼Œæ¢¯åº¦æ’å®šä¸º1)ï¼Œ$\alpha$æ§åˆ¶å¯¹å°lossçš„æ¢¯åº¦å¢å¼ºï¼Œ$b$ä¸ºå¹³è¡¡é¡¹ï¼Œæ±‚å‡ºæ¯ä¸ªä½ç½®lossåmeanæˆ–sumï¼Œå³$L_{l o c}=\sum_{i \in\{x, y, w, h\}} L_{b}\left(t_{i}^{u}-v_{i}\right)$ğŸ‘‡

![image-20200325234316019](Figures/image-20200325234316019.png)

#### Pipeline

![image-20200325234511396](Figures/image-20200325234511396.png)

---

## RepPoints: Point Set Representation for Object Detection

> Bboxç‰¹å¾ä¸å¯¹é½é—®é¢˜ï¼Œæä¾›æ›´å¥½çš„objectè¡¨ç¤ºæ–¹æ³•(ä¸æ˜¯ä¸­å¿ƒç‚¹é¢†åŸŸå·ç§¯)
>
> Deformable convå‡çº§ç‰ˆï¼ŒRepresentative Points
>
> ç›¸æ¯”deformableï¼Œ**ç›´æ¥ä½¿ç”¨å˜å½¢åçš„ç»“æœä½œä¸ºé¢„æµ‹çš„bboxä½ç½®ï¼ˆor anchorçš„åç§»ï¼‰**ï¼Œè€Œä¸æ˜¯explicitå›å½’xywhã€‚ä¸­å¿ƒç‚¹ + implicit-offsets + wh
>
> **é‡‡æ ·ç‚¹åŒæ—¶ç”¨æ¥æå–è¯­ä¹‰å¯¹é½çš„ç‰¹å¾ï¼Œåˆç”¨æ¥è¡¨ç¤ºç‰©ä½“çš„å‡ ä½•å½¢æ€**

![img](Figures/v2-a1bdfbed69d464815ddcd8f2ed097e96_1440w.jpg)

å¤§éƒ¨åˆ†ä¸ºèƒŒæ™¯ï¼Œéœ€è¦é€‰æ‹©æ›´representativeçš„ç‚¹æ±‚ç‰¹å¾

![image-20200326202246723](Figures/image-20200326202246723.png)

ğŸ‘†**è®­ç»ƒ**ï¼šåˆ†ç±»ç½‘ç»œé‡‡ç”¨deformable convå³å¯ï¼Œå›å½’ç½‘ç»œå…ˆæŠŠreppointsè¡¨ç¤ºè½¬ä¸ºbboxè¡¨ç¤º(pseudo box)ï¼Œç„¶åè®¡ç®—å’ŒGTçš„offsetï¼ˆpoint lossï¼‰

è½¬æ¢æ–¹å¼ï¼š1) <u>RepPoint setåæ ‡æœ€å€¼</u> 2) subsetåæ ‡æœ€å€¼ 3) ä½¿ç”¨meanå’Œdeviationä¼°è®¡ï¼Œ$\mu$ä¼°è®¡ä¸­å¿ƒç‚¹ï¼Œ$\sigma$ä¼°è®¡å°ºåº¦(wh)

![image-20200326201955046](Figures/image-20200326201955046.png)

ğŸ‘†ä½¿ç”¨ç½‘ç»œè®¡ç®—åç§»é‡(offsets over the center points)ï¼Œå¾—åˆ°reppointçš„ç‚¹/ç‰©ä½“ç‰¹å¾è¡¨ç¤º$\mathcal{R}=\{(x_k,y_k)\}^n_{k=1}$ï¼Œnä¸ªç‚¹ç‰¹å¾è¡¨ç¤ºsample points/objectçš„ç‰¹å¾ã€‚å­¦ä¹ $\mathcal{R}_{r}=\left\{\left(x_{k}+\Delta x_{k}, y_{k}+\Delta y_{k}\right)\right\}_{k=1}^{n}$

ğŸ‘†ç»´æŠ¤ä¸¤ä¸ªRepPoint setï¼ŒäºŒæ¬¡refine

**Learned via weak localization supervision from rectangular ground-truth boxes and implicit recognition feedback**

**ä½¿ç”¨åŸºäºä¸­å¿ƒç‚¹xyè€Œä¸æ˜¯xywhé¢„æµ‹bboxï¼Œå‡å°‘hypothesis spaceï¼Œä¸€æ¬¡åªéœ€è¦é¢„æµ‹2D vecï¼Œæ›´å¥½è®­ç»ƒ**

#### RPDet

![image-20200326203026414](Figures/image-20200326203026414.png)

ğŸ‘†backbone FPN (å¤šå°ºåº¦å³å¯è§£å†³åŸºäºç‚¹é¢„æµ‹çš„é‡åˆç‰©ä½“é—®é¢˜ï¼ŒåŒFCOS)

ğŸ‘†ä¸¤ä¸ªåˆ†æ”¯ï¼šlocateåˆ†æ”¯ä¸¤æ¬¡è®¡ç®—offset refineï¼Œclassåˆ†æ”¯ç”¨offsetå˜å‹å·ç§¯ï¼ˆdconv=deformable convï¼‰

Ref: https://www.zhihu.com/question/322372759/answer/798327725

---

## Learning Rich Features at High-Speed for Single-Shot Object Detection (LSN)

> åˆ†ç±»ä»»åŠ¡é¢„è®­ç»ƒå’Œæ£€æµ‹ä»»åŠ¡gapï¼Œfeature pyramidèåˆ
>
> Light-weight Scratch Networkäº§ç”Ÿå‡†ç¡®åº•å±‚ç‰¹å¾è¾“å…¥FPN
>
> åº•å±‚ç‰¹å¾é«˜å±‚ç‰¹å¾åŒå‘ä¼ æ’­

![image-20200424161122580](Figures/image-20200424161122580.png)

#### LSN

è¾“å…¥ä¸ºdownsampleåçš„åŸå§‹å›¾ç‰‡ï¼Œä½åˆ†è¾¨ç‡æµ…å±‚ç½‘ç»œ **train from scratch**

![image-20200424161326166](Figures/image-20200424161326166.png)

#### Bi-directional Network

**Bottom-up Scheme**ğŸ‘†ğŸ‘†(b)ï¼š$f_{k}=\phi_{k}\left(\left(s_{k} \otimes o_{k}\right) \oplus\left(w_{k-1} f_{k-1}\right)\right)$

$s_k$ä¸ºLSNçš„ç‰¹å¾è¾“å‡ºï¼Œ$o_k$ä¸ºSSD (baseline)çš„è¾“å‡ºï¼Œ$f_{k-1}$ä¸ºä¸Šä¸€å±‚ç‰¹å¾ï¼Œ<u>cascadeä¾æ¬¡è®¡ç®—</u>

**Top-down Scheme**ğŸ‘†ğŸ‘†(c)ï¼š$$b_{k}=\gamma_{k}\left(\sum\left(W_{k} f_{k}, W_{m k}\left(\sum_{k+1}^{n} \mu_{k}\left(W_{i} f_{i}\right)\right)\right)\right)$$

$W_k$ä¸º`1x1`convé™é€šé“ï¼Œ$W_{mk}$ä¸º`1x1` convèåˆç‰¹å¾ï¼Œ<u>denseèåˆæ‰€æœ‰ä¸Šå±‚ç‰¹å¾(low-res)</u>ï¼Œ$\mu_k$ä¸º`upsample`

å…ˆç»è¿‡bottom-upå†top-downï¼Œç”¨top-downè¾“å‡ºé¢„æµ‹

#### Experiment

![image-20200424162957980](Figures/image-20200424162957980.png)

![image-20200424163032355](Figures/image-20200424163032355.png)

---

## Detection in Crowded Scenes: One Proposal, Multiple Predictions

> ä¸€ä¸ªanchor/å€™é€‰æ¡†è´Ÿè´£é¢„æµ‹å¤šä¸ªç‰©ä½“ã€‚anchor-GT ä¸€å¯¹å¤š
>
> å¯†é›†è¡Œäººæ£€æµ‹ã€‚å¯†é›†ï¼Œé‡å /é®æŒ¡

![image-20200501145202227](Figures/image-20200501145202227.png)

ä¹‹å‰ï¼šä¸€ä¸ªanchorè´Ÿè´£é¢„æµ‹ä¸€ä¸ªç‰©ä½“ï¼›æå‡ºï¼š<u>ä¸€ä¸ªanchoré¢„æµ‹ä¸€ç»„</u>

å¯¹äºä¸€ä¸ªanchor/prior/proposal $b_i$ï¼Œé¢„æµ‹çš„GTï¼š$\mathrm{G}\left(b_{i}\right)=\left\{g_{j} \in \mathcal{G} | \operatorname{IoU}\left(b_{i}, g_{j}\right) \geq \theta\right\}$

é¢„æµ‹ä¸ºsetï¼š$\mathrm{P}\left(b_{i}\right)=\left\{\left(\mathbf{c}_{i}^{(1)}, \mathbf{l}_{i}^{(1)}\right),\left(\mathbf{c}_{i}^{(2)}, \mathbf{l}_{i}^{(2)}\right), \ldots,\left(\mathbf{c}_{i}^{(K)}, \mathbf{l}_{i}^{(K)}\right)\right\}$ï¼Œ$c_i^(j)$è¡¨ç¤ºç¬¬$i$ä¸ªanchoré¢„æµ‹çš„ç¬¬$j$ä¸ªæ¡†çš„ç±»åˆ«å’Œç½®ä¿¡åº¦ï¼Œ$l$ä¸ºä½ç½®

åŒ¹é…æ—¶$K$ä¸ªç‰©ä½“ï¼Œä½†é¢„æµ‹æ—¶ä»å¯èƒ½éƒ¨åˆ†é¢„æµ‹ç»“æœä¸ºèƒŒæ™¯ã€Œ<u>è‡³å¤šé¢„æµ‹$K$ä¸ªç»“æœ</u>ã€   *æ˜¯å¦å¯ä»¥æ‰©å±•ä¸ºé¢„æµ‹æ›´å¤šï¼Ÿ*

è®­ç»ƒçœ‹ä½œæœ€å°åŒ–é¢„æµ‹é›†å’ŒGTé›†ä¹‹é—´çš„<u>æ¨åœŸæœºè·ç¦»</u> $\operatorname{EMD}(P(b_i),G(b_i))$ï¼Œå’Œé›†åˆä¸­ä½ç½®æ— å…³ï¼Œä¸åˆ†å¸ƒæœ‰å…³

$\mathcal{L}\left(b_{i}\right)=\min _{\pi \in \Pi} \sum_{k=1}^{K}\left[\mathcal{L}_{c l s}\left(\mathbf{c}_{i}^{(k)}, g_{\pi_{k}}\right)+\mathcal{L}_{r e g}\left(\mathbf{l}_{i}^{(k)}, g_{\pi_{k}}\right)\right]$

é¢„æµ‹çš„èƒŒæ™¯boxè®¡ç®—$\mathcal{L}_{cls}$ä¸è®¡ç®—$\mathcal{L}_{reg}$

#### æ¨åœŸæœºè·ç¦» (Earth Mover's Distance, Wasserstein)

ä¸¤ä¸ªåˆ†å¸ƒé—´è·ç¦»ï¼šä»ä¸€ä¸ªåˆ†å¸ƒå˜åŒ–åˆ°å¦ä¸€ä¸ªåˆ†å¸ƒæ‰€éœ€è¦çš„æœ€å°åšåŠŸ

![img](Figures/v2-a157b48ccb585193b1074fa09f2e3f83_1440w.jpg)

![Screen Shot 2018-03-03 at 6.11.00 PM.png](Figures/screen-shot-2018-03-03-at-6-11-00-pm.png)

Ref: https://jeremykun.com/2018/03/05/earthmover-distance/, [https://zxth93.github.io/2017/09/27/KLæ•£åº¦JSæ•£åº¦Wassersteinè·ç¦»](https://zxth93.github.io/2017/09/27/KLæ•£åº¦JSæ•£åº¦Wassersteinè·ç¦»/index.html), https://zhuanlan.zhihu.com/p/74075915

#### Set NMS

ä¸€ä¸ªanchoré¢„æµ‹çš„å¤šä¸ªç‰©ä½“æ˜¯uniqueçš„ï¼Œé‡å¤é¢„æµ‹åªå¯èƒ½å‡ºç°åœ¨ä¸åŒanchoré¢„æµ‹é›†ä¹‹é—´

NMSæ—¶å¢åŠ ï¼šå¦‚æœä¸¤ä¸ªpred-boxå‡ºè‡ªåŒä¸€ä¸ªanchorï¼Œåˆ™ä¸è¿›è¡ŒæŠ‘åˆ¶

#### ç½‘ç»œæ¶æ„

![image-20200501151947284](Figures/image-20200501151947284.png)

`FPN`ï¼Œå¢åŠ `RoIAlign`

æ›´å¤šé¢„æµ‹ï¼Œå¯èƒ½å‡ºç°æ›´å¤šFalse positiveï¼Œå¯å¢åŠ `Refinement Module`è¿›è¡ŒäºŒæ¬¡é¢„æµ‹refine

![image-20200501152246868](Figures/image-20200501152246868.png)

Cityperson, CrowdHumanæ•ˆæœå¥½

---

## AugFPN: Improving Multi-scale Feature Learning for Object Detection

> FPNæ”¹è¿›ï¼Œç‰¹å¾èåˆ

![image-20200512164653675](Figures/image-20200512164653675.png)

#### Consistent Supervision

ä¸åŒå°ºåº¦çš„ç‰¹å¾å›¾æœ‰semantic gapï¼Œå¢åŠ ä¸€ä¸ªç›‘ç£ä¿¡å·æ¥é™åˆ¶å­¦ä¹ åˆ°çš„ç‰¹å¾çš„å·®å¼‚

å¢åŠ å¤šä¸ª<u>å…±äº«æƒé‡</u>çš„<u>é¢„æµ‹å¤´</u>(detect head)å¯¹ä¸åŒå°ºåº¦ç‰¹å¾å›¾($M_{1..5}$)ä¸Šçš„proposalè¿›è¡Œ<u>é¢„æµ‹</u>ï¼ŒåŠ ç›‘ç£ä¿¡å·ã€Œ**multi-head prediction**ã€

$\begin{aligned}L_{r c n n}= &\lambda\left(L_{c l s, M}\left(p_{M}, t^{*}\right)+\beta\left[t^{*}>0\right] L_{l o c, M}\left(d_{M}, b^{*}\right)\right)\\ &+L_{c l s, P}\left(p, t^{*}\right)+\beta\left[t^{*}>0\right] L_{l o c, P}\left(d, b^{*}\right)\end{aligned}$
å…¶ä¸­$p_M,\;d_M$è¡¨ç¤ºä¸­é—´å±‚çš„é¢„æµ‹ï¼Œ$p,\;d$è¡¨ç¤ºæœ€ç»ˆå±‚çš„é¢„æµ‹ï¼Œ$t^*,\;b^*$è¡¨ç¤ºGTçš„labelå’Œbox

#### Residual Feature Augmentation

æœ€é«˜å±‚ç‰¹å¾æ²¡æœ‰ä¸Šå±‚ç‰¹å¾ä¸å…¶èåˆã€‚é‡‡ç”¨ä¸åŒå°ºåº¦çš„$C_5$ç‰¹å¾è¿›è¡Œç»„åˆå¾—åˆ°$M_6$ï¼Œå¹¶èåˆåˆ°$M_5$ä¸­ï¼Œæ¥**å¢å¼ºæœ€é«˜å±‚ç‰¹å¾**

![image-20200512165827388](Figures/image-20200512165827388.png)

`Ratio-invariant Adaptive Pooling`ä¸ºæŠŠ$C_5\;@S$ poolingåˆ°ä¸åŒå°ºåº¦$(\alpha_1\times S, \alpha_2\times S,...,\alpha_n\times S)$

å…¶ä¸­`Adaptive Spatial Fusion`ä¸º

![image-20200512165942891](Figures/image-20200512165942891.png)

#### Soft RoI Selection

*two RoIs with similar sizes may be assigned to different levels*

ä½¿ç”¨`ASF`å¯¹å¤šå±‚ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆï¼Œä½œä¸ºRoIçš„ç‰¹å¾

èåˆä¸ºäº†ä½¿anchor-featureçš„åŒ¹é…ä¸åªæ˜¯ä¸€å¯¹ä¸€ï¼Œä¸´è¿‘å°ºåº¦çš„ç‰¹å¾å›¾ä¹Ÿ**å‚ä¸é¢„æµ‹**ï¼Œä¸€ä¸ªæ ·æœ¬**å­¦ä¹ ä¿¡å·**ä¹Ÿä¼ åˆ°å¤šä¸ªå°ºåº¦ç‰¹å¾å›¾

---

## Learning to Separate: Detecting Heavily-Occluded Objects in Urban Scenes (SGE/Serial R-FCN)

> å¯†é›†æ£€æµ‹ï¼Œembeding+NMS (ç±»ä¼¼feature NMS)ï¼Œcascade

å¯†é›†æ£€æµ‹ä¸­ä¸åŒç±»ç‰©ä½“çš„åŒºåˆ†å’ŒåŒä¸€ç±»ä¸åŒç‰©ä½“çš„åŒºåˆ†

#### Semantics-Geometry Embedding & SG-NMS

å¢åŠ å°†æ£€æµ‹æ¡†æ˜ å°„åˆ°éšç©ºé—´ä¸­	$e=\mathbf{s}^T\cdot \mathbf{g}$ï¼Œå…¶ä¸­$\mathbf{g}$å³ä¸ºæ£€æµ‹æ¡†$(x,y,w,h)$ï¼Œ$\mathbf{s}$ä¸ºè¯­ä¹‰åµŒå…¥å‘é‡ã€‚å¯ä»¥çœ‹ä½œ**å°†ä½ç½®ä¿¡æ¯ä»¥è¯­ä¹‰ä¿¡æ¯ä½œä¸ºæƒé‡è¿›è¡Œçº¿æ€§å˜æ¢å¾—åˆ°embed**

boxå’ŒGT<u>åŒ¹é…æ—¶</u>ï¼Œå¯¹æ¯ä¸ªproposal $b_i$ï¼Œé€‰æ‹©æœ€å¤§IoUçš„ç‰©ä½“$b^*_j$ï¼Œå¦‚æœiå’Œjçš„IoUå¤§äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºi proposalåŒ¹é…åˆ°jç‰©ä½“ã€‚**Select max then thresholding**

<u>æŸå¤±å‡½æ•°</u>å¢åŠ  1. Groupï¼šproposalçš„embedå’ŒåŒ¹é…çš„ç‰©ä½“çš„embedè·ç¦»å°½å¯èƒ½å° 2. Sepï¼šproposalçš„embedå’Œä¸å…¶ç¬¬äºŒå¤§IoUçš„ç‰©ä½“çš„embedè·ç¦»å¢å¤§ï¼ˆ**ç¬¬ä¸€å¤§çš„obj: è·ç¦»å‡å°ï¼Œç¬¬äºŒå¤§çš„obj: è·ç¦»å¢å¤§**ï¼‰

<u>NMSæ—¶</u>ï¼ŒIoUå°äºé˜ˆå€¼$N_T$çš„ä¿ç•™(Greedy)ï¼Œå¤§äºé˜ˆå€¼çš„è®¡ç®—**embedçš„è·ç¦»**(SG)ï¼Œembedè·ç¦»å¤§äº$\Phi$çš„ä¿ç•™ï¼Œ$\Phi\propto\operatorname{IoU}$ï¼Œ**IoUå¤§çš„ä¸¤ä¸ªç‰©ä½“éœ€è¦æœ‰æ›´å¤§çš„embedè·ç¦»**

![image-20200608203857935](Figures/image-20200608203857935.png)

ç”±äº<u>FPN</u>ä½œä¸ºbackboneæœ‰å¤šå±‚ï¼Œåœ¨FPNçš„æ¯ä¸€å±‚è¿›è¡ŒGreedy+SG NMSã€‚åœ¨ä¸åŒå±‚ä¹‹é—´åªè¿›è¡ŒGreedy-NMSï¼Œä¸”ä¸€ä¸ªboxåªä¼šè¢«FPNå…¶ä»–å±‚çš„boxæŠ‘åˆ¶

#### Serial R-FCN

![image-20200608205053055](Figures/image-20200608205053055.png)

åˆ†ç±»åˆ†æ”¯å’ŒSGè®¡ç®—åˆ†æ”¯åœ¨å›å½’åˆ†æ”¯ä¹‹åcascadeè¿›è¡Œã€‚**ç›´æ¥ä½¿ç”¨refined-boxè€Œä¸æ˜¯RoI/proposalè¿›è¡Œç‰¹å¾æå–**ï¼Œå¯ä»¥ä½¿ç”¨æ›´é«˜çš„IoUé˜ˆå€¼æ¥è®­ç»ƒåˆ†ç±»åˆ†æ”¯

åˆ†ç±»åˆ†æ”¯è¾¨åˆ«å›å½’åˆ†æ”¯å›å½’çš„refined-boxå±äºç±»åˆ«/BGã€‚éšç€å›å½’åˆ†æ”¯èƒ½åŠ›å¢å¼ºï¼ŒBGç±»åˆ«æ ·æœ¬æ•°é‡å‡å°‘ï¼Œéœ€è¦**hard negative mining**ã€‚åœ¨refined-boxä¸Šå¢åŠ <u>éšæœºå™ªå£°</u>è¾“å…¥åˆ†ç±»åˆ†æ”¯ï¼Œä½œä¸ºæ¥è¿‘ä¸”ä½äºIoUé˜ˆå€¼çš„éš¾è´Ÿæ ·æœ¬

![image-20200608205026817](Figures/image-20200608205026817.png)

æ‰€æœ‰åˆ†æ”¯éƒ½é‡‡ç”¨Position Sensitive RoI-PoolingğŸ‘‡ï¼Œå¹¶å¢åŠ self-attentionğŸ‘†

![img](Figures/v2-247cdb8e1305f64996fbd336813ce80e_1440w.jpg)

proposal/RoIåŒºåŸŸå†…çš„æ¯ä¸ªä½ç½®æœ‰ä¸€ä¸ªç‰¹å¾å›¾ï¼Œåœ¨å¯¹åº”ç‰¹å¾å›¾ä¸ŠRoIåŒºåŸŸå†…poolingå¾—åˆ°ç»“æœå¯¹åº”ä½ç½®çš„å“åº”å€¼

Ref: https://zhuanlan.zhihu.com/p/30867916

---

## VoVNet

ç›¸æ¯”densenetï¼Œåªè¿›è¡Œä¸€æ¬¡ç‰¹å¾èåˆæ“ä½œ

![image-20200620214131006](Figures/image-20200620214131006.png)

è½»é‡çº§ç½‘ç»œå¹³DenseNetæ€§èƒ½ï¼ˆä¸æ˜æ˜¾ $\approx 33@all$ï¼‰

VoVNetV2 å¢åŠ æ®‹å·®è¿æ¥å’ŒSE-block

---

## DRConv: Dynamic Region-Aware Convolution

> åŠ¨æ€é€‰æ‹©å·ç§¯æ ¸ï¼Œä¸æ˜¯receptive fieldã€‚ç±»ä¼¼ç©ºåŸŸå’Œé€šé“ä¸Šçš„attention

**ç©ºé—´çš„åŠ¨æ€å·ç§¯æ ¸**ï¼Œå·ç§¯æ ¸åŒºåŸŸé—´ä¸åŒï¼ŒåŒºåŸŸå†…å…±äº«

ä¼ ç»Ÿå·ç§¯æ ¸ï¼šé€šé“é—´ä¸åŒï¼ŒåŒºåŸŸé—´å®Œå…¨ç›¸åŒï¼ˆå…±äº«å·ç§¯æ ¸ï¼‰$W_c$

å±€éƒ¨å·ç§¯ï¼šä¸åŒä½ç½®**pixel**å·ç§¯æ ¸ä¸åŒ$W_{u,v,c}$

$Y_{u, v, o}=\sum_{c=1}^{C} X_{u, v, c} * W_{u, v, c}^{(o)} \quad(u, v) \in S$

DRConvï¼šä¸åŒåŒºåŸŸå·ç§¯æ ¸ä¸åŒï¼ŒåŒä¸€ä¸ªåŒºåŸŸå†…å…±äº«$W_{t,c}$

$Y_{u, v, g}=\sum_{c=1}^{C} X_{u, v, c} * W_{t, c}^{(o)} \quad(u, v) \in S_{t}$

é¦–å…ˆå­¦ä¹ åˆ’åˆ†åŒºåŸŸmaskï¼Œä¹‹ååœ¨æ¯ä¸ªåŒºåŸŸå†…è¿›è¡ŒåŠ¨æ€å·ç§¯

#### Learnable guided mask

å­¦ä¹ åˆ†åŒºï¼Œå­¦ä¹ å·ç§¯æ ¸åœ¨ç‰¹å¾å›¾ä¸Šçš„åˆ†å¸ƒ

ä½¿ç”¨æ™®é€šå·ç§¯è®¡ç®—ç‰¹å¾å›¾ï¼ˆkernelç©ºåŸŸä¸Šç›¸åŒï¼Œé€šé“ç»´ä¸åŒï¼‰ï¼Œ**åœ¨ç‰¹å¾å›¾é€šé“ç»´é€‰æ‹©æœ€å¤§çš„å¯¹åº”çš„å·ç§¯æ ¸**ä½œä¸ºè¯¥ä½ç½®ä¸Šä½¿ç”¨çš„å·ç§¯æ ¸

![image-20200620223935673](Figures/image-20200620223935673.png)

$M_{u, v}=\operatorname{argmax}\left(F_{u, v}^{0}, F_{u, v}^{1}, \cdots, F_{u, v}^{m-1}\right)$ï¼Œmä¸ªchannel

é€‰æ‹©**é€šé“ç»´**æœ€å¤§çš„kernelä½œä¸ºåŒºåŸŸçš„kernel (åŒæ ·å¤§å°ä¸åŒå‚æ•°)

ç”±äºargmaxæ²¡æœ‰æ¢¯åº¦(mask $M_{u,v}$æ˜¯one-hotå‘é‡)ï¼Œæ‰€ä»¥åå‘ä¼ æ’­æ—¶ä½¿ç”¨softmaxå–ä»£$M_{u,v}$

$\hat{F}_{u, v}^{j}=\frac{e^{F_{u, v}^{j}}}{\sum_{n=0}^{m-1} e^{F_{u, v}^{n}}} \quad j \in[0, m-1]$

#### Dynamic Filter

**æ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€äº§ç”Ÿ**æ¯ä¸ªåŒºåŸŸçš„å·ç§¯æ ¸

ç±»ä¼¼é€šé“+ç©ºåŸŸçš„attentionæœºåˆ¶ï¼ˆæ¯ä¸ªåŒºåŸŸé€‰æ‹©æœ€å¤§é€šé“å¯¹åº”çš„å·ç§¯æ ¸ï¼‰

æ€§èƒ½æå‡1-2ç‚¹ Mask-RCNN

---

# Object Detection Tricks

## Bag of Freebies for Training Object Detection Neural Networks

#### Visually Coherent Image Mixup Training

æŒ‰ç…§betaåˆ†å¸ƒèåˆä¸¤å¼ å›¾ç‰‡è®­ç»ƒï¼Œä½ç½®ä¿¡æ¯ä¸å˜ï¼ˆgeometry preservedï¼‰æ±‚lossæ—¶æŒ‰ç…§èåˆå æ¯”åŠ æƒ

![image-20200701100926363](Figures/image-20200701100926363.png)

Beta distribution

![image-20200701101103962](Figures/image-20200701101103962.png)

æ•ˆæœï¼šè§£å†³ **unprecedented** scenes (å¦‚å±‹ä¸­å¤§è±¡) å’Œ very **crowded** object groupï¼Œä½†å¯èƒ½ä¼šä½¿ç½®ä¿¡åº¦é™ä½

#### Label Smoothing

åˆ†ç±»å¤´ä¸Šä½¿ç”¨ï¼Œå¢åŠ CE-Lossä¸­é”™è¯¯labelçš„æ¢¯åº¦ï¼Œé˜²æ­¢æ¨¡å‹too-confident & over-fitting

$q_{i}=\left\{\begin{array}{ll}1-\varepsilon & \text { if } i=y \\ \varepsilon /(K-1) & \text { otherwise }\end{array}\right.$

#### Data Augmentation

* Random geometry transformation: crop, expansion, flip, resize
* Random color jittering: brightness, hue, saturation, contrast

äºŒé˜¶æ®µæœ‰proposalçš„å‰ªè£ä¸éœ€è¦geometry transformation

#### Training Schedule

é‡‡ç”¨cosineå­¦ä¹ ç‡ï¼Œé˜²æ­¢step schedulerå‰§çƒˆå˜åŒ–ä¸ç¨³å®š

warm-upé˜²æ­¢è®­ç»ƒåˆæœŸæ¢¯åº¦çˆ†ç‚¸

![image-20200701102240277](Figures/image-20200701102240277.png)

#### Sync BN

Batch-size å¯¹æ€§èƒ½å½±å“å¤§

`model = apex.parallel.convert_syncbn_model(model)`

#### Multi-scale

---

**Multi-scale training $\to$ Image level pyramid**

**Multi-level/stage feature $\to$ Feature pyramid**

ç›¸æ¯”image pyramidï¼Œç‰¹å¾é‡‘å­—å¡”åªæå–ä¸€æ¬¡å›¾åƒç‰¹å¾ï¼Œä¸åŒstageè¾“å‡ºï¼ˆå¤šå°ºåº¦ï¼‰ï¼Œé€Ÿåº¦æ›´å¿«

Multi-scale training + Feature pyramid $\to$ all in

## Scale-Equalizing Pyramid Convolution for Object Detection (SEPC)

>é‡‡ç”¨3Då·ç§¯è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾èåˆï¼Œä¸åŒå±‚èåˆæƒé‡ä¸åŒ

ç‰¹å¾é‡‘å­—å¡”ä¸åŒå±‚ä¹‹é—´å­˜åœ¨semantic gapï¼Œä¹‹å‰é‡‡ç”¨feature fusionè§£å†³ä½†æ²¡æœ‰æå–intrinsic property

**Pyramid Conv** `PConv`: **åœ¨å°ºåº¦ç»´ä¸Š (ä¸åŒå±‚ç‰¹å¾å›¾)** åšå·ç§¯ï¼ˆlike spatial convï¼‰ï¼Œ3D conv

é€‰æ‹©ä¸‰å±‚ï¼Œåº•å±‚é‡‡ç”¨strideå·ç§¯ï¼Œä¸­é—´æ™®é€šå·ç§¯ï¼Œä¸Šå±‚bilinear upsamplingã€‚æ§åˆ¶ä¸åŒå±‚çš„ä¸åŒstrideæ¥è·å¾—ç›¸åŒå¤§å°çš„è¾“å‡ºï¼Œå¹¶ç›¸åŠ ã€‚æ—¶é—´æˆæœ¬ä¸º1.5å€

![image-20200704214032431](Figures/image-20200704214032431.png)

BNä¿®æ”¹ä¸ºæ•´ä¸ªç‰¹å¾é‡‘å­—å¡”å±‚å…±äº«ç»Ÿè®¡é‡ï¼Œæ•ˆæœç±»ä¼¼sync_bnï¼ŒåŒæ­¥ä¸åŒä½ç½®ä¸Šçš„ç»Ÿè®¡é‡ï¼Œç»Ÿè®¡èŒƒå›´æ›´å¹¿

BN = standardization($\mu$,$\sigma$) + scale_shift($\gamma$,$\beta$)ï¼Œç»Ÿè®¡å·ç§¯çš„bnä¸ºæ¯å¼ ç‰¹å¾å›¾ï¼Œå³channelç»´çš„ç»Ÿè®¡ï¼Œç»Ÿè®¡ä¸åŒæ ·æœ¬åœ¨åŒä¸€ä¸ªchannelåŒä¸€ä¸ªä½ç½®çš„batch statistics $\mu$å’Œ$\sigma$ï¼ˆreduce at batch dimensionï¼‰ã€‚æ¯å¼ ç‰¹å¾å›¾ä¸€å¯¹parameters $\gamma$å’Œ$\beta$

å¯ä»¥è¯æ˜PConvèƒ½å¤Ÿæå–Gaussian Pyramid(ä¸åŒé«˜æ–¯æ¨¡ç³Šæ ¸å¯¹å›¾ç‰‡å¤„ç†) å°ºåº¦ä¸å˜çš„ç‰¹å¾ï¼Œå³å¦‚æœåŸå›¾ç‰©ä½“å°ºåº¦å˜åŒ–ï¼Œåˆ™å¯é€šè¿‡å¯¹PConvæå–çš„ç‰¹å¾shiftå¾—åˆ°æ”¹å˜åç‰©ä½“çš„ç‰¹å¾ *<u>è®ºæ–‡ä¸­æœ‰è¯æ˜</u>*

**Scale-Equalizing Pyramid Conv** `SEPC`: PConvçš„å‡çº§ç‰ˆï¼Œé€šè¿‡deformable convå®ç°å¯¹ä¸åŒå°ºåº¦ç‰¹å¾çš„å¯¹é½ã€‚

```python
def pconv_module_forward(x, conv2D_list):
	# x: input feature list [p3,p4,p5,p6,p7] # conv2D_list: conv2D module list,
	# [nn.Conv2D(stride=2),nn.Conv2D(),nn.Conv2D()]
	out_x = []
	for level in range(len(x)):
		tmp = conv2D_list[1](x[level])
		if level > 0:
			tmp += conv2D_list[0](x[level-1])
		if level < len(x) - 1:
			tmp += Upsample(conv2D_list[2](x[level+1])
		out_x.append(tmp) 
	return out_x
```

æœ€åº•å±‚æ™®é€šå·ç§¯kernel sizeä¸å˜ï¼Œå…¶ä»–å±‚ä½¿ç”¨deformable conv

![image-20200704213956083](Figures/image-20200704213956083.png)

å› ä¸ºfeature pyramidä¸­ç”±äºbackboneéçº¿æ€§æ“ä½œï¼Œç›¸å¯¹äºgaussian pyramidç‰¹å¾ä¸å¯¹é½ï¼Œæ‰€ä»¥åªé€šè¿‡dilationæå–ç‰¹å¾ä¸å…¨é¢ï¼Œæ”¹ç”¨d-conv

*SEPC is an improved version of pconv, to relax the discrepancy of feature pyramid from a Gaussian pyramid by aligning the feature map of higher layers with the lowest layer*

åº•å±‚ç‰¹å¾é‡‡ç”¨æ™®é€šå·ç§¯ï¼Œå·ç§¯ç»“æœä½œä¸ºæƒé‡å‘ä¸Šå±‚çš„d-convå…±äº«ï¼ˆå·ç§¯ç»“æœä½œä¸ºd-convçš„ä¸€ä¸ªè¾“å…¥æ±‚offsetï¼Œç±»ä¼¼CentripetalNetä¸­ï¼‰

SEPCæ¨¡å—ç”¨äºå–ä»£RetinaNetçš„detection headçš„å·ç§¯æ¨¡å—

æœ¬è´¨ä¸ºä¸€ç§pix-wise or fine-grained çš„feature fusionï¼Œå¤šå°ºåº¦ç‰¹å¾å›¾é€šè¿‡å·ç§¯æ“ä½œè¿›è¡Œç‰¹å¾èåˆ

![image-20200704214335943](Figures/image-20200704214335943.png)

æ€§èƒ½æå‡æ˜æ˜¾

![image-20200704214411536](Figures/image-20200704214411536.png)

---

## Rethinking Classification and Localization for Object Detection (Double-Head RCNN)

> æ£€æµ‹å¤´è¿›è¡ŒåŒè·¯åˆ†æ”¯æ”¹è¿›

äºŒé˜¶æ®µæ£€æµ‹å™¨ä¸­ï¼Œå¯¹proposalå¤„ç†çš„ä¼ ç»Ÿæ£€æµ‹å¤´é‡‡ç”¨convå±‚ï¼Œæˆ–è€…fast rcnnä½¿ç”¨fcå±‚å¤„ç†

![image-20200706183720979](Figures/image-20200706183720979.png)

fcå±‚ä½œä¸ºheadï¼Œç”±äºå…¨è¿æ¥æ“ä½œä¸åŒä½ç½®æƒé‡ä¸å…±äº«ï¼Œå¯¹äºä½ç½®æ›´åŠ æ•æ„Ÿï¼ˆå°ä½ç½®å˜åŒ–å¤§çš„è¾“å‡ºç»“æœå˜åŒ–ï¼‰ï¼Œé€‚åˆåˆ†ç±»

convå±‚ä½œä¸ºheadï¼Œç”±äºå·ç§¯æ“ä½œä¸åŒä½ç½®æƒé‡å…±äº«ï¼Œé€‚åˆå›å½’æ•´ä¸ªç‰©ä½“ï¼ˆcoco2018ç»“æœè¡¨æ˜å¯ä»¥ä½¿ç”¨convå±‚ä½œä¸ºbboxçš„é¢„æµ‹å¤´ï¼‰

![image-20200706185341027](Figures/image-20200706185341027.png)

åˆ†æconvä½œä¸ºheadçš„åˆ†ç±»å›å½’ç»“æœğŸ‘‡

![image-20200706184401851](Figures/image-20200706184401851.png)

fcä½œä¸ºheadçš„åˆ†ç±»å›å½’ç»“æœğŸ‘†

1. åˆ†ç±»ï¼šfcä½œä¸ºheadæ—¶ï¼Œå…¶é«˜iouéƒ¨åˆ†åˆ†ç±»ç»“æœæ˜æ˜¾æ¯”convä½œä¸ºheadå¥½ã€‚è®¡ç®—Pearson correlation coefficientğŸ‘‡fc-headçš„score-iouå…³ç³»å¤§äºconv-headçš„score-iouå…³ç³»

![image-20200706184732698](Figures/image-20200706184732698.png)

2. å®šä½ï¼šconv-headç•¥å¥½äºfc-head

å¯è§†åŒ–å…³è”å…³ç³»ï¼šconv-headé‡‡ç”¨å¾—åˆ°çš„ç‰¹å¾å›¾cosè·ç¦»ï¼Œ7x7gridï¼Œæ¯ä¸ªç‚¹äº§ç”Ÿ7x7å¤§å°çš„å’Œå…¶ä»–ä»»æ„ç‚¹çš„å…³è”çŸ©é˜µï¼›fc-headå¯¹fcå±‚æƒé‡å˜æ¢è¾“å‡º7x7ç‰¹å¾å›¾

![image-20200706185049024](Figures/image-20200706185049024.png)

conv-headè®¡ç®—çš„ç‚¹å’Œå‘¨å›´å…³è”å…³ç³»å¤§

#### Double head RCNN

ä¸¤ä¸ªåˆ†æ”¯ï¼Œconv-headç”¨äºå›å½’ï¼Œfc-headç”¨äºåˆ†ç±»

![image-20200706185402627](Figures/image-20200706185402627.png)

æ”¹è¿›ï¼šextendï¼Œè®­ç»ƒæ—¶åˆ©ç”¨conv-headçš„åˆ†ç±»æŸå¤±å’Œfc-headçš„å›å½’æŸå¤±è¿›è¡Œç›‘ç£(unfocused task supervision)

$\mathcal{L}^{f c}=\lambda^{f c} L_{c l s}^{f c}+\left(1-\lambda^{f c}\right) L_{r e g}^{f c}$, $\mathcal{L}^{c o n v}=\left(1-\lambda^{c o n v}\right) L_{c l s}^{c o n v}+\lambda^{c o n v} L_{r e g}^{c o n v}$

åŒæ—¶å¯¹åˆ†ç±»åˆ†æ•°è¿›è¡ŒäºŒåˆ†æ”¯èåˆ(classifiers are complimentary)

![image-20200706185811416](Figures/image-20200706185811416.png)

FPN+2.5~3.8 val AP

---

## D2Det: Towards High Quality Object Detection and Instance Segmentation

> ä¸€é˜¶æ®µå›å½’åˆ†æ”¯æ“ä½œç§»æ¤åˆ°äºŒé˜¶æ®µ
>
> deformable pooling + weighted pooling

![image-20200706234538094](Figures/image-20200706234538094.png)

#### Dense local regression

æŠŠRoIçœ‹ä½œç±»ä¼¼ä¸€é˜¶æ®µä¸­çš„ç‰¹å¾å›¾ã€‚ä¹‹å‰äºŒé˜¶æ®µæ£€æµ‹å™¨å¯¹äºä¸€ä¸ªRoIåªé¢„æµ‹ä¸€ä¸ªæ¡† (FC+é¢„æµ‹)ï¼Œæ”¹æˆRoIä¸­æ¯ä¸ªç‚¹éƒ½é¢„æµ‹ä¸€ä¸ªæ¡†(dense prediction)ã€‚é¢„æµ‹ä¸­é—´ç‚¹åˆ°boxä¸Šä¸‹å·¦å³çš„è·ç¦» (FCOS)

å¢åŠ binary overlap prediction ($\hat{m}$)ï¼Œå¯¹RoIå’ŒGTé‡å éƒ¨åˆ†ä¸º1ï¼Œåªæœ‰ä¸º1çš„ç‚¹çš„é¢„æµ‹ç»“æœæœ‰æ•ˆã€‚

<u>ç±»ä¼¼</u>ä¸€é˜¶æ®µä¸­æ¯ç‚¹é¢„æµ‹bboxï¼Œé¢„æµ‹IoU/objectness

#### Discriminative RoI pooling

<u>deformable pooling + weighted pooling</u>

poolingçš„weightedæƒé‡é€šè¿‡è¾“å…¥ç‰¹å¾æ“ä½œå¾—åˆ°ï¼ˆç±»ä¼¼self-attentionï¼‰ğŸ‘‡Wä¸ºè®¡ç®—çš„æƒé‡

$\tilde{F}=W(F) \odot F$

![image-20200707000247234](Figures/image-20200707000247234.png)æ€§èƒ½æå‡è¾ƒæ˜æ˜¾: multi-scale 50.1

---

## NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing (PBM)

>åŸºäºbi-box regre. æ ¹æ®visæ¡†è¿›è¡ŒNMS

è¡Œäººæ£€æµ‹ä¸­é‡å ï¼Œç±»å†…é‡å Replusion Losså’ŒAggLossæƒ©ç½šä¸¤ä¸ªäººä¸­é—´çš„boxï¼Œä½†æ˜¯boxçš„é‡å ä»ç„¶ä¼šå¯¼è‡´åœ¨NMSè¿‡ç¨‹ä¸­è¢«è¯¯åˆ 

Adaptive NMSä¸­GT-densityå’ŒPred-densityçš„inconsistency

![image-20200707105643820](Figures/image-20200707105643820.png)

GTæ¡†ä¸å¯†é›†ï¼Œä½†Pred-boxæ˜¯å¯†é›†çš„ã€‚Adaptive NMSé¢„æµ‹GTçš„å¯†é›†ç¨‹åº¦ï¼Œè€ŒNMSå’ŒPredçš„å¯†é›†ç¨‹åº¦æœ‰å…³ã€‚ğŸ‘†é¢„æµ‹ç»¿æ¡†ä¸å¯†é›†ï¼Œæ‰€ä»¥çº¢æ¡†ä¸ä¼šè¢«ä¿ç•™

#### NMS by representative region

é€šè¿‡è®¡ç®—visibleéƒ¨åˆ†çš„IoUè€Œä¸æ˜¯fulléƒ¨åˆ†çš„IoUè¿›è¡ŒNMS

é‡å ä¸åŒç‰©ä½“çš„æ£€æµ‹æ¡†full-IoUå¤§ï¼Œvis-IoUå°ã€‚é‡å åŒä¸€ç‰©ä½“full-IoUå’Œvis-IoUéƒ½å¤§ $\to$ vis-IoUåˆ¤åˆ«

![image-20200707132234249](Figures/image-20200707132234249.png)

#### Paired BBox Faster RCNN

Paired RPN + Paired Proposal Feature Extractor +  Pair RCNN

##### Paired RPN

GTæ ‡æ³¨ä¸ºpair Q=(F,V)ï¼Œå…¨èº«æ¡†å’Œå¯è§æ¡†

**ä»ä¸€ä¸ªanchor**å›å½’å‡ºfullå’Œvisibleçš„proposalï¼ˆä¸ºäº†inherent correspondenceï¼‰

anchorå’Œä¸€å¯¹GTæ ‡æ³¨çš„åŒ¹é…æ ‡å‡†ï¼š

$\operatorname{IoU}(\mathrm{A}, \mathrm{F}) \geq \alpha_{1}$ and $\operatorname{IoF}(\mathrm{A}, \mathrm{V}) \geq \beta_{1}$

$\operatorname{IoU}(\mathrm{A}, \mathrm{F})=\frac{\operatorname{Area}(\mathrm{A} \cap \mathrm{F})}{\operatorname{Area}(\mathrm{A} \cup \mathrm{F})},\;\;\operatorname{IoF}(\mathrm{A}, \mathrm{V})=\frac{\operatorname{Area}(\mathrm{A} \cap \mathrm{V})}{\operatorname{Area}(\mathrm{V})}$

fullå’Œvisçš„å›å½’è®­ç»ƒç­–ç•¥åŒbi-box regression

##### Paired Proposal Feature Extractor

èåˆfull/visä¸¤ä¸ªproposalçš„ç‰¹å¾

![image-20200707133322847](Figures/image-20200707133322847.png)

<font color="#00dd00">ç»¿è‰²åˆ†æ”¯</font>+<font color="dd0000">çº¢è‰²åˆ†æ”¯</font>ï¼šç®€å•çš„concatä¸¤ä¸ªproposalçš„ç‰¹å¾å‘é‡

<font color="00dd00">ç»¿è‰²åˆ†æ”¯</font>+é»‘è‰²åˆ†æ”¯ï¼švisible region attention/maskï¼Œæ„å»ºvisibleéƒ¨åˆ†çš„0/1 maskï¼Œä¸fullç‰¹å¾$F_f$ç‚¹ä¹˜å¾—$F_m$ï¼Œå†concat $F_m$å’Œ$F_v$

##### Pair RCNN

èåˆç‰¹å¾å’Œproposalä½œä¸ºè¾“å…¥ï¼Œä¸¤ä¸ªåˆ†æ”¯åˆ†åˆ«é¢„æµ‹fullå’Œvisçš„æ£€æµ‹æ¡†

proposalå’Œä¸€å¯¹GTçš„åŒ¹é…æ ‡å‡†ï¼š

$\operatorname{IoU}\left(\mathrm{P}_{\mathrm{f}}, \mathrm{F}\right) \geq \alpha_{2}$ and $\operatorname{IoU}\left(\mathrm{P}_{\mathrm{v}}, \mathrm{V}\right) \geq \beta_{2}$

æ€§èƒ½æå‡æ˜æ˜¾

![image-20200707134837180](Figures/image-20200707134837180.png)

---

## Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection

>NASæ•´ä½“æœç´¢ï¼ŒåŸºäºFBNet

![image-20200708170606633](Figures/image-20200708170606633.png)

äºŒé˜¶æ®µæ£€æµ‹å™¨ï¼šbackbone + neck(feature pyramid & fusing) + RPN(fixed) + Head

NATSå’ŒDetNASæœç´¢backboneï¼ŒNAS-FPNæœFPNï¼ŒAuto-FPNæœfusingå’ŒHead

å•ç‹¬æœç´¢æ•ˆæœè¾ƒå·®ï¼Œé‡‡ç”¨æ•´ä½“æ¯ä¸ªéƒ¨ä»¶(backbone: $\alpha$, neck: $\beta$, head: $\gamma$)ä¸€èµ·æœç´¢ï¼Œ**end-to-end search**

$\alpha^{*}, \beta^{*}, \gamma^{*}=\underset{\alpha, \beta, \gamma}{\arg \min } \mathcal{L}_{v a l}^{d e t}\left(\alpha, \beta, \gamma, w^{*}(\alpha, \beta, \gamma)\right)=\underset{\alpha, \beta, \gamma}{\arg \min } \mathcal{L}_{v a l}^{d e t}\left(\alpha, \beta, \gamma, \underset{w}{\arg \min } \mathcal{L}_{t r a i n}^{d e t}(\alpha, \beta, \gamma, w)\right)$

æ¯ä¸ªcomponenté‡‡ç”¨é€å±‚æœç´¢ï¼Œæ„å»ºæ¯ä¸ªæ“ä½œç”¨åœ¨æ¯ä¸€å±‚çš„å¾—åˆ†çŸ©é˜µ$\alpha$ï¼Œé€‰æ‹©æ¦‚ç‡

å¯¹ä¸€ä¸ªæ“ä½œç”¨åœ¨ä¸åŒå±‚çš„å¾—åˆ†è¿›è¡ŒL2**æ­£åˆ™åŒ–(column-space regularization)**ï¼Œå‡å°‘å¾—åˆ†çš„æ•°å€¼å¤§å°ï¼š

$\min _{\alpha} f(\alpha)+\mu \min _{i}(\sqrt{\sum_{l=1}^{L} \alpha_{l, i}^{2}})$

é€‰æ‹©$\alpha_{l,i}$æœ€å¤§maxçš„ä¸€ä¸ªæ“ä½œï¼Œä½†ä¸ºäº†å¯å¾®é‡‡ç”¨softmaxæ¥é€‰æ‹©(continuous relaxation)

æœç´¢backboneæ—¶æœç´¢ç©ºé—´çš„èŠ‚ç‚¹è®¡ç®—(differentiable)ï¼š$x_{l}=\sum_{o \in \mathcal{O}_{b}} \frac{\exp \left(\alpha_{l}^{o}\right)}{\sum_{o^{\prime} \in \mathcal{O}_{b}} \exp \left(\alpha_{l}^{o^{\prime}}\right)} o\left(x_{l-1}\right)$

æœç´¢neckæ—¶æœlateral connectionï¼Œæœç´¢Headæ—¶æœfcé¢„æµ‹å‰çš„block

ä¼˜åŒ–æ—¶å¢åŠ FLOPSçº¦æŸï¼š$\mathrm{C}(\alpha)=\sum_{l} \sum_{o \in \mathcal{O}_{b}} \alpha_{l}^{o} \mathrm{FLOPs}(o, l)$

é¦–å…ˆå›ºå®š$\{\alpha,\beta,\gamma\}$åœ¨ä¸€åŠçš„æ•°æ®ä¸Šè®­ç»ƒæ±‚$\partial \mathcal{L}/\partial \mathcal{w}$æ›´æ–°$w$ï¼Œå†å›ºå®š$w$åœ¨å‰©ä¸‹çš„æ•°æ®ä¸Šè®­ç»ƒæ±‚$\partial\mathcal{L}/\partial\alpha,\partial\mathcal{L}/\partial\beta,\partial\mathcal{L}/\partial\gamma$æ›´æ–°æ¶æ„$\alpha,\beta,\gamma$

---

## SaccadeNet: A Fast and Accurate Object Detector

![image-20200708225553326](Figures/image-20200708225553326.png)

<u>Center Attentive Module</u>äº§ç”Ÿä¸­å¿ƒç‚¹å’Œç±»åˆ«ï¼›<u>Attention Transitive Module</u>äº§ç”Ÿwhï¼›<u>Aggregation Attentive Module</u>èåˆä¸¤åˆ†æ”¯çš„ç‰¹å¾ï¼ˆåŒçº¿æ€§æ’å€¼ï¼‰å’Œé¢„æµ‹ï¼Œå¹¶å¯¹é¢„æµ‹æ¡†è¿›è¡Œrefineï¼›<u>Corner Attentive Module</u>åªæœ‰ä¸€ä¸ªblockäº§ç”Ÿè§’ç‚¹é¢„æµ‹ï¼Œç”¨äºbackboneç›‘ç£ä¿¡å·ï¼Œæµ‹è¯•æ—¶å»æ‰ã€‚

ç›¸æ¯”CenterNetæ”¹è¿›ï¼š1. é€šè¿‡Aggregationè¿›è¡ŒäºŒæ­¥refine 2. å¢åŠ corneråˆ†æ”¯çš„ç›‘ç£ä¿¡å·æœ‰åŠ©äºå¯¹è¾¹ç¼˜çš„æ£€æµ‹ï¼ˆå¸®åŠ©é¢„æµ‹whçš„åˆ†æ”¯ï¼‰ 3. L1ä½œä¸ºboxè®­ç»ƒæŸå¤±

![image-20200708230331922](Figures/image-20200708230331922.png)

ğŸ‘†<u>test-dev</u>ä¸Šæ€§èƒ½ï¼Œé€Ÿåº¦æå‡æ˜æ˜¾ï¼Œå¿«äºyolov3

---

