# Deep Learning Related Topics

## ResNeXt

> ä¼ ç»Ÿä¸ºå¢åŠ æ·±åº¦ï¼ˆå±‚æ•°ï¼‰å’Œå®½åº¦ï¼ˆç‰¹å¾ç»´åº¦ï¼‰ï¼Œå˜æˆç»“åˆVGGçš„å †å ç½‘ç»œ+Inceptionçš„ _split-transform-merge_ ç­–ç•¥ã€‚å¢åŠ å‡†ç¡®ç‡å’Œå¯æ‰©å±•æ€§åŒæ—¶ä¸æ”¹å˜æˆ–å‡å°å¤æ‚åº¦  

æå‡º<u>**cardinality**</u>ï¼Œè¡¨ç¤ºå¤šåˆ†æ”¯çš„åˆ†æ”¯æ•°é‡ï¼Œmore effective
![](Figures/EC005B55-327B-4F4C-A5FE-9232BA9D04E8.png)
ğŸ‘†å·¦ä¸ºResNetï¼Œå³ä¸ºResNeXt cardinality=32
<u>**å¹³è¡Œå †å **</u>ä»£æ›¿çº¿æ€§åŠ æ·±ï¼Œä¸æ˜¾è‘—å¢åŠ å‚æ•°æ•°é‡çº§å¢åŠ å‡†ç¡®ç‡ï¼›åŒæ—¶å¹³è¡Œåˆ†æ”¯çš„æ‹“æ‰‘ç»“æ„ç›¸åŒå‡å°‘äº†è¶…å‚æ•°ï¼ˆè®¾è®¡æˆæœ¬ï¼‰
![](Figures/C1CCACF2-6F03-4B74-99C6-E8397306F05E.png)
ğŸ‘†split-transform-mergeè¿‡ç¨‹
![](Figures/E210EA79-5D84-4C33-B0BA-146AFABB08A2.png)
ğŸ‘†ä¸‰ç§ç­‰ä»·çš„ç»“æ„ï¼Œç¬¬ä¸‰ç§ç®€æ´ï¼Œé€Ÿåº¦æ›´å¿«

- - - -

## å·ç§¯å¤§å°

$O=\lfloor\frac{W-F+2P}{S}\rfloor+1$

**Dilated Conv**

$O=\frac{W+2P-(d(k-1)+1)}{S}+1$

## MobileNet

æŠŠä¼ ç»Ÿå·ç§¯å˜æˆdepth-wiseå·ç§¯å’Œ1x1å·ç§¯
æ ‡å‡†å·ç§¯ï¼ˆDkå·ç§¯æ ¸å¤§å°ï¼ŒDfè¾“å‡ºç‰¹å¾å›¾å°ºå¯¸ï¼ŒMè¾“å…¥é€šé“ï¼ŒNè¾“å‡ºé€šé“ï¼‰
$D_K\times D_K\times M\times N\times D_F\times D_F$
**æ·±åº¦å¯åˆ†ç¦»å·ç§¯**ï¼šdepth-wiseå·ç§¯å’Œ`1x1`å·ç§¯

$D_K\times D_K\times M\times D_F\times D_F + M\times N\times D_F\times D_F$

Depthå·ç§¯æŠŠè¾“å…¥ç‰¹å¾å›¾Dk x Dk x MæˆDf x Df x Må±‚ï¼Œç„¶å`1x1`æŠŠMå±‚å˜ä¸ºNå±‚è¾“å‡º
![](Figures/6B22C4FC-30D7-45C9-9CD8-DD3469070EDC.png)
ğŸ‘†<u>**depth-wise conv**</u>ä¸åŒç‰¹å¾å›¾é€šé“ä½¿ç”¨ä¸åŒå·ç§¯æ ¸ï¼ŒMå±‚ç‰¹å¾å›¾å·ç§¯è¿˜æ˜¯Må±‚ï¼ˆä¸ç›¸åŠ ï¼‰æ‰€ä»¥`Dk x Dk x M x Df x Df`ï¼ˆä¸€ä¸ªå·ç§¯æ ¸è®¡ç®—`Dk x Dk`æ¬¡ï¼Œæ„æˆè¾“å‡ºç‰¹å¾å›¾çš„ä¸€ä¸ªç‚¹ï¼Œæ‰€ä»¥ä¸€å¼ è¾“å‡ºç‰¹å¾å›¾ä¸€å…±è®¡ç®—`Dk x Dk x Df x Df`æ¬¡ï¼Œä¸€å…±Må±‚ï¼‰
![](Figures/770C8B8D-2E90-4D9E-99D4-E1BE24CAE3A4.png)
ğŸ‘†depth-wiseè§£é‡Š
![](Figures/C8B12662-17AF-4AE3-8753-17E1F0A5FAC0.png)
ğŸ‘†<u>**point-wise conv**</u>ä¸åŒç‰¹å¾å›¾ä½¿ç”¨åŒä¸€ä¸ªå·ç§¯æ ¸ï¼Œä½†æ˜¯1x1ã€‚æŠŠMé€šé“å·ç§¯æˆNé€šé“ï¼Œæ‰€ä»¥`M x N x Df x Df` ï¼ˆä¸€ä¸ªå·ç§¯æ ¸è®¡ç®—Må±‚ç›¸åŠ ï¼Œå†è®¡ç®—Næ¬¡æ„æˆNé€šé“ï¼‰
![](Figures/B1AE5F5A-4B07-4A4E-A1F3-932F477265B7.png)
ğŸ‘†point-wiseè§£é‡Š

---

## Convolution

![](Figures/B712984B-A621-4D75-B9C1-7DB0D7D0226F.png)
![](Figures/4ECAAE30-D4AB-45FC-A130-F8DE831CC235.png)
å‚æ•°é‡ `H x W x C1 x C2`

## Group Convolution

![](Figures/FB2404B4-04DF-4E98-923F-0E5DDB739C0A.png)
å·ç§¯æ ¸ä¹‹é—´çš„å…³ç³»æ˜¯**ç¨€ç–**çš„ã€‚group convå‡å°‘å·ç§¯æ ¸ä¹‹é—´çš„å…³è”æ€§ï¼Œ **regularization**ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
Ref: [A Tutorial on Filter Groups (Grouped Convolution) - A Shallow Blog about Deep Learning](https://blog.yani.io/filter-group-tutorial/)

---

## MobileNet V2

å¢åŠ <u>**inverted residual with linear bottleneck**</u>ï¼Œé¦–å…ˆå‡ç»´ï¼Œå·ç§¯ï¼Œå†é™ç»´ã€‚ <u>ç‰¹å¾æå–åœ¨é«˜ç»´ç©ºé—´è¿›è¡Œ</u> ã€‚çººé”¤å½¢ï¼Œå’Œresnetçš„hour-glassç›¸åï¼Œæ‰€ä»¥inverted
åœ¨DWä¹‹å‰ <u>**å¢åŠ PWå·ç§¯**</u>ï¼šä¸Šä¸€å±‚é€šé“æ•°å°‘ï¼Œåˆ™DWåªèƒ½åœ¨ä½ç»´ç©ºé—´æå–ç‰¹å¾ï¼Œå¢åŠ PWåï¼Œå…ˆå‡ç»´ï¼Œå†æDWç‰¹å¾
![](Figures/20A1CAA2-880C-4C27-9475-C66CC955FDA5.png)
å»æ‰äº†ç¬¬äºŒä¸ªPWçš„ <u>**æ¿€æ´»å‡½æ•°**</u> ï¼šåªæœ‰åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ¿€æ´»å‡½æ•°å¯ä»¥å¢åŠ éçº¿æ€§ï¼›è€Œåœ¨ä½ç»´ç©ºé—´ä¸­ï¼Œæ¿€æ´»å‡½æ•°ä¼šç ´åç‰¹å¾ã€‚å› æ­¤é‡‡ç”¨çº¿æ€§
å¢åŠ  **shortcut** è¿æ¥ï¼Œè¾“å‡ºä¸è¾“å…¥ç›¸åŠ ï¼šåŒResNet

- - - -

## ShuffleNet

> ç»„å†…point-wiseå·ç§¯ï¼Œå¢åŠ shuffleæ“ä½œé€šé“ä¹‹é—´ä¿¡æ¯æ²Ÿé€š  

![](Figures/B747E511-B1B6-4CD8-9F8F-446708466A2B.png)
ğŸ‘†aï¼šnormalï¼Œbï¼šåˆ†ç»„å·ç§¯ï¼Œcï¼šchannel shuffle

*å¦ä¸€ç§ç†è§£ğŸ‘‡*

![image-20200322164220501](Figures/image-20200322164220501.png)

#### Channel shuffle

ğŸ‘‡<u>**å±•å¼€ï¼Œè½¬ç½®ï¼Œå¹³é“º**</u>
![](Figures/6C36658F-B3E1-4407-9517-D585F622C5BE.png)
![](Figures/3C97825C-0822-455C-AB65-9D0E27EB706C.png)
ğŸ‘†å¯¹æ¯”mobilenetï¼Œshufflenetå’Œshuffleneté™é‡‡æ ·
ğŸ‘†gé‡‡ç”¨åˆ†ç»„å·ç§¯ï¼Œgå°ï¼›å»æ‰ReLUï¼Œå‡å°‘ä¿¡æ¯æŸè€—ï¼›é™é‡‡æ ·ä¿è¯å‚æ•°é‡ä¸éª¤å‡ï¼Œéœ€è¦å¢åŠ é€šé“æ•°é‡ï¼Œé‡‡ç”¨concatè€Œä¸æ˜¯element-wise add
**æ€§èƒ½è¯„ä»·**ï¼šMACè®¿å­˜ï¼ŒGPUå¹¶è¡Œæ€§
**è®¾è®¡å‡†åˆ™**

* å½“è¾“å…¥é€šé“æ•°å’Œè¾“å‡ºé€šé“æ•°ç›¸åŒæ—¶ï¼ŒMACæœ€å°
* MACä¸åˆ†ç»„æ•°é‡gæˆæ­£æ¯”ï¼ˆè°¨æ…ä½¿ç”¨åˆ†ç»„å·ç§¯ï¼‰
* ç½‘ç»œçš„åˆ†æ”¯æ•°é‡é™ä½å¹¶è¡Œèƒ½åŠ›ï¼ˆå·ç§¯æ ¸åŠ è½½å’ŒåŒæ­¥ï¼‰ï¼Œå•åˆ†æ”¯é€Ÿåº¦å¿«--ç½‘ç»œç»“æ„ç®€å•
  * ResNeXtå‡†ç¡®ç‡æå‡ä½†æ˜¯é€Ÿåº¦æ…¢
* Element-wiseæ“ä½œæ˜¯éå¸¸è€—æ—¶

![](Figures/EA15DE00-DC00-4A95-B656-20AA985E0269.png)
ğŸ‘†shufflenet v2å’Œdownsample
å¢åŠ **é€šé“åˆ†å‰²**ï¼Œé€šé“åˆ†ä¸ºc1å’Œc2è¾“å…¥åˆ°ä¸¤ä¸ªåˆ†æ”¯ä¸­ï¼Œä½¿ç”¨concatæ›¿ä»£element-wise add
å †å blockçš„æ—¶å€™ï¼Œå¯ä»¥å°†concat, channel-shuffle, channel-splitåˆå¹¶ä¸ºä¸€ä¸ªelement-wiseæ“ä½œ
æ€æƒ³â€”<u>**ç‰¹å¾é‡ç”¨**</u>ï¼Œä¸Šå±‚çš„feature mapç›´æ¥ä¼ å…¥ä¹‹åçš„æ¨¡å—ï¼Œç›´æ¥æ˜ å°„ï¼ˆshufflenet v2å·¦ä¾§åˆ†æ”¯ï¼‰

- - - -

## SqueezeNet

> <u>**åˆ†å—è®¾è®¡**</u>æ€æƒ³ï¼Œæ¨¡å‹<u>**å‹ç¼©**</u>

1. ä½¿ç”¨`1x1`å·ç§¯ä»£æ›¿`3x3`å·ç§¯  
2. å‡å°‘`3x3`å·ç§¯è¾“å…¥é€šé“æ•°  
3. å»¶è¿Ÿä¸‹é‡‡æ ·ï¼Œå‰é¢layerè·å¾—æ›´å¤§ç‰¹å¾å›¾æå‡æ€§èƒ½  

**Fire Module**

ä¸¤å±‚å·ç§¯æ“ä½œï¼šsqueeze `1x1`, expand `1x1 + 3x3`
![](Figures/v2-5fde12f060519e493cb059484514f88a_hd.jpg)
ğŸ‘†ğŸ‘‡ <u>squeezeæ˜¯å•åˆ†æ”¯ï¼Œexpandæ˜¯äºŒåˆ†æ”¯</u>  
![](Figures/1DCBEDA4-1CA7-4029-8E78-BF80ABEF9898.png)
éƒ¨åˆ†3x3å˜æˆ1x1ï¼Œå‚æ•°æ•°é‡å‡å°‘ï¼Œä½†ä¸ºè·å¾—æ€§èƒ½éœ€è¦åŠ æ·±ç½‘ç»œæ·±åº¦ï¼ŒåŒæ—¶å¹¶è¡Œèƒ½åŠ›ä¸‹é™ï¼Œä¹Ÿå¯¼è‡´æµ‹è¯•æ—¶é—´å˜é•¿
![](Figures/v2-5f8ff8cb94babde05e69365336e77a62_hd.jpg)
ğŸ‘†ç½‘ç»œç»“æ„

- - - -

## Squeeze-and-Excitation Networks

[arXiv](https://www.arxiv.org/pdf/1709.01507.pdf)  
å·ç§¯æ ¸ï¼šç©ºé—´ç»´åº¦ä¿¡æ¯ï¼Œç‰¹å¾ç»´åº¦ä¿¡æ¯èšé›†  
ç©ºé—´spatialï¼šinception(multiscale)ï¼Œinside-outside(context)  
SENet->ç‰¹å¾ç»´åº¦,feature channel  
Motivation:

1. Explicitly model channel-interdependcies
2. Feature recalibration: enhance useful, suppress less useful

ç‰¹å¾é€šé“ä¹‹é—´çš„å…³ç³»ï¼šç‰¹å¾é‡æ ‡å®šï¼ˆé€šè¿‡å­¦ä¹ çš„æ–¹å¼æ¥è‡ªåŠ¨è·å–åˆ°æ¯ä¸ªç‰¹å¾é€šé“çš„é‡è¦ç¨‹åº¦ï¼Œç„¶åä¾ç…§è¿™ä¸ªé‡è¦ç¨‹åº¦å»æå‡æœ‰ç”¨çš„ç‰¹å¾å¹¶æŠ‘åˆ¶å¯¹å½“å‰ä»»åŠ¡ç”¨å¤„ä¸å¤§çš„ç‰¹å¾ï¼‰  
Squeeze: global pooling, é¡ºç€ç©ºé—´ç»´åº¦å‹ç¼©ï¼Œå¢åŠ å…¨å±€ç©ºé—´ä¿¡æ¯ï¼Œæ¯ä¸€ä¸ªäºŒç»´ç‰¹å¾å›¾å˜ä¸ºä¸€ä¸ªå®æ•°ã€‚è¡¨ç¤ºç‰¹å¾é€šé“ä¸Šå…¨å±€åˆ†å¸ƒï¼ŒåŠ ä¸ŠSæ¨¡å—ä½¿å¾—é è¿‘è¾“å…¥çš„å±‚ä¹Ÿå¯ä»¥è·å¾—å…¨å±€æ„Ÿå—é‡  
Excitation: like gate in RNN. æ¯ä¸ªé€šé“ç”Ÿæˆæƒé‡ï¼Œå»ºæ¨¡ç›¸å…³æ€§ï¼Œcapture channel-wise dependencies  
Wçš„è¦æ±‚ * learn non-linear interaction * learn a non-mutually-exclusive relationship since we would like to ensure that multiple channels are allowed to be emphasised opposed to one-hot activation  
Reweight: multiply with feature map Â 

åµŒå…¥Inceptionï¼š  
åµŒå…¥ResNetï¼š additionä¹‹å‰è¿›è¡Œscaleæ“ä½œï¼Œé˜²æ¢¯åº¦å¼¥æ•£

---

## Hard Negative Mining in SSD & Focal Loss

1. Hard Negative Mining in SSD ä½œä¸ºä¸­é—´ç»“æœå¤„ç†çš„æ­¥éª¤ã€‚åªæœ‰GTæ¡†/å’ŒGTæ¡†IoUå¤§äºé˜ˆå€¼çš„æ‰æ˜¯æ­£æ ·æœ¬ï¼ˆå³æ­£ç¡®æ£€æµ‹æ¡†ï¼Œæ•°é‡å°‘ï¼‰ï¼Œå…¶ä»–éƒ½æ˜¯è´Ÿæ ·æœ¬ï¼ˆå³é”™è¯¯çš„æ£€æµ‹æ¡†ï¼Œæ•°é‡å¤§ï¼‰

   > <u>**ä¸ºäº†æ­£è´Ÿæ ·æœ¬æ•°é‡å¹³è¡¡**</u>ï¼Œé˜²æ­¢å°‘é‡å…³é”®çš„ï¼ˆæå‡æ€§èƒ½ï¼‰çš„è´Ÿæ ·æœ¬è¢«å¤§é‡æ­£æ ·æœ¬æ©ç›–è€Œæ— æ³•è¢«å­¦ä¹ /ä¼˜åŒ–åˆ°ã€‚   
   > è§£å†³é”™è¯¯æ ·ä¾‹å¤ªå¤šï¼Œ<u>**æ­£ç¡®æ ·ä¾‹å¤ªå°‘**</u>ï¼Œæ©ç›–æ­£ç¡®æ ·ä¾‹çš„é—®é¢˜ã€‚  

    **Hard Negative Mining in SSD**: ç›´æ¥é€šè¿‡æ ¹æ®ç½®ä¿¡åº¦æŸå¤±ï¼Œ<u>**æ’åºç­›é€‰**</u> æ¥é€‰æ‹©åˆ†ç±»æŸå¤±æœ€å¤§çš„è´Ÿæ ·æœ¬ï¼ˆå³ä¸æ˜¯ç‰©ä½“ä½†æ˜¯æœ‰æœ€é«˜çš„åˆ†ç±»ç½®ä¿¡åº¦ -> å›°éš¾åˆ†ç±»æ ·æœ¬_è¿·æƒ‘æ€§ï¼Œä¸¢å¼ƒä¸æ˜¯ç‰©ä½“ä½†åˆ†ç±»ç½®ä¿¡åº¦ç›¸å¯¹è¾ƒä½ -> ç®€å•_é”™è¯¯ä¸ä¸¥é‡/ä¸æ˜æ˜¾ï¼‰ï¼Œåªä¿ç•™åˆ†ç±»ç½®ä¿¡åº¦æŸå¤±è¾ƒå¤§çš„ï¼Œäººä¸ºä¿è¯æ ·æœ¬æ•°é‡å¹³è¡¡ã€‚

2. Focal Loss ç”¨äºæŸå¤±å‡½æ•°ä¸­

   > <u>**ä¸ºäº†èƒ½å­¦åˆ°å›°éš¾æ ·ä¾‹**</u>ï¼Œå­¦åˆ°æ›´å¤šï¼Œä¸è¢«ç®€å•æ©ç›–ã€‚  
   > è§£å†³é”™è¯¯æ ·ä¾‹ä¸­ï¼Œ<u>**ç®€å•çš„é”™è¯¯æ ·ä¾‹å¤ªå¤š**</u>ï¼Œå›°éš¾é”™è¯¯æ ·ä¾‹å¤ªå°‘ï¼Œä¸”æ±‚å’Œåæ©ç›–å›°éš¾çš„é”™è¯¯æ ·ä¾‹ï¼Œè€Œå¯¼è‡´æ£€æµ‹å™¨å­¦ä¸åˆ°å›°éš¾çš„é”™è¯¯æ ·ä¾‹ï¼ˆçœŸæ­£éœ€è¦å­¦/ä¼˜åŒ–çš„ï¼‰ã€‚  

    **Focal Loss**: é€šè¿‡ç»™ä¸åŒç½®ä¿¡åº¦çš„æ ·æœ¬<u>**å¢åŠ æƒé‡**</u>çš„æ–¹æ³•ã€‚æ¥è¿‘0/1ä¸ºç®€å•æ ·æœ¬ï¼Œæ¥è¿‘0.5ä¸ºéš¾æ ·æœ¬ã€‚æ‰€ä»¥æ­£ä¾‹x (1-p)ï¼Œè´Ÿä¾‹x pï¼Œä½¿ç”¨ _ä¸ç¡®å®šç¨‹åº¦_ ä½œä¸ºæƒé‡ã€‚éš¾æ˜“çš„é”™è¯¯éƒ½ä¼šå­¦ï¼Œä½†å›°éš¾çš„é”™è¯¯å¯¹losså½±å“æ›´å¤§ã€‚
    $L(p,y)=-(y\cdots (1-p)\cdots \log(p)+(1-y)\cdots p\cdots\log(1-p))$

---

## One-stage & Two-stage detector

#### RCNN

![](Figures/6F235DE5-4774-4527-9FC9-F8EAF8252AEE.png)


## ICCV 2019 workshop

#### LPIRC

https://rebootingcomputing.ieee.org/lpirc/2019
Winner talk:  [http://ieeetv.ieee.org/conference-highlights/award-winning-methods-for-lpirc-tao-sheng-lpirc-2018?rf=series|3](http://ieeetv.ieee.org/conference-highlights/award-winning-methods-for-lpirc-tao-sheng-lpirc-2018?rf=series%7C3) 
 [http://ieeetv.ieee.org/conference-highlights/deeper-neural-networks-kurt-keutzer-lpirc-2018?rf=series|3](http://ieeetv.ieee.org/conference-highlights/deeper-neural-networks-kurt-keutzer-lpirc-2018?rf=series%7C3) 
_Real Time Object Detection On Low Power Embedded Platforms_
_1810.01732.pdf_

#### Compact and Efficient Feature Representation and Learning

http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html
DCNN network quantization and compression, energy efficient network architectures, binary hashing techniques and data efficient techniques like meta learning

---

## Triplet Loss

![](Figures/20150707120209693.png)
ä¸‰å…ƒç»„: [anchor, positive, negative] æ‹‰è¿‘posï¼Œæ¨è¿œneg

$\mathcal{L}_{\mathrm{tri}}(\theta)=\sum_{a, p, n \atop y_{a}=y_{p} \neq y_{n}}\left[m+D_{a, p}-D_{a, n}\right]_{+}$

> é€‰å‡ºBä¸ªtripletsï¼Œåªç”¨äº†Bä¸ªè®­ç»ƒï¼Œå®é™…ä¸Šå¯ä»¥æœ‰`6B^2-4B`ç§tripletsçš„ç»„åˆï¼ˆ_Bä¸ªanchorï¼Œposå›ºå®šä¸€å¯¹ä¸€ï¼Œé™¤æ­¤äºŒå…¶ä»–æ‰€æœ‰éƒ½å¯ä»¥ä¸ºnegï¼Œ3B-2ç§ï¼›anchorå’Œposäº¤æ¢ä¹˜2_ï¼‰ `2*B*(3B-2)`ç§  

éš¾è®­ç»ƒï¼Œéœ€è¦**triplet mining**ã€‚åˆ†å‡ºhardï¼Œsemi-hardï¼Œeasyä¸‰ç§æ ·æœ¬
![](Figures/02_triplets.png)

1. <u>**Batch-Hard**</u>
   <u>é€‰æ‹©Pä¸ªç±»åˆ«ï¼ˆäººï¼‰ï¼Œæ¯ä¸ªç±»åˆ«Kä¸ªæ ·æœ¬ï¼ˆç…§ç‰‡ï¼‰ï¼ŒPKä¸ªæ ·æœ¬ä½œä¸ºanchor</u> ã€‚æ¯ä¸ªanchoråªé€‰æ‹©**è·ç¦»æœ€è¿œçš„poså’Œè·ç¦»æœ€è¿‘çš„neg**ï¼ˆæœ€hardï¼‰

   $\mathcal{L}_{\mathrm{BH}}(\theta ; X)=\overbrace{\sum_{i=1}^{P} \sum_{a=1}^{K}}^{\text {all anchors}} \left[m +  \overbrace{\max_{p=1 \ldots K} D\left( f_{\theta}\left(x_{a}^{i}\right), f_{\theta}(x_p^i)\right)}^{\text {hardest positive}} - \underbrace{\min\limits_{j=1 \ldots P,\; n=1\ldots K \atop j \neq i} D\left(f_{\theta}\left(x_{a}^{i}\right), f_{\theta}\left(x_{n}^{j}\right)\right)}_{\text {hardest negative}}\right]_{+} $
   ä¸€å…±$P_K$ä¸ªtriplets

2. <u>**Batch-All**</u>
   é€‰æ‹©Pä¸ªç±»åˆ«ï¼ˆäººï¼‰ï¼Œæ¯ä¸ªç±»åˆ«Kä¸ªæ ·æœ¬ï¼ˆç…§ç‰‡ï¼‰ï¼ŒPKä¸ªæ ·æœ¬ä½œä¸ºanchorï¼Œlossè®¡ç®—æ‰€æœ‰çš„poså’Œæ‰€æœ‰çš„negï¼ˆå’Œbaselineé€‰æ³•ç›¸åŒï¼‰

   $\mathcal{L}_{\mathrm{BA}}(\theta ; X)= \overbrace{\sum_{i=1}^{P} \sum_{a=1}^{K}}^{\text {all anchors}} \overbrace{\sum_{p=1 \atop p \neq a}^{K}}^{\text{all pos.}} \overbrace{\sum_{j=1 \atop j \neq i}^{P} \sum_{n=1}^{K}}^{\text {all neg.}}\left[m+d_{j, a, n}^{i, a, p}\right]_{+}$

   $d_{j, a, n}^{i, a, p}=D\left(f_{\theta}\left(x_{a}^{i}\right), f_{\theta}\left(x_{p}^{i}\right)\right)-D\left(f_{\theta}\left(x_{a}^{i}\right), f_{\theta}\left(x_{n}^{j}\right)\right)$
   $P_K$ä¸ªanchorï¼Œæ¯ä¸ªæœ‰$K-1$çš„posï¼Œ$P_K-K$ä¸ªnegï¼ˆå…¶ä»–æ‰€æœ‰ç±»åˆ«çš„æ ·æœ¬ï¼‰ã€‚ä¸€å…±$P_K(K-1)(P_K-K)$ä¸ªtriplets

---

## Long Short Term Memory (LSTM)

> Handling long-term dependencies

LSTM blocksğŸ‘‡

![image-20191018172300763](Figures/image-20191018172300763.png)

#### Core idea

![image-20191018172422527](Figures/image-20191018172422527.png)

ğŸ‘†**Cell state** convey information straight down along the entire chain

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png)

ğŸ‘†**Gate** controls whether add/remove information to the cell state

---

![image-20191018174346388](Figures/image-20191018174346388.png)

ğŸ‘†**Forget Gate**: how many last state information($c_{t-1}$) keep/forget.

![image-20191018174812836](Figures/image-20191018174812836.png)

ğŸ‘†**Input Gate**: what new information weâ€™re going to store in the cell state.

Two parts: **sigmoid** layer decide which part of values we will update, **tanh** layer create a vector of new candidate values $\tilde{C}_t$

![image-20191018175359262](Figures/image-20191018175359262.png)

ğŸ‘†Apply to cell state: forget first, then partially add new candidate

![image-20191018175611169](Figures/image-20191018175611169.png)

ğŸ‘†**Output Gate**: decide what weâ€™re going to output

Two parts: **sigmoid** layer decide which parts of the cell state going to output, **tanh** layer filters cell state. **Multiply** them together to get final output.

---

#### Variants

**1. Peephole Connection**

gate layer look at the cell state

![image-20191018192328653](Figures/image-20191018192328653.png)

**2. Input&Forget Together**

make what to forget and what to add together. Only forget when going to input something, only input when we forget.

![image-20191018192633284](Figures/image-20191018192633284.png)

**3. Gated Recurrent Unit**

Combines the forget and input gated into "update gate". Merge the cell state and the hidden state.

![image-20191018192913785](Figures/image-20191018192913785.png)

[Reference](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

---

## NASNet

> å…ˆåœ¨å°æ•°æ®é›†ä¸­è®­ç»ƒç½‘ç»œå•å…ƒï¼Œå†åœ¨å¤§æ•°æ®é›†ä¸­å †å å•å…ƒ  

å­¦ä¹ ç½‘ç»œä¸­å †å çš„ç½‘ç»œå•å…ƒ1. Normal cellå°ºå¯¸ä¸å˜ 2. Reduction cellå‡å°ºå¯¸
æ§åˆ¶å™¨ï¼šä¸€ç›´åœ¨æ‰§è¡Œä¸¤ä¸ªç‰¹å¾å›¾çš„<u>**èåˆ**</u>
![](Figures/v2-88f671125c2996924eb8a2c5b48c965d_r.jpg)
é€‰æ‹©ç¬¬ä¸€ä¸ªfeature mapå’Œç¬¬äºŒä¸ªfeature map(ç°è‰²) `2` ï¼Œè®¡ç®—è¾“å…¥çš„feature map A B(é»„è‰²) `2` ï¼Œé€‰æ‹©æ“ä½œèåˆä¸¤ä¸ªfeature map(ç»¿è‰²) `1` 
RNNé¢„æµ‹ï¼Œè¾“å‡º`2 x 5 x B`å…¶ä¸­ _normal cell_ + _reduction cell_ ï¼›æ¯ä¸ªéƒ½æœ‰Bä¸ªå—å †å ï¼›æ¯ä¸ªblockæœ‰äº”ä¸ªè¾“å‡ºğŸ‘†
NASNetè¿ç§»å­¦ä¹ ä¼˜åŒ–ç­–ç•¥ä¸ºProximal Policy Optimization(PPO) ğŸ‘ˆ 
æå‡º**scheduled drop path**ï¼Œéšæœºä¸¢å¼ƒéƒ¨åˆ†åˆ†æ”¯ï¼Œå¢åŠ ç½‘ç»œå†—ä½™overfittingï¼Œç±»ä¼¼ã€ŒInceptionã€
ä¸¢å¼ƒæ¦‚ç‡éšæ—¶é—´çº¿å‹å¢åŠ ï¼Œè®­ç»ƒæ¬¡æ•°å¤šå®¹æ˜“è¿‡æ‹Ÿåˆ

---

## DetNAS

**weight sharing** inherit its weights from supernet instead of training from scratch
**joint optimization** weight and architecture
**No proxy task or dataset** ä¸éœ€è¦æå‰è®­ç»ƒå°ç½‘ç»œ/å°æ¨¡å—ï¼Œä¹Ÿä¸éœ€è¦ä»å°æ•°æ®é›†åˆ°å¤§æ•°æ®é›†è®­ç»ƒ
**train from scratch** æ›´å¤šè¿­ä»£ï¼Œä¸é€‚ç”¨äºå°æ•°æ®é›†
é¦–å…ˆè®­ç»ƒ(pretrain+finetune)ä¸€ä¸ªsupernetï¼Œç„¶åå†supernetç©ºé—´ä¸­æ‰¾ï¼›ç›´æ¥åœ¨detä»»åŠ¡`Vdet`ä¸Šæœç´¢ <u>proxyless</u> ğŸ‘‡
![](Figures/DEEE8DD1-BD56-46A9-BA8A-6D16942C8049.png)
ä¼˜ç‚¹ï¼š

1. Decoupling: æ²¡æœ‰weightå’Œarchitectureä¹‹é—´çš„bias interaction

2. <u>supernetè®­å¥½åï¼Œç›´æ¥ç”¨valåœ¨supernetä¸Šæœç´¢</u> ç‰¹å®šåº”ç”¨åœºæ™¯çš„ç»“æ„ï¼Œè€Œä¸æ˜¯finetune

#### è®­ç»ƒsupernetğŸ‘‡

![](Figures/78DB7B34-1276-4AE0-AC78-0C38A7308692.png)

1. Single path sampling: ä½¿è®­ç»ƒå’Œæµ‹è¯•çš„é…ç½®ä¸€è‡´
   ![](Figures/74B60042-F0B7-48C0-BC3E-47E80BE0E4B0.png)

2. åŒæ­¥BN: <u>BN can not be frozen</u>, ä¸åŒpath BNçš„å‚æ•°ä¸åŒï¼ˆGroupNormé€Ÿåº¦æ…¢ï¼‰

#### æœç´¢å­ç½‘ç»“æ„

éå†æ¯ä¸ªpathï¼Œéœ€è¦é‡æ–°åœ¨trainsetä¸Šæœé›†è®¡ç®—BNçš„meanå’Œvar
ä¸ç”¨trainsetè®­ç»ƒå­ç½‘ï¼Œè€Œç›´æ¥åœ¨valsetä¸Ševal

#### Target task datasetä¸Šfinetuneå­ç½‘ç»“æ„

---

## Self-supervised Sample Mining

> <u>**semi-supervised**</u> <u>**weakly-supervised**</u> ä½¿ç”¨æœªæ ‡æ³¨çš„æ•°æ®æå‡æ¨¡å‹æ€§èƒ½   

![](Figures/image.png)
è¿™ä¸ªæ¡†æ¶æœ‰ä¸¤ä¸ªé˜¶æ®µï¼Œåˆ†åˆ«æ˜¯é€šè¿‡SSMå¯¹é«˜ä¸€è‡´æ€§æ ·æœ¬è¿›è¡Œä¼ªæ ‡æ³¨é˜¶æ®µå’Œé€šè¿‡ALé€‰æ‹©ä½ä¸€è‡´æ€§æ ·æœ¬é˜¶æ®µã€‚é¦–å…ˆä½¿ç”¨å·²æ ‡æ³¨çš„å›¾ç‰‡å¯¹æ¨¡å‹è¿›è¡Œfine-tuneï¼Œå¯¹æœªæ ‡æ³¨æˆ–éƒ¨åˆ†æ ‡æ³¨çš„å›¾ç‰‡æå–region proposalsï¼ˆæœªæ ‡æ³¨æ ·æœ¬ï¼‰ï¼ŒæŠŠè¿™äº› **region proposals <u>ç²˜è´´</u> åˆ°å·²æ ‡æ³¨çš„å›¾ç‰‡ä¸­è¿›è¡Œäº¤å‰å›¾ç‰‡éªŒè¯ï¼Œæ ¹æ®é‡æ–°é¢„æµ‹å‡ºæ¥çš„ç½®ä¿¡åº¦ç¡®å®šå¦‚ä½•å¯¹æœªæ ‡æ³¨æ ·æœ¬è¿›è¡Œæ ‡æ³¨** ã€‚å¯¹äºé«˜ä¸€è‡´æ€§æ ·æœ¬ï¼Œç›´æ¥è¿›è¡Œä¼ªæ ‡æ³¨ï¼Œå¯¹äºä½ä¸€è‡´æ€§æ ·æœ¬ï¼Œé€šè¿‡ALæŒ‘é€‰å‡ºæ¥ï¼Œè®©ç›¸å…³äººå‘˜è¿›è¡Œæ ‡æ³¨ã€‚ä¼ªæ ‡æ³¨çš„æ ·æœ¬ç”¨äºæ¨¡å‹çš„fine-tuneï¼Œè€Œæ–°æ ‡æ³¨çš„æ ·æœ¬æ·»åŠ åˆ°å·²æ ‡æ³¨çš„å›¾ç‰‡ä¸­ï¼ŒåŒæ—¶ä¹Ÿç”¨äºæ¨¡å‹çš„fine-tune  
å¯¹äºå¥½çš„æ ·æœ¬`xi`ï¼Œproposalä¸­çš„å†…å®¹å¯ä»¥å¾ˆå¥½çš„å±•ç¤ºjç±»çš„ç‰¹å¾ï¼Œç²˜è´´åˆ°æ²¡æœ‰jç±»çš„å›¾ç‰‡ä¸­ï¼Œæ–°ç”Ÿæˆçš„å›¾ç‰‡ä¸­çš„proposalæœ‰åŒ…å«`xi`çš„proposalï¼Œä¸”å…·æœ‰å¾ˆå¤§çš„æ¦‚ç‡å€¼ï¼Œ<u>**é«˜ä¸€è‡´æ€§**</u>è®¤ä¸ºä¹‹å‰çš„æ ·æœ¬æ¡†å‡†ç¡®æ— é”™è¯¯â¡ï¸æ­£æ ·æœ¬
ä»»åŠ¡åˆ†ç±»ğŸ‘‡
![](Figures/image%202.png)

---

## Bi-box Regression for Pedestrian Detection and Occlusion Estimation

> Part detectorï¼Œé’ˆå¯¹è¡Œäººé®æŒ¡é—®é¢˜ï¼Œå›å½’ã€Œå…¨èº«ã€+ã€Œå¯è§ã€ä¸¤ä¸ªæ¡†  

![](Figures/97B54C4E-F68D-4AB8-B030-A7C87315CE3E.png)
äºŒåˆ†æ”¯ç½‘ç»œï¼š

1. **Full body estimation** åªå¯¹pos proposalå›å½’
2. **Visible part estimation** å¯¹poså’Œnegçš„proposalå›å½’ï¼ˆFG&BGï¼‰ï¼Œ <u>neg proposalå›å½’ç¼©å°åŒºåŸŸ $\to$ 0</u>
   pos/neg proposalæ ¹æ®å’Œæ ‡æ³¨(full-body)çš„IoUå†³å®š
   ![](Figures/AFA72E30-3E28-4167-8CC3-28C74B49AA10.png)
   ğŸ‘†proposalç”±RPNè·å¾—ï¼Œsoftmax1å’Œsoftmax2é¢„æµ‹è¡Œäººå¾—åˆ†ã€‚ <u>fuse</u> æ—¶ï¼Œä¸¤ä¸ªéƒ½ä¸ºposï¼Œè¾“å‡ºpæ›´é«˜ï¼›ä¸€ä¸ªä¸ºnegï¼Œå¦ä¸€åˆ†æ”¯posï¼Œåˆ™posåˆ†æ”¯å¢åŠ p

#### Training

<u>æ ·æœ¬æ ‡æ³¨ä¸¤ä¸ªæ¡† (vis/full)</u>ã€‚æŠŠäº§ç”Ÿçš„proposal`P={x,y,w,h}`å’Œæ ‡æ³¨æ¡†`Q=(Full,Vis)`matchï¼Œpos-proposalè§„åˆ™ä¸º`IoU(P,F)>thresh_1` && `C(P,V)>thresh_2`. Cå®šä¹‰ä¸ºğŸ‘‡$C(P,\bar{V})=\frac{\text {Area}(P\cap\bar{V})}{\text {Area}(\bar{V})}$

è®­ç»ƒæ ·æœ¬`X=(Img, P, cate=1, F, V)`ï¼Œregression targetä¸ºğŸ‘‡
$\bar{f}^x=\frac{\bar{F}^x-P^x}{P^W},\;\bar{f}^y=\frac{\bar{F}^y-P^y}{P^h}$

$\bar{f}^w=\log(\frac{\bar{F}^w}{P^w}),\;\bar{f}^h=\log(\frac{\bar{F}^h}{P^h})$

Neg-proposal=1.background   2.poorly aligned proposal   å›å½’w,h -> 0

> Why should force _visible part of neg-proposal_ shrink? ã€Œvisåˆ†æ”¯å¯¹poså’Œnegéƒ½å¤„ç†ã€ #æ²¡æ‡‚  
> If the visible part estimation branch is trained to only regress visible parts for positive pedestrian proposals, the training of this branch would be dominated by pedestrian examples which are non-occluded or slightly occluded. For these pedestrian proposals, their ground-truth visible part and full body regions overlap largely. As a result, the estimated visible part region of a negative pedestrian proposal is often close to its estimated full body re- gion and the difference between the two branches after training would not be as large as the case in which the visible part regions of negative examples are forced to shrink to their centers.  

ä¸å¯¹è´Ÿæ ·æœ¬å¤„ç†ï¼Œåˆ™å¯¹è´Ÿæ ·æœ¬çš„é¢„æµ‹ç»“æœä¸¤ä¸ªåˆ†æ”¯ç›¸åŒã€Œfullåˆ†æ”¯å¯¹æ‰€æœ‰æ ·æœ¬å›å½’åˆ°GTï¼Œvisåˆ†æ”¯å¯¹æ­£æ ·æœ¬å›å½’åˆ°GTï¼Œå¯¹è´Ÿæ ·æœ¬å›å½’åˆ°0ã€
![](Figures/D29D7BC4-9682-4E65-AB16-3A92A974DD47.png)
ğŸ‘†è“è‰²æ¡†ï¼Œå’Œfull(GT)é‡åˆåº¦é«˜ï¼Œä½†å’Œvisé‡åˆåº¦ä½ã€‚åœ¨Faster-RCNNä¸­è¢«è®¤ä¸ºposï¼Œåœ¨æœ¬æ–‡ä¸­è®¤ä¸ºnegã€‚æ›´å¼ºçš„è¯„ä»·æ ‡å‡†

---

## Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations

<u>**ç½‘ç»œé‡åŒ–**</u>
Network compression: sparsity, quantization, and binarization
ä½¿ç”¨ä½ç²¾åº¦çš„æµ®ç‚¹è¿ç®—ï¼Œç›¸æ¯”äºé™æ€ç¡®å®šæ¯ä¸ªweightå’Œactivationçš„ç²¾åº¦ï¼Œæœ¬æ–‡**æ ¹æ®ç½‘ç»œè¾“å…¥**(ä¾‹å¦‚èƒŒæ™¯ä¸éœ€è¦ç²¾ç¡®è®¡ç®—)åŠ¨æ€ç¡®å®š **Tuning the bit width per layer**

#### Precision Gating

> Computes most features with low-precision arithmetic ops and only updates few important features to a high-precision  

1. å¯¹äºæ‰€æœ‰å±‚ä½ç²¾åº¦è®¡ç®—
2. å¯¹äºè¾“å‡ºçš„feature mapä¸­è¾ƒå¤§å€¼è®¤ä¸ºé‡è¦ç‰¹å¾ï¼Œå¯¹å…¶è¿›è¡Œç¨€ç–æ›´æ–°ï¼Œæé«˜ç²¾åº¦(sparse back propagation)

ç”¨åœ¨shufflenet v2ä¸Šæå‡26% ImageNetåˆ†ç±»ç²¾åº¦

---

## Repulsion Loss: Detecting Pedestrians in a crowd

> ReIdçš„occludeé—®é¢˜ï¼Œä½¿ä¸åŒç›®æ ‡çš„æ£€æµ‹æ¡†è¿œç¦»ã€Œç±»ä¼¼triplet lossã€  

![](Figures/%E6%88%AA%E5%B1%8F2020-02-23%E4%B8%8A%E5%8D%8812.28.04.png)

$L=L_{Attr}+\alpha*L_{RepGT}+\beta*L_{RepBox}$

#### Attraction term

é‡‡ç”¨æ£€æµ‹æ¡†æ¶ä¸­bboxå›å½’loss

$L_{\mathrm{Attr}}=\frac{\sum_{P \in \mathcal{P}_{+}} \operatorname{Smooth}_{L 1}\left(P, G_{A t t r}^{P}\right)}{\left|\mathcal{P}_{+}\right|}$

#### Repulsion Term (RepGT)

å’Œå‘¨å›´GTç›®æ ‡æ¡†è¿œç¦»ï¼Œè¿œç¦»IoUå¤§ä¸”æ²¡æœ‰åŒ¹é…çš„ç›®æ ‡æ¡†
å³$G_{R e p}^{P}=\underset{G \in \mathcal{G} \backslash\left\{G_{A t r}^{P}\right\}}{\arg \max } \operatorname{IoU}(G, P)$

ç±»ä¼¼IoU lossï¼ˆä¸æ˜¯IoUè€Œæ˜¯IoGï¼šè‹¥æœ€å°åŒ–IoUï¼Œåˆ™é¢„æµ‹æ¡†è¶Šå¤§IoUè¶Šå°ï¼‰

$\operatorname{IoG}(P,G) \overset{\triangle}{=}\frac{area(P\cap G)}{area(G)}$

$L_{\mathrm{RepGT}}=\frac{\sum_{P \in \mathcal{P}_{+}} \operatorname{Smooth}_{ln}\left(\operatorname{IoG}(P, G_{Rep}^{P})\right)}{\left|\mathcal{P}_{+}\right|}$

where

$\operatorname{smooth}_{l n}=\left\{\begin{array}{ll}
{-\ln (1-x)} & {x \leq \sigma} \\
{\frac{x-\sigma}{1-\sigma}-\ln (1-\sigma)} & {x>\sigma}
\end{array}\right.$
ä½¿é¢„æµ‹æ¡†é›†ä¸­åœ¨åŒ¹é…çš„ç›®æ ‡é™„è¿‘ï¼Œè€Œä¸ä¼šåç§»åˆ°ä¸´è¿‘ç‰©ä½“

#### Repulsion Term (RepBox)

é¢„æµ‹æ¡†å’Œå…¶ä»–é¢„æµ‹æ¡†è¿œç¦»ï¼ˆåŒ¹é…ä¸Šä¸åŒç‰©ä½“çš„ç›®æ ‡æ¡†ï¼‰
$L_{\mathrm{RepBox}}=\frac{\sum_{i \neq j} \operatorname{Smooth}_{l n}\left(\operatorname{IoU}\left(B^{P_{i}}, B^{P_{j}}\right)\right)}{\sum_{i \neq j} \mathbb{1}\left[\operatorname{IoU}\left(B^{P_{i}}, B^{P_{j}}\right)>0\right]+\epsilon}$

é˜²æ­¢ä¸åŒç‰©ä½“çš„ä¸¤ä¸ªæ£€æµ‹æ¡†è¢«NMSè¿‡æ»¤æ‰

---

## Normalization

![](Figures/2020-02-24-16-07-43-image.png)

è¾“å…¥$X$ï¼Œ è¾“å‡º$Y$ï¼Œå‚æ•°$\gamma$å’Œ$\beta$ (parameters, æ¯ä¸ªç‰¹å¾å›¾ä¸€å¯¹)

$y_i=\gamma\cdot\frac{x_i-\mu}{\sigma}+\beta$

Where $\mu=\frac{\sum_i^Nx_i}{N}$, $\sigma=\sqrt{\frac{\sum_I^N(x_i-\mu)^2}{N}+\epsilon}$ å‡å€¼å’Œæ–¹å·® (batch statistics)

ç»Ÿè®¡ä¸åŒæ ·æœ¬åœ¨åŒä¸€ä¸ªchannelåŒä¸€ä½ç½®æ•°æ®çš„å‡å€¼æ–¹å·® (reduce at batch dim hwc)

åå‘ä¼ æ’­æ—¶ï¼Œç”±äºå‡å€¼å’Œæ–¹å·®æ˜¯è¾“å…¥çš„å‡½æ•°

$\frac{d_{\ell}}{d_{x_{i}}}=\frac{d_{\ell}}{d_{y_{i}}} \cdot \frac{\partial_{y_{i}}}{\partial_{x_{i}}}+\frac{d_{\ell}}{d_{\mu}} \cdot \frac{d_{\mu}}{d_{x_{i}}}+\frac{d_{\ell}}{d_{\sigma}} \cdot \frac{d_{\sigma}}{d_{x_{i}}}$

where $\frac{\partial_{y_{i}}}{\partial_{x_{i}}}=\frac{\gamma}{\sigma}$, $\frac{d_{\ell}}{d_{\mu}}=-\frac{\gamma}{\sigma}\sum_i^N\frac{d_{\ell}}{d_{y_{i}}}$, $\frac{d_{\sigma}}{d_{x_{i}}}=-\frac{1}{\sigma}(\frac{x_i\mu}{N})$

![http://hangzh.com/blog/images/bn1.png](Figures/bn1.png)

Ref: https://kevinzakka.github.io/2016/09/14/batch_normalization/

[https://kiddie92.github.io/2019/03/06/å·ç§¯ç¥ç»ç½‘ç»œä¹‹Batch-Normalizationï¼ˆä¸€ï¼‰ï¼šHowï¼Ÿ](https://kiddie92.github.io/2019/03/06/å·ç§¯ç¥ç»ç½‘ç»œä¹‹Batch-Normalizationï¼ˆä¸€ï¼‰ï¼šHowï¼Ÿ/)

[https://www.cnblogs.com/shine-lee/p/11989612.html](https://www.cnblogs.com/shine-lee/p/11989612.html#å·ç§¯å±‚å¦‚ä½•ä½¿ç”¨batchnormï¼Ÿ)

#### Training

==Batch Norm==å¯¹ä¸€ä¸ªbatchä¸­æ‰€æœ‰æ ·æœ¬çš„åŒä¸€channelè®¡ç®—ç»Ÿè®¡é‡ï¼Œå—batch_sizeå½±å“

==Layer Norm==å¯¹å•ä¸ªæ ·æœ¬è®¡ç®—ç»Ÿè®¡é‡ï¼Œç”¨äºRNN

==Instance Norm==å•æ ·æœ¬å•é€šé“è®¡ç®—ï¼Œé£æ ¼è¿ç§»

==Group Norm==å¯¹é€šé“åˆ†ç»„ï¼Œè§£å†³BNä¾èµ–batch_sizeçš„é—®é¢˜

> åŒä¸€å±‚ç‰¹å¾é€šé“ä¹‹é—´å…³è”æ€§å¼ºï¼Œç‰¹å¾å…·æœ‰ç›¸åŒåˆ†å¸ƒï¼Œå¯ä»¥group

Switchable Normalizationè®¡ç®—BNã€LNã€INä¸‰ç§çš„ç»Ÿè®¡é‡ï¼Œç„¶ååŠ æƒ$w_k$ä½œä¸ºSNçš„å‡å€¼$\mu$å’Œæ–¹å·®$\sigma$ã€Œè§£å†³batch_sizeå½±å“ï¼Œè‡ªé€‚åº”ä¸åŒä»»åŠ¡ã€

normğŸ‘‡

$\hat{h}_{n c i j}=\gamma \frac{h_{n c i j}-\Sigma_{k \in \Omega} w_{k} \mu_{k}}{\sqrt{\Sigma_{k \in \Omega} w_{k}^{\prime} \sigma_{k}^{2}+\epsilon}}+\beta$

åŠ æƒæƒé‡å½’ä¸€åŒ–ğŸ‘‡

$w_k=\frac{e^{\lambda_k}}{\sum_{z\in\{\text {bn,ln,in}\}}e^{\lambda_z}}$

![](Figures/2020-02-24-16-22-39-image.png)

#### Testing

LN, INæ­£å¸¸è®¡ç®—ï¼ŒBNé‡‡ç”¨<u>batch average</u>æ–¹å¼ï¼Œå…·ä½“è¿‡ç¨‹æ˜¯ï¼Œå†»ç»“æ‰€æœ‰çš„å‚æ•°ï¼Œä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–ä¸€å®šæ•°é‡çš„æ ·æœ¬ï¼Œè®¡ç®—SNçš„å‡å€¼å’Œæ–¹å·®ï¼Œç„¶åä½¿ç”¨ä»–ä»¬çš„å¹³å‡å€¼ä½œä¸ºBNçš„ç»Ÿè®¡å€¼

Ref: [https://zhuanlan.zhihu.com/p/39296570](https://zhuanlan.zhihu.com/p/39296570)

### Sync BN

> å¤šå¡è®­ç»ƒæ—¶ï¼Œä¼ ç»ŸBNåªåœ¨å•GPUä¸Šå½’ä¸€åŒ–ï¼Œæ”¹æˆå¤šä¸ªGPUä¹‹é—´åŒæ­¥ä¿¡æ¯ï¼›æ€§èƒ½æ˜æ˜¾æå‡

é€šè¿‡è®¡ç®—å‡å€¼å’Œæ–¹å·®çš„ä¸­é—´é‡$\sum x$å’Œ$\sum x^2$ï¼Œåªéœ€åŒæ­¥ä¸€æ¬¡

æ ¹æ®$D[x]=E[x^2]-E[x]^2$

**FP** è®¡ç®—å‡å€¼å’Œæ–¹å·® $\mu=\frac{\sum x}{N}$, $\sigma=\sqrt{\frac{\sum x^2}{N}-\mu^2+\epsilon}$

**BP** è®¡ç®—$\frac{d_{\ell}}{d_{x_{i}}}$, $\frac{d_\ell}{d_{\sum_k x}}$å’Œ$\frac{d_\ell}{d_{\sum_k x^2}}$ï¼Œéƒ½å¯ä»¥å•å¡ä¸Šè®¡ç®—ï¼ŒåªåŒæ­¥ä¸€æ¬¡

![http://hangzh.com/blog/images/bn3.png](Figures/bn3-20200319223355972.png)

Ref: https://hangzhang.org/PyTorch-Encoding/notes/syncbn.html

**Improvement**: MABN(ç§»åŠ¨å¹³å‡+å‡å°‘ç»Ÿè®¡é‡+ä¸­å¿ƒåŒ–æƒé‡) [https://arxiv.org/abs/2001.06838](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2001.06838)

---

## Universal-RCNN: Universal Object Detection via Transferable Graph R-CNN

> è§£å†³æ£€æµ‹åŸŸè¿ç§»ï¼Œä¸åŒåŸŸ<u>**ç±»åˆ«**</u>å…³ç³»æ¨ç†()reasoning)å’Œè¿ç§»(transfer)

**Intra-Domain Reasoning**: åŸŸå†…éƒ¨å›¾è¡¨ç¤ºä¸Šä¼ æ’­ï¼Œç±»åˆ«ä¹‹é—´ç›¸å…³æ€§

**Inter-Graph Transfer**: åŸŸé—´è¿ç§»ï¼Œä¸åŒåŸŸç±»åˆ«ä¹‹é—´å±‚æ¬¡å…³ç³»

![](Figures/2020-02-25-20-41-49-image.png)

#### Intra-Domain Reasoning Module

1. ä½¿ç”¨backboneè®¡ç®—proposal feature (global **semantic pool**)

2. æ„å»º**åŒºåŸŸ**å…³ç³»å›¾$G_{T\to T}$ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºregion proposalï¼Œè¾¹è¡¨ç¤ºå…³ç³»ã€‚proposalä¹‹é—´å…³è”å…³ç³»(attribute similarities, interactions)

3. å­¦ä¹ è¾¹æƒé‡$\mathbf{Z}\mathbf{Z}^T$ï¼Œåªä¿ç•™é‡è¦proposalå…³ç³»çš„**ç¨€ç–é‚»æ¥çŸ©é˜µ**

4. propsoal featureåœ¨GCNè®¡ç®—ï¼Œå’ŒåŸå§‹ç‰¹å¾concatï¼Œå¾—åˆ°æ–°çš„global semantic pool $\mathbf{P_S}$

sharing common features between categories via connected edges such as similar attributes & relations.

improve feature rep. by adding adaptive contexts from global semantic pool

#### Inter-Domain Transfer Module

> bridge the gap between domains

æºåŸŸç”¨ä¸Šè¿°æ¨¡å—æ„å»º$\mathbf{P_S}$ï¼Œæ„å»º$\mathbf{G_{S\to T}}$ï¼ŒGCNè®¡ç®—ä»æºåŸŸåˆ°ç›®æ ‡åŸŸè¿ç§»ï¼ŒconcatæºåŸŸç‰¹å¾

![](Figures/2020-02-25-22-03-19-image.png)

---

## Model-Agnostic Structured Sparsification with Learnable Channel Shuffle

> åˆ’åˆ†groupï¼Œconv$\to$group convï¼Œåˆ†ç»„è¿›è¡Œé€šé“shuffle

å¸¸è§ç½‘ç»œå‹ç¼©æ–¹å¼ 

1. é‡åŒ–æƒé‡weight quantization 

   ä½ç²¾åº¦è¡¨ç¤ºï¼Œæ˜“æ€§èƒ½ä¸‹é™

2. çŸ¥è¯†è’¸é¦knowledge distillation 

   æ˜“å—æ•™å¸ˆç½‘ç»œå½±å“

3. ç½‘ç»œå‰ªænetwork pruning

   å»æ‰éƒ¨åˆ†unimportantç½‘ç»œå‚æ•° (eg. filter pruning algo.)

å‚æ•°weight normæ¥åˆ¤æ–­æ˜¯å¦pruneã€ŒL1æ­£åˆ™åŒ–å¯å¢åŠ sparsityã€

ä½¿ç”¨GroupConvä½¿é€šé“é—´ç¨€ç–è¿æ¥

ä½¿ç”¨learning-basedé€šé“shuffleå¢å¼ºinter-group info flow

**æ­¥éª¤**ï¼šä½¿ç”¨structured regularizationè®­ç»ƒå¤§æ¨¡å‹ $\to$ å°†éƒ¨åˆ†convè½¬ä¸ºgroup conv $\to$ finetuneæ¢å¤ç²¾åº¦

#### connectivity patterns

å·ç§¯æ ¸æƒé‡å€¼$W_{j,i}$è®¤ä¸ºinput output channelä¹‹é—´å…³è”æ€§

å·ç§¯**groupable**ï¼šæƒé‡å¯ä»¥æ’åˆ—æˆ**block diagonal**

ğŸ‘‡ä¸€èˆ¬convçš„weightsï¼ˆchannelä¹‹é—´å…³è”ç¨‹åº¦ï¼‰

![image-20200226235116107](/Users/mk/Library/Application Support/typora-user-images/image-20200226235116107.png)

ğŸ‘‡groupable convï¼Œä¸€ä¸ªchannelåªå’ŒåŒç»„å†…å‡ ä¸ªchannelæœ‰é«˜å“åº”ï¼Œå³block diagonalï¼ˆğŸ‘‡æœ‰ä¸¤ä¸ªblockï¼Œcardinality=2ï¼Œæ¯ä¸ªblockä¸­çš„å¯¹è§’å—0æƒ©ç½šï¼Œblockä¸­å·¦ä¸‹å³ä¸Šå—0.5æƒ©ç½šï¼Œblockå—å¤–1æƒ©ç½šï¼Œè§ä¸‹ä¸‹å›¾ï¼‰

![image-20200226235221529](/Users/mk/Library/Application Support/typora-user-images/image-20200226235221529.png)

å˜ä¸ºå­¦ä¹ ==åˆ’åˆ†channel==é—®é¢˜ï¼ŒæŸå¤±å‡½æ•°æƒ©ç½šå·¦ä¸‹å³ä¸Šæ²¡æœ‰è¢«zero-outçš„æƒé‡ï¼Œä½¿ç”¨coordinate descentè®¡ç®—ï¼ˆçº¿æ€§è§„åˆ’é—®é¢˜ï¼Œor optimal transportï¼‰

#### Structured regularization

![image-20200227000502981](/Users/mk/Library/Application Support/typora-user-images/image-20200227000502981.png)

channel shuffleè§£é‡ŠğŸ‘‡

![image-20200227002933679](/Users/mk/Library/Application Support/typora-user-images/image-20200227002933679.png)

## Cosine Learning rate decay

1. warm upï¼šè®­ç»ƒå¼€å§‹å°†lrä»0é€æ­¥å¢åŠ åˆ°åˆå§‹å€¼
2. decayï¼šä½¿ç”¨cosineå‡½æ•°

![img](Figures/1*BJCssPOCn4u__NoAZs392w-20200228202856582.png)

å¸¸è§ä¸ºstep devay

![stepdecay](Figures/stepdecay.png)

Ref: https://medium.com/@scorrea92/cosine-learning-rate-decay-e8b50aa455b

## Learning When and Where to Zoom with Deep Reinforcement Learning

> åˆ†ç±»ä»»åŠ¡ä¸­å­¦ä¹ å¦‚ä½•ç¼©æ”¾ï¼Œspatially-sample

**Attention**ï¼š<u>å…ˆLR</u>ç”¨saliency networkäº§ç”Ÿattention mapï¼Œç„¶å<u>æ˜ å°„</u>åˆ°HRå›¾ç‰‡ä¸Šä½¿ç”¨

![image-20200306005012796](Figures/image-20200306005012796.png)

å­¦ä¹ ï¼šæ ¹æ®LRå›¾åƒå†³ç­–sampleå“ªäº›HRå›¾åƒã€‚sampleæˆæœ¬å’Œå‡†ç¡®ç‡tradeoff

![image-20200306010155518](Figures/image-20200306010155518.png)

1. ğŸ‘†ç”±LRå›¾ç‰‡é€šè¿‡policy networkè®¡ç®—å‡ºéœ€è¦zoom-inçš„grid
2. ç›´æ¥åœ¨HRä¸Šç›´æ¥sampleç›¸åº”ä½ç½®ï¼Œæ„æˆpath
3. HR-patchå’ŒLRåŸå›¾ç‰‡(optional)è¾“å…¥åˆ†ç±»ç½‘ç»œåˆ†ç±»
4. åˆ†ç±»æŸå¤±rewardè¿”å›ç»™policy network

é€‚ç”¨äºé¥æ„Ÿ/é«˜åˆ†è¾¨ç‡å›¾ç‰‡åˆ†ç±»/æ£€æµ‹(?)

---

### Jaccard distance

è¡¡é‡ä¸¤ä¸ªsetä¹‹é—´çš„è·ç¦»

**Jaccard index**: $J(X,Y)=\frac{|X\cap Y|}{|X\cup Y|}$

**Distance**: $D(X,Y)=1-J(X,Y)$

Ref: https://www.statisticshowto.datasciencecentral.com/jaccard-index/

---

### Label Smoothing

é’ˆå¯¹æ•°æ®é›†ä¸­**é”™è¯¯æ ‡æ³¨**ï¼Œä½¿`[0, 1]`å˜æˆ`[0.2, 0.8]`ï¼Œå‡å¼±æ­£æ ·æœ¬çš„lossï¼Œå†²æ·¡é”™è¯¯ä¿¡å·çš„å½±å“

$new\_onehot\_labels = labels * (1 - label\_smoothing) + label\_smoothing * \frac{1}{|labels|}$

$new\_label=[0\;1]\times(1-0.2)+0.2\times \frac{1}{2}=[0.1\;0.9]$

> Large logit gaps combined with the bounded gradient will make the model less adaptive and too confident about its predictions.

---

## BiDet: An Efficient Binarized Object Detector

> äºŒå€¼ç¥ç»ç½‘ç»œï¼Œä½¿ç”¨Information Bottleneckç†è®ºç®€åŒ–ç½‘ç»œ(ä½œä¸ºä¸€ä¸ªæŸå¤±å‡½æ•°)

æ£€æµ‹ä»»åŠ¡çœ‹ä½œ$X\to F\to L,C$ï¼Œå›¾ç‰‡åˆ°ç‰¹å¾åˆ°ç±»åˆ«ä½ç½®

**Information Bottleneckç†è®º**

ç›®æ ‡ä¸º$\min _{\phi_{b}, \phi_{d}} I(X ; F)-\beta I(F ; C, L)$

æœ€å°åŒ–ç‰¹å¾æå–çš„äº’ä¿¡æ¯ï¼ˆä¿¡æ¯å¢ç›Šï¼ŒçŸ¥é“Aå¯¹ç¡®å®šBçš„å½±å“ï¼‰ï¼Œæœ€å¤§åŒ–æ£€æµ‹ç½‘ç»œçš„äº’ä¿¡æ¯

*äº’ä¿¡æ¯å¯ä»¥çœ‹ä½œç½‘ç»œçš„ä¿¡æ¯æŸå¤±ï¼Œäº’ä¿¡æ¯å¤§ï¼Œä¿¡æ¯æŸå¤±å¤§ã€‚å‰åŠéƒ¨åˆ†ä¸ºç½‘ç»œæå–è¶³å¤Ÿçš„ç‰¹å¾ï¼ŒååŠéƒ¨åˆ†ä¸ºåªæ£€æµ‹æœ‰æ•ˆçš„æ¡†ï¼Œå»é™¤å†—ä½™ä¿¡æ¯*

**æŸå¤±å‡½æ•°**ï¼š$\begin{aligned}
&\min J=J_{1}+J_{2}\\
&\begin{aligned}
=&\left(\sum_{t, s} \log \frac{p\left(f_{s t} | \boldsymbol{x}\right)}{p\left(f_{s t}\right)}-\beta \sum_{i=1}^{b} \log \frac{p\left(c_{i} | \boldsymbol{f}\right) p\left(\boldsymbol{l}_{1, i} | \boldsymbol{f}\right) p\left(\boldsymbol{l}_{2, i} | \boldsymbol{f}\right)}{p\left(c_{i}\right) p\left(\boldsymbol{l}_{1, i}\right) p\left(\boldsymbol{l}_{2, i}\right)}\right) \\
&-\gamma \cdot \frac{1}{m} \sum_{i=1}^{m} s_{i} \log s_{i}
\end{aligned}
\end{aligned}$

å…¶ä¸­$J_1$ä¸ºIBç†è®ºçš„ä¼˜åŒ–ç›®æ ‡ï¼Œç±»åˆ«å¤šé¡¹å¼åˆ†å¸ƒï¼Œä½ç½®æ­£æ€åˆ†å¸ƒï¼Œ$p\left(f_{s t} | \boldsymbol{x}\right)$é€šè¿‡é‡‡æ ·å¾—åˆ°

$J_2=-\gamma \cdot \frac{1}{m} \sum_{i=1}^{m} s_{i} \log s_{i}$ä¸ºå‡å°‘ä¸€å¼ å›¾ä¸­ç›®æ ‡çš„è‡ªä¿¡æ¯($s_i$ä¸ºç¬¬iä¸ªç‰©ä½“çš„objectness)ã€‚æŠŠä¸€å¼ å›¾çš„å¤šä¸ªç‰©ä½“åˆ†ä¸ºå¤šä¸ªblockï¼Œä½¿$\sum^{m}_{i}s_i=1$ç›®æ ‡confå’Œä¸º1ï¼Œå‡å°‘ç›®æ ‡æ•°é‡/å‡é˜³æ€§ï¼Œå³è®©ä¸€å¼ å›¾å†…ä¸è¦æœ‰å¤šä¸ªhigh objectnessçš„ç‰©ä½“ï¼Œé™åˆ¶ä¸ºåªæœ‰å°‘é‡ç‰©ä½“ã€Œ<u>é€‚ç”¨äºæ¯å¼ ç…§ç‰‡ç›®æ ‡è¾ƒå°‘çš„æ•°æ®é›†</u>ã€

---

## Mixup: Beyond Empirical Risk Minimization

å¸¸è§ç½‘ç»œè®­ç»ƒä¸º**ERM**ï¼šè®­ç»ƒæ•°æ®ä¸ºç»éªŒï¼Œè®°ä½æ‰€æœ‰æ ·æœ¬ï¼Œä¸èƒ½<u>æ³›åŒ–</u>

æ•°æ®å¢å¼ºå¯ä»¥ç†è§£ä¸º**VRM (vicinal risk minimization)**ï¼šé¢†åŸŸé£é™©æœ€å°åŒ–ï¼Œæ„é€ æ•°æ®é›†æ ·æœ¬çš„é¢†åŸŸå€¼ï¼Œå­¦ä¹ é¢†åŸŸçš„å°å˜åŒ–ï¼Œä½†æ–¹å¼<u>dataset-dependent&heuristic</u>

æå‡ºåˆ©ç”¨çº¿æ€§æ’å€¼äº§ç”Ÿæ–°çš„æ ‡æ³¨æ ·æœ¬å¯¹ï¼Œæ··åˆåˆ°æ•°æ®é›†ä¸­è®­ç»ƒã€Œå¢åŠ æœ‰å™ªå£°çš„æ ·æœ¬ã€

$\tilde{x}=\lambda x_i+(1-\lambda)x_j$

$\tilde{y}=\lambda y_i+(1-\lambda)y_j$

å…¶ä¸­$(x_i,y_i)$å’Œ$(x_j,y_j)$ä¸ºæ•°æ®é›†ä¸­ã€Œæ ‡æ³¨-æ ·æœ¬ã€å¯¹ï¼Œ$\lambda\in [0,1]$

---

## Hybrid Task Cascade for Instance Segmentation

> å®ä¾‹åˆ†å‰²ï¼Œçº§è”ï¼Œmaskå’ŒboxåŒæ—¶é‡‡ç”¨cascadeæ–¹å¼refine

![image-20200321232008290](Figures/image-20200321232008290.png)

ğŸ‘†çº§è”refineï¼Œboxå’Œmaskä¸¤ä¸ªåˆ†æ”¯ã€‚æ¥å—ä¸Šä¸€stageå›å½’åçš„boxä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹æ–°çš„maskå’Œbox

![image-20200321232235092](Figures/image-20200321232235092.png)

ğŸ‘†mask1æ˜¯box1ç»è¿‡poolä¹‹åå¾—åˆ°çš„ï¼Œmaskå’Œboxä¸¤ä¸ªåˆ†æ”¯æœ‰äº¤äº’ï¼Œbox$\to$mask

![image-20200321232338714](Figures/image-20200321232338714.png)

ğŸ‘†maskä¹‹é—´ä¹Ÿè¿›è¡Œä¿¡æ¯äº¤äº’

![image-20200321232559233](Figures/image-20200321232559233.png)

äº¤äº’æ–¹å¼ğŸ‘†ï¼Œä¸Šä¸€ä¸ªstageçš„maskç»è¿‡`1x1` convï¼Œç„¶åå’Œbox_poolç›¸åŠ ï¼Œä¸Šä¸€ä¸ªstageçš„maskå’Œboxå…±åŒäº§ç”Ÿä¸‹ä¸€ä¸ªstageçš„mask

![image-20200321232644137](Figures/image-20200321232644137.png)

ğŸ‘†æœ€ååŠ å…¥è¯­ä¹‰ä¿¡æ¯åˆ†æ”¯

æ€§èƒ½æå¼ºï¼Œé€Ÿåº¦æ…¢ğŸ‘‡

![image-20200321232902678](Figures/image-20200321232902678.png)

![image-20200321233013589](Figures/image-20200321233013589.png)

---

## æ¨¡å‹é›†æˆ/é›†æˆå­¦ä¹ 

å¤šä¸ªå¼±å­¦ä¹ å™¨é›†æˆç»„åˆä¸ºæ›´ç²¾ç¡®æ›´é²æ£’æ¨¡å‹

å•ä¸ªæ¨¡å‹ æˆ–ä¸ºé«˜åç½®ï¼ˆä½è‡ªç”±åº¦æ¨¡å‹ï¼‰ï¼Œæˆ–ä¸ºé«˜æ–¹å·®ï¼ˆé«˜è‡ªç”±åº¦æ¨¡å‹ï¼‰ã€‚æ ¹æ®å¼±å­¦ä¹ å™¨çš„æ€§èƒ½é€‰æ‹©é›†æˆæ–¹æ³•ï¼Œé«˜åç½®ä½¿ç”¨`compose`ï¼Œé«˜æ–¹å·®ä½¿ç”¨`average`

![img](Figures/640.png)

#### Bagging (bootstrap+aggregating)

å¹¶è¡Œçš„ç‹¬ç«‹å­¦ä¹ å¤šä¸ªåŒè´¨å¼±å­¦ä¹ å™¨ï¼Œç›®æ ‡ä¸º**å‡å°æ–¹å·®**

1. ä½¿ç”¨bootstrapæ–¹æ³•ä»æºæ•°æ®é›†å–æ ·Nä¸ªæ•°æ®é›†ï¼Œè®¡ç®—ç»Ÿè®¡é‡ã€Œç½®ä¿¡åº¦ï¼Œrepresentative samplesã€
2. ä½¿ç”¨Nä¸ªæ•°æ®é›†ç‹¬ç«‹è®­ç»ƒNä¸ªå¼±å­¦ä¹ å™¨
3. æœ€ç»ˆç»“æœç”±Nä¸ªæ¨¡å‹çš„é¢„æµ‹æŠ•ç¥¨å¾—åˆ°

å¦‚ï¼šéšæœºæ£®æ—

#### Boosting

è¿­ä»£çš„æ‹Ÿåˆæ¨¡å‹ï¼Œä¸²è¡Œï¼Œç›®æ ‡ä¸º**å‡å°åç½®**

1. è®­ç»ƒé›†æˆå­¦ä¹ å™¨
2. reweightæ•°æ®é›†ã€Œå…³æ³¨éš¾æ ·æœ¬ã€
3. åŠ å…¥æ–°çš„å¼±å­¦ä¹ å™¨$s_{l}(.)=s_{l-1}(.)+c_{l} \times w_{l}(.)$ ã€ŒAdaBoostæƒé‡ä¸º$\arg\min\limits_{C_i} Error$ æ€§èƒ½è¶Šå¥½è´¡çŒ®è¶Šå¤§ã€

![img](Figures/640-20200411154812850.png)

å¦‚ï¼šAdaBoostï¼ŒGBDT

#### Stacking

å¤šä¸ªå¼±å­¦ä¹ å™¨å †å ï¼Œå­¦ä¹ å…ƒæ¨¡å‹å°†å…¶ç»„åˆã€Œä¾‹å¦‚ï¼šKNN+Logistic-Reg+SVMï¼ŒNNå°†å…¶ç»„åˆã€

1. è®­ç»ƒæ•°æ®é›†åˆ†ä¸ºä¸¤ç»„
2. ç¬¬ä¸€ç»„è®­ç»ƒæ‹Ÿåˆå¼±å­¦ä¹ å™¨
3. å¼±å­¦ä¹ å™¨åœ¨ç¬¬äºŒç»„ä¸Šçš„é¢„æµ‹ï¼Œä½œä¸ºå…ƒæ¨¡å‹çš„è¾“å…¥ã€‚åœ¨ç¬¬äºŒç»„æ•°æ®é›†ä¸Šæ‹Ÿåˆå…ƒæ¨¡å‹

![img](Figures/640-20200411155054069.png)

Ref: https://www.jiqizhixin.com/articles/2019-05-15-15, https://zhuanlan.zhihu.com/p/27689464

---

## Improving Convolutional Networks with Self-Calibrated Convolutions (SC Conv / SCNet)

> åœ¨ Group ConvåŸºç¡€ä¸Šæ”¹è¿›å·ç§¯ heterogeneous
>
> æ‹†åˆ†ä¸ºä¸è§„åˆ™çš„æ“ä½œ
>
> å¢å¤§æ„Ÿå—é‡ï¼Œå¤§èŒƒå›´çš„context

![image-20200512102143071](Figures/image-20200512102143071.png)

é¦–å…ˆæŠŠåŸå§‹ç‰¹å¾å›¾<u>é€šé“ä¸Š</u>åˆ†ä¸ºä¸¤éƒ¨åˆ† `Group Conv`

$\mathbf{X_1}$ä¸º**Self-Calibration**åˆ†æ”¯ï¼Œ$\mathbf{X_2}$ä¸ºå·ç§¯åˆ†æ”¯

#### Self-Calibration

åˆ†åˆ«åœ¨small scaleçš„`latent`ç©ºé—´å’Œlarge scaleçš„`original`ç©ºé—´è¿›è¡Œå·ç§¯æ“ä½œï¼Œå†èåˆ`calibration`

**Latentç©ºé—´**ï¼š

$\mathbf{T}_{1}=\operatorname{AvgPool}_{r}\left(\mathbf{X}_{1}\right)$ ä¸‹é‡‡æ ·å‡å°å°ºåº¦

$\mathbf{X}_{1}^{\prime}=\operatorname{Up}\left(\mathcal{F}_{2}\left(\mathbf{T}_{1}\right)\right)=\mathrm{Up}\left(\mathbf{T}_{1} * \mathbf{K}_{2}\right)$ å°å°ºåº¦ä¸Šå·ç§¯å¹¶upsampleå›åŸå°ºåº¦

**Originalç©ºé—´å’Œlatentç©ºé—´èåˆ**ï¼š$\mathbf{Y}_{1}^{\prime}=\mathcal{F}_{3}\left(\mathbf{X}_{1}\right) \cdot \sigma\left(\mathbf{X}_{1}+\mathbf{X}_{1}^{\prime}\right)$

$\sigma$ for sigmoid function

**Final output**ï¼š$\mathbf{Y}_{1}=\mathcal{F}_{4}\left(\mathbf{Y}_{1}^{\prime}\right)=\mathbf{Y}^{\prime} * \mathbf{K_4}$

Self-calibrationåœ¨å°å°ºåº¦æ“ä½œï¼Œå¯ä»¥æ‰©å¤§æ„Ÿå—é‡ (*Regional context*)ï¼Œèåˆå¤šå°ºåº¦çš„ä¿¡æ¯

![image-20200512103726329](Figures/image-20200512103726329.png)

---

## Strip Pooling: Rethinking Spatial Pooling for Scene Parsing

> å¸¦çŠ¶poolingï¼Œå„å‘å¼‚æ€§ç‰¹å¾ï¼Œåˆ†å‰²ä»»åŠ¡

![image-20200530103440982](Figures/image-20200530103440982.png)

é•¿æ¡å½¢æ„Ÿå—é‡ï¼Œ<font color=#FF0000 >è¡Œæ„Ÿå—é‡</font>åœ¨æ¯ä¸€è¡Œçš„æ‰€æœ‰åˆ—è¿›è¡Œaverageï¼Œ<u>å‹ç¼©åˆ°1åˆ—</u>ï¼Œpoolingç»“æœä¸ºåˆ—å‘é‡ï¼›<font color=#0000FF >åˆ—æ„Ÿå—é‡</font>åœ¨æ¯ä¸€åˆ—çš„æ‰€æœ‰è¡Œaverageï¼Œ<u>å‹ç¼©åˆ°ä¸€è¡Œ</u>ï¼Œpoolingç»“æœä¸ºè¡Œå‘é‡

ç„¶åæ‰©å±•æˆåŸå›¾å°ºå¯¸ï¼Œå†fusion

å¯¹é•¿æ¡çŠ¶ç‰©ä½“åˆå¸®åŠ©ï¼Œå¯ä»¥é›†åˆä»»æ„ä¸¤ä¸ªä½ç½®çš„dependency (long-range)

![image-20200530104233646](Figures/image-20200530104233646.png)

![image-20200530104306845](Figures/image-20200530104306845.png)

---

## Res2Net: A New Multi-scale Backbone Architecture

> å¤šå°ºåº¦å·ç§¯ï¼Œé€šç”¨ï¼Œä½†é€Ÿåº¦æœ‰ä¸‹é™ï¼›åŸºäºResNetï¼Œå¯ç”¨äºResNeXt

å°†ä¸€ä¸ªå·ç§¯åˆ†æˆä¸åŒæ·±åº¦çš„æ“ä½œï¼Œè·å¾—ä¸åŒå°ºåº¦çš„ç©ºé—´/è¯­ä¹‰ä¿¡æ¯

![image-20200530113431833](Figures/image-20200530113431833.png)

ç›¸æ¯”ResNetï¼Œå‡å°‘å‚æ•°é‡ææ€§èƒ½ã€‚åŒæ ·GFLOPSï¼Œé€Ÿåº¦ä¼šæ…¢ï¼Œä½†ç²¾åº¦æ›´é«˜ï¼›åŒæ ·ç²¾åº¦ä¸‹ï¼Œé€Ÿåº¦æ›´å¿«ã€‚

![image-20200530113939005](Figures/image-20200530113939005.png)

---

